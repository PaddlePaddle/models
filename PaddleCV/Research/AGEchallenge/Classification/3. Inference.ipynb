{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Angle closure Glaucoma Evaluation Challenge](https://age.grand-challenge.org/Details/)\n",
    "## Angle closure classification Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "- Assume `Training100.zip` and `Validation_ASOCT_Image.zip` are stored @ `./AGE_challenge Baseline/datasets/`\n",
    "- Assume `weights` are stored @ `./AGE_challenge Baseline/weights/`\n",
    "- In training phase, we use standard ResNet34 with `sigmoid(fc(1))` output\n",
    "- We split a single image into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, functools, math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Verify Fluid Program ... \n",
      "Your Paddle Fluid works well on SINGLE GPU or CPU.\n",
      "Your Paddle Fluid works well on MUTIPLE GPU or CPU.\n",
      "Your Paddle Fluid is installed successfully! Let's start deep Learning with Paddle Fluid now\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import paddle.fluid.layers as FL\n",
    "import paddle.fluid.optimizer as FO\n",
    "fluid.install_check.run_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = \"../datasets/Training100/\"\n",
    "image_path = os.path.join(data_root_path, \"ASOCT_Image\")\n",
    "\n",
    "val_file_path = os.path.join(data_root_path, \"cls_val_split.csv\")\n",
    "\n",
    "output_file = \"./Classification_Results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 // 2 # image split * 2\n",
    "THREAD = 8\n",
    "BUF_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Inference Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real time data augmentation\n",
    "def crop_image(img, target_size, center):\n",
    "    \"\"\" crop_image \"\"\"\n",
    "    height, width = img.shape[:2]\n",
    "    size = target_size\n",
    "    if center == True:\n",
    "        w_start = (width - size) // 2\n",
    "        h_start = (height - size) // 2\n",
    "    else:\n",
    "        w_start = np.random.randint(0, width - size + 1)\n",
    "        h_start = np.random.randint(0, height - size + 1)\n",
    "    w_end = w_start + size\n",
    "    h_end = h_start + size\n",
    "    img = img[h_start:h_end, w_start:w_end, :]\n",
    "    return img\n",
    "\n",
    "def split_image(img):\n",
    "    rows,_,_ = img.shape\n",
    "    # left, right split\n",
    "    return [img[:, :rows, :], img[:, -rows:, :]]\n",
    "    \n",
    "# data reader and xmap wrapper to enable multiprocessing data load\n",
    "\n",
    "def reader(img_path, file_list, batch_size=32, shuffle=True, shuffle_seed=42):\n",
    "    def read_file_list():\n",
    "        batch_data = []\n",
    "        np.random.shuffle(file_list)\n",
    "        for line in file_list:\n",
    "            single_img_path, _, _ = line.split(\",\")\n",
    "            batch_data.append(single_img_path)\n",
    "            if len(batch_data) == batch_size:\n",
    "                yield batch_data\n",
    "                batch_data = []\n",
    "        if len(batch_data) != 0:\n",
    "            yield batch_data\n",
    "    return read_file_list\n",
    "\n",
    "def process_batch_data(input_data):\n",
    "    batch_data = []\n",
    "    for sample in input_data:\n",
    "        file = sample\n",
    "\n",
    "        img = cv2.imread( file )\n",
    "        img = img[:, :, ::-1].astype('float32') / 255\n",
    "        \n",
    "        img = np.concatenate(split_image(img), axis=-1) # concat at channel dim\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        \n",
    "        img = crop_image(img, target_size=224, center=True)\n",
    "        \n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        batch_data.append((file, 0, img[:3,:,:]))\n",
    "        batch_data.append((file, 1, img[3:,:,:]))\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(img_list, img_path, batch_size, order=False):\n",
    "    data_reader = reader(img_path, img_list, batch_size)\n",
    "    mapper = functools.partial(process_batch_data)\n",
    "    \n",
    "    data_reader = paddle.reader.shuffle(data_reader, 32)\n",
    "    \n",
    "    return paddle.reader.xmap_readers(\n",
    "        mapper, data_reader, THREAD, BUF_SIZE, order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(val_file_path) as flist:\n",
    "    val_file_list = [os.path.join(image_path,line.strip()) for line in flist]\n",
    "    \n",
    "val_dataloader = data_loader(val_file_list, image_path, BATCH_SIZE, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model (compute graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network():\n",
    "    data_shape = [3, 224, 224]\n",
    "    \n",
    "    model = ResNet34()\n",
    "    \n",
    "    input_feature = FL.data(name='pixel', shape=data_shape, dtype='float32')\n",
    "    \n",
    "    logit = model.net(input_feature, class_dim=1)\n",
    "    predict = FL.sigmoid(logit)\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(use_cuda, pretrained_model, threshold=0.5):\n",
    "    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
    "    \n",
    "    startup_prog = fluid.Program()\n",
    "    val_prog = fluid.Program()\n",
    "\n",
    "    # 定义预测网络\n",
    "    with fluid.program_guard(val_prog, startup_prog):\n",
    "        # Use fluid.unique_name.guard() to share parameters with train network\n",
    "        with fluid.unique_name.guard():\n",
    "            val_output = network()\n",
    "\n",
    "    val_prog = val_prog.clone(for_test=True)\n",
    "    val_output.persistable = True\n",
    "            \n",
    "    exe = fluid.Executor(place)\n",
    "    exe.run(startup_prog)\n",
    "\n",
    "    if pretrained_model:\n",
    "        def if_exist(var):\n",
    "            return os.path.exists(os.path.join(pretrained_model, var.name))\n",
    "\n",
    "        fluid.io.load_vars(\n",
    "            exe, pretrained_model, main_program=val_prog, predicate=if_exist)\n",
    "    \n",
    "    positive_ratio = 1. / (1. - threshold)\n",
    "    negative_ratio = 1. / threshold\n",
    "    \n",
    "    result = {}\n",
    "    for tid, data in enumerate(val_dataloader()):\n",
    "        file_names, part_splits, val_datas = [],[],[]\n",
    "        for item in data:\n",
    "            file_names.append(item[0])\n",
    "            part_splits.append(item[1])\n",
    "            val_datas.append(item[2])\n",
    "        \n",
    "        batch_preds, = exe.run(\n",
    "           program=val_prog,\n",
    "           feed={\"pixel\":np.array(val_datas)},\n",
    "           fetch_list=[val_output],\n",
    "           use_program_cache=True)\n",
    "\n",
    "        for file, part, pred in zip(file_names, part_splits, batch_preds[:,0]):\n",
    "            if pred >= threshold:\n",
    "                threshold_pred = (pred-threshold) * positive_ratio\n",
    "            else:\n",
    "                threshold_pred = (pred-threshold) * negative_ratio\n",
    "            if file not in result.keys():\n",
    "                result[file] = [0, 0]\n",
    "            result[file][part] = threshold_pred\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = inference(True, \"../weights/classify_weights_best/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, \"w+\") as f:\n",
    "    f.write(\"{},{},{}\\n\".format(\"ASOCT_NAME\", \"LEFT_ANGLE_RESULTS\", \"RIGHT_ANGLE_RESULTS\"))\n",
    "    for file, pred_labels in result.items():\n",
    "        f.write(\"{},{},{}\\n\".format(file.split(\"/\")[-1], *pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
