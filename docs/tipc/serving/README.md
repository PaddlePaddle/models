# Linux GPU/CPU 服务化部署开发规范

# 目录

- [1. 简介](#1)
- [2. 服务化部署开发规范](#2)
    - [2.1 开发流程](#2.1)
    - [2.2 核验点](#2.2)
- [3. 服务化部署测试开发规范](#3)
    - [3.1 开发流程](#3.1)
    - [3.2 核验点](#3.2)

<a name="1"></a>

## 1. 简介

本文档主要介绍飞桨模型在 Linux GPU/CPU 下服务化部署能力的开发规范。主要包含2个步骤：

（1） 参考[《Linux GPU/CPU 服务化部署开发文档》](./py_serving.md)，在Paddle Inference的模型推理基础上，完成服务化部署能力的开发。

（2） 参考[《Linux GPU/CPU 服务化部署测试开发文档》]()，完成 TIPC 服务化部署测试开发

在此之前，您需要完成下面的内容（如果您已经开发完基于Paddle Inference的模型推理过程，该步骤可以跳过）。

（3） 参考 [《Linux GPU/CPU 基础训练推理开发文档》](../linux_train_infer_python/README.md)，完成模型的训练和基于Paddle Inference的模型推理开发。


<a name="2"></a>

## 2. 服务化部署开发规范

<a name="2.1"></a>

### 2.1 开发流程

服务化部署开发流程如下图所示。

<div align="center">
    <img src="./images/py_serving_deploy_pipeline.jpg" width="800">
</div>

更多内容请参考：[服务化部署开发文档](./py_serving.md)。

<a name="2.2"></a>

### 2.2 核验点

在开发过程中，至少需要产出下面的内容。

#### 2.2.1 模型预测服务启动

* 可以成功启动模型预测服务，并在客户端完成访问。

#### 2.2.2 服务化部署结果正确性

* 返回结果与基于Paddle Inference的模型推理结果完全一致。

#### 2.2.3 说明文档

* 文档中给出服务化部署的数据、环境准备、启动服务、访问服务的具体命令说明。

<a name="3"></a>

## 3. 服务化部署测试开发规范

<a name="3.1"></a>

### 3.1 开发流程

服务化部署测试开发流程如下所示。

coming soon!

更多的介绍可以参考：[服务化部署测试开发文档](./test_serving.md)。

<a name="3.2"></a>

### 3.2 核验点
