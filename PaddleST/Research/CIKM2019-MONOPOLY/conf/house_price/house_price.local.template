[DEFAULT]
sample_seed: 1234
# The value in `DEFAULT` section will be referenced by other sections.
# For convinence, we will put the variables which changes frequently here and 
# let other section refer them

# Input settings
dataset_name: HousePrice
max_house_num: 100
max_public_num: 100

batch_shuffle: False
CUDA_VISIBLE_DEVICES: 0
FLAGS_fraction_of_gpu_memory_to_use: 0.8
# Input settings
#reader: dataset |  pyreader | async | datafeed | sync
#data_reader: pyreader
data_reader: datafeed
dataset_mode: Memory
#local-cpu | local-gpu 
platform: local-gpu
#platform: local-cpu
dis_radius: 1.0
avg_eval: False
with_car_dis: False
with_house_attr: False

bj_batch_size: 5256
#bj_batch_size: 7573
#bj_batch_size: 423
sh_batch_size: 8126
#sh_batch_size: 11604
#sh_batch_size: 822
gz_batch_size: 4560
#gz_batch_size: 6508
#gz_batch_size: 367
sz_batch_size: 2693
#sz_batch_size: 3849
#sz_batch_size: 192

city_name: <c>
num_samples_train: ${DEFAULT:<c>_batch_size}
train_batch_size: ${DEFAULT:<c>_batch_size}
#train_batch_size: 2
num_samples_eval: 10
eval_batch_size: 10

kv_path: None

# Model settings
model_name: HousePrice
preprocessing_name: None 
file_pattern: part-
num_in_dimension: 3
num_out_dimension: 1

# Learning options
max_number_of_steps: None
init_learning_rate: 0.2
emb_lr: ${DEFAULT:init_learning_rate}
fc_lr: ${DEFAULT:init_learning_rate}
base_lr: ${DEFAULT:init_learning_rate}

[Convert]
# The name of the dataset to convert
dataset_name: ${DEFAULT:dataset_name}

#dataset_dir: ${DEFAULT:dataset_dir}
dataset_dir: stream

# The output Records file name prefix.
dataset_split_name: train

# The number of Records per shard
num_per_shard: 100000

# The dimensions of net input vectors, it is just used by svm dataset
# which of input are sparse tensors now
num_in_dimension: ${DEFAULT:num_in_dimension}

# The output file name pattern with two placeholders ("%s" and "%d"), 
# it must correspond to the glob `file_pattern' in Train and Evaluate
# config sections


[Train]
#######################
#  Dataset Configure  #
#######################
# The name of the dataset to load
dataset_name: ${DEFAULT:dataset_name}

# The directory where the dataset files are stored
dataset_dir:  ${DEFAULT:dataset_dir}

file_list: ../tmp/data/poi/raw/<c>/poi_sample.train

# dataset_split_name
dataset_split_name: train

# The glob pattern for data path, `file_pattern' must contain only one "%s" 
# which is the placeholder for split name (such as 'train', 'validation')
file_pattern: ${DEFAULT:file_pattern}

# The file type text or record
file_type: record

# kv path, used in image_sim
kv_path: ${DEFAULT:kv_path}

# The number of input sample for training
num_samples: ${DEFAULT:num_samples_train}

# The number of parallel readers that read data from the dataset
num_readers: 2

# The number of threads used to create the batches
num_preprocessing_threads: 4

# Number of epochs from dataset source
num_epochs_input: 200

###########################
#  Basic Train Configure  #
###########################
# Directory where checkpoints and event logs are written to.
train_dir: ../tmp/model/house_price/save_model/${DEFAULT:city_name}

# The max number of ckpt files to store variables
save_max_to_keep: 40

# The frequency with which the model is saved, in steps.
save_model_steps: 5

# The name of the architecture to train
model_name: ${DEFAULT:model_name}

# The dimensions of net input vectors, it is just used by svm dataset
# which of input are sparse tensors now
num_in_dimension: ${DEFAULT:num_in_dimension}

# The dimensions of net output vector, it will be num of classes in image classify task 
num_out_dimension: ${DEFAULT:num_out_dimension}

#####################################
#  Training Optimization Configure  #
#####################################
# The number of samples in each batch
batch_size: ${DEFAULT:train_batch_size}

# The maximum number of training steps
max_number_of_steps: ${DEFAULT:max_number_of_steps}

# The weight decay on the model weights
#weight_decay: 0.00000001
weight_decay: None

# The decay to use for the moving average. If left as None, then moving averages are not used
moving_average_decay: None

# ***************** learning rate options ***************** #
# Initial learning rate
init_learning_rate: ${DEFAULT:init_learning_rate}

# Specifies how the learning rate is decayed. One of "fixed", "exponential" or "polynomial"
learning_rate_decay_type: fixed 

# Learning rate decay factor
learning_rate_decay_factor: 0.1

num_learning_rate_warmup_epochs: None

# The minimal end learning rate used by a polynomial decay learning rate
end_learning_rate: 0.0001

# Number of epochs after which learning rate decays
num_epochs_per_decay: 10

# A boolean, whether or not it should cycle beyond decay_steps
learning_rate_polynomial_decay_cycle: False

# ******************* optimizer options ******************* #
# The name of the optimizer, one of the following:
# "adadelta", "adagrad", "adam", "ftrl", "momentum", "sgd" or "rmsprop"
#optimizer: weight_decay_adam
optimizer: adam
#optimizer: sgd
# Epsilon term for the optimizer, used for adadelta, adam, rmsprop
opt_epsilon: 1e-6

# conf for adadelta
# The decay rate for adadelta
adadelta_rho: 0.95
# Starting value for the AdaGrad accumulators
adagrad_initial_accumulator_value: 0.1

# conf for adam
# The exponential decay rate for the 1st moment estimates
adam_beta1: 0.9
# The exponential decay rate for the 2nd moment estimates
adam_beta2: 0.999

adam_weight_decay: 0.01
#adam_exclude_from_weight_decay: LayerNorm,layer_norm,bias

# conf for ftrl
# The learning rate power
ftrl_learning_rate_power: -0.1
# Starting value for the FTRL accumulators
ftrl_initial_accumulator_value: 0.1
# The FTRL l1 regularization strength
ftrl_l1: 0.0
# The FTRL l2 regularization strength
ftrl_l2: 0.01

# conf for momentum
# The momentum for the MomentumOptimizer and RMSPropOptimizer
momentum: 0.9

# conf for rmsprop
# Decay term for RMSProp
rmsprop_decay: 0.9

# Number of model clones to deploy
num_gpus: 1

# The frequency with which logs are trace.
trace_every_n_steps: 5


[Evaluate]
#######################
#  Dataset Configure  #
#######################
# The name of the dataset to load
dataset_name: ${DEFAULT:dataset_name}

# The name of the train/test split
#dataset_split_name: validation
dataset_split_name: train

# The glob pattern for data path, `file_pattern' must contain only one "%s" 
# which is the placeholder for split name (such as 'train', 'validation')
file_pattern: ${DEFAULT:file_pattern}

#reader: dataset |  pyreader | async | datafeed | sync
data_reader: datafeed

#local-cpu | local-gpu 
platform: local-cpu

file_list: ../tmp/data/poi/raw/<c>/poi_sample.test

# The file type or record
file_type: text

# kv path, used in image_sim
kv_path: ${DEFAULT:kv_path}

# The number of input sample for evaluation
num_samples: ${DEFAULT:num_samples_eval}

# The number of parallel readers that read data from the dataset
num_readers: 2

# The number of threads used to create the batches
num_preprocessing_threads: 2

# Number of epochs from dataset source
num_epochs_input: 1

# The name of the architecture to evaluate
model_name: ${DEFAULT:model_name}

# The dimensions of net input vectors, it is just used by svm dataset
# which of input are sparse tensors now
num_in_dimension: ${DEFAULT:num_in_dimension}

# The dimensions of net output vector, it will be num of classes in image classify task 
num_out_dimension: ${DEFAULT:num_out_dimension}

# Directory where the results are saved to
eval_dir: ${Train:train_dir}/checkpoint_1

# The number of samples in each batch
batch_size: ${DEFAULT:eval_batch_size}
