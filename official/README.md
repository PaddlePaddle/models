<table border="1">
    <tr>
        <th>序号</th>
        <th>模型简称</th>
        <th>论文名称(链接)</th>
        <th>摘要</th>
        <th>数据集</th>
        <th width="10%">快速开始</th>
        <th>支持 TIPC</th>
    </tr>
    <tr>
        <td>1</td>
        <td>PP-ShiTu_mainbody_det</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=ShiTu">PP-ShiTu: A Practical Lightweight Image Recognition System</a></td>
        <td><details><summary>Abstract</summary><div>In recent years, image recognition applications have developed rapidly. A large number of studies and techniques have emerged in different fields, such as face recognition, pedestrian and vehicle re-identification, landmark retrieval, and product recognition. In this paper, we propose a practical lightweight image recognition system, named PP-ShiTu, consisting of the following 3 modules, mainbody detection, feature extraction and vector search. We introduce popular strategies including metric learning, deep hash, knowledge distillation and model quantization to improve accuracy and inference speed. With strategies above, PP-ShiTu works well in different scenarios with a set of models trained on a mixed dataset. Experiments on different datasets and benchmarks show that the system is widely effective in different domains of image recognition. All the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleClas on PaddlePaddle</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/quick_start/quick_start_recognition.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>2</td>
        <td>PP-ShiTu_general_rec</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=ShiTu">PP-ShiTu: A Practical Lightweight Image Recognition System</a></td>
        <td><details><summary>Abstract</summary><div>In recent years, image recognition applications have developed rapidly. A large number of studies and techniques have emerged in different fields, such as face recognition, pedestrian and vehicle re-identification, landmark retrieval, and product recognition. In this paper, we propose a practical lightweight image recognition system, named PP-ShiTu, consisting of the following 3 modules, mainbody detection, feature extraction and vector search. We introduce popular strategies including metric learning, deep hash, knowledge distillation and model quantization to improve accuracy and inference speed. With strategies above, PP-ShiTu works well in different scenarios with a set of models trained on a mixed dataset. Experiments on different datasets and benchmarks show that the system is widely effective in different domains of image recognition. All the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleClas on PaddlePaddle</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/quick_start/quick_start_recognition.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>3</td>
        <td>PPLCNet_x0_25</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet">PP-LCNet: A Lightweight CPU Convolutional Neural Networ</a></td>
        <td><details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>4</td>
        <td>PPLCNet_x0_35</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet">PP-LCNet: A Lightweight CPU Convolutional Neural Networ</a></td>
        <td><details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>5</td>
        <td>PPLCNet_x0_5</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet">PP-LCNet: A Lightweight CPU Convolutional Neural Networ</a></td>
        <td><details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>6</td>
        <td>PPLCNet_x0_75</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet">PP-LCNet: A Lightweight CPU Convolutional Neural Networ</a></td>
        <td><details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>7</td>
        <td>PPLCNet_x1_0</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet">PP-LCNet: A Lightweight CPU Convolutional Neural Networ</a></td>
        <td><details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>8</td>
        <td>PPLCNet_x1_5</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet">PP-LCNet: A Lightweight CPU Convolutional Neural Networ</a></td>
        <td><details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>9</td>
        <td>PPLCNet_x2_0</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet">PP-LCNet: A Lightweight CPU Convolutional Neural Networ</a></td>
        <td><details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>10</td>
        <td>PPLCNet_x2_5</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet">PP-LCNet: A Lightweight CPU Convolutional Neural Networ</a></td>
        <td><details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>11</td>
        <td>SE_ResNeXt50_vd_32x4d</td>
        <td><a href="https://paperswithcode.com/model/seresnext?variant=seresnext50-32x4d">Squeeze-and-Excitation Networks</a></td>
        <td><details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>12</td>
        <td>SE_ResNeXt50_32x4d</td>
        <td><a href="https://paperswithcode.com/model/seresnext">Squeeze-and-Excitation Networks</a></td>
        <td><details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>13</td>
        <td>SE_ResNet18_vd</td>
        <td><a href="https://paperswithcode.com/paper/squeeze-and-excitation-networks">Squeeze-and-Excitation Networks</a></td>
        <td><details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>14</td>
        <td>SE_ResNet34_vd</td>
        <td><a href="https://paperswithcode.com/paper/squeeze-and-excitation-networks">Squeeze-and-Excitation Networks</a></td>
        <td><details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>15</td>
        <td>SE_ResNet50_vd</td>
        <td><a href="https://paperswithcode.com/paper/squeeze-and-excitation-networks">Squeeze-and-Excitation Networks</a></td>
        <td><details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>16</td>
        <td>ResNet50_vd</td>
        <td><a href="https://paperswithcode.com/paper/squeeze-and-excitation-networks">Squeeze-and-Excitation Networks</a></td>
        <td><details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>17</td>
        <td>HRNet_W18_C</td>
        <td><a href="https://paperswithcode.com/method/hrnet">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>18</td>
        <td>HRNet_W30_C</td>
        <td><a href="https://paperswithcode.com/method/hrnet">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>19</td>
        <td>HRNet_W32_C</td>
        <td><a href="https://paperswithcode.com/method/hrnet">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>20</td>
        <td>HRNet_W40_C</td>
        <td><a href="https://paperswithcode.com/method/hrnet">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>21</td>
        <td>HRNet_W44_C</td>
        <td><a href="https://paperswithcode.com/method/hrnet">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>22</td>
        <td>HRNet_W48_C</td>
        <td><a href="https://paperswithcode.com/method/hrnet">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>23</td>
        <td>HRNet_W64_C</td>
        <td><a href="https://paperswithcode.com/method/hrnet">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>24</td>
        <td>SE_ResNeXt101_32x4d</td>
        <td><a href="https://paperswithcode.com/paper/squeeze-and-excitation-networks">Squeeze-and-Excitation Networks</a></td>
        <td><details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL.</div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>25</td>
        <td>SENet154_vd</td>
        <td><a href="https://paperswithcode.com/paper/squeeze-and-excitation-networks">Squeeze-and-Excitation Networks</a></td>
        <td><details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL.</div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>26</td>
        <td>GoogLeNet</td>
        <td><a href="https://paperswithcode.com/model/inception-v4?variant=inception-v4-1">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></td>
        <td><details><summary>Abstract</summary><div>Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>27</td>
        <td>InceptionV3</td>
        <td><a href="https://paperswithcode.com/model/inception-v4?variant=inception-v4-1">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></td>
        <td><details><summary>Abstract</summary><div>Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>28</td>
        <td>InceptionV4</td>
        <td><a href="https://paperswithcode.com/model/inception-v4?variant=inception-v4-1">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></td>
        <td><details><summary>Abstract</summary><div>Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>29</td>
        <td>ResNet18</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>30</td>
        <td>ResNet18_vd</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>31</td>
        <td>ResNet34</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>32</td>
        <td>ResNet34_vd</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>33</td>
        <td>ResNet50</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>34</td>
        <td>ResNet50_vd</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>35</td>
        <td>ResNet50_vd-FPGM</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>36</td>
        <td>ResNet50_vd-PACT</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>37</td>
        <td>ResNet50_vd-KL</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>38</td>
        <td>ResNet101</td>
        <td><a href="https://paperswithcode.com/paper/adaptively-connected-neural-networks">Adaptively Connected Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div> This paper presents a novel adaptively connected neural network (ACNet) to improve the traditional convolutional neural networks (CNNs) {in} two aspects. First, ACNet employs a flexible way to switch global and local inference in processing the internal feature representations by adaptively determining the connection status among the feature nodes (e.g., pixels of the feature maps) \footnote{In a computer vision domain, a node refers to a pixel of a feature map{, while} in {the} graph domain, a node denotes a graph node.}. We can show that existing CNNs, the classical multilayer perceptron (MLP), and the recently proposed non-local network (NLN) \cite{nonlocalnn17} are all special cases of ACNet. Second, ACNet is also capable of handling non-Euclidean data. Extensive experimental analyses on {a variety of benchmarks (i.e.,} ImageNet-1k classification, COCO 2017 detection and segmentation, CUHK03 person re-identification, CIFAR analysis, and Cora document categorization) demonstrate that {ACNet} cannot only achieve state-of-the-art performance but also overcome the limitation of the conventional MLP and CNN \footnote{Corresponding author: Liang Lin (linliang@ieee.org)}. The code is available at \url{this https URL}. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>39</td>
        <td>ResNet101_vd</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>40</td>
        <td>ResNet152</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>41</td>
        <td>ResNet152_vd</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>42</td>
        <td>ResNet200_vd</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>43</td>
        <td>Res2Net50_26w_4s</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>44</td>
        <td>Res2Net50_14w_8s</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>45</td>
        <td>Res2Net50_vd_26w_4s</td>
        <td><a href="https://paperswithcode.com/model/resnet">Deep Residual Learning for Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>46</td>
        <td>Res2Net101_vd_26w_4s</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>47</td>
        <td>Res2Net200_vd_26w_4s</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>48</td>
        <td>ResNeXt50_32x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>49</td>
        <td>ResNeXt50_64x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>50</td>
        <td>ResNeXt50_vd_32x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>51</td>
        <td>ResNeXt50_vd_64x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>52</td>
        <td>ResNeXt101_32x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>53</td>
        <td>ResNeXt101_64x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>54</td>
        <td>ResNeXt101_vd_32x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>55</td>
        <td>ResNeXt101_vd_64x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>56</td>
        <td>ResNeXt152_32x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>57</td>
        <td>ResNeXt152_64x4d</td>
        <td><a href="﻿﻿﻿https://paperswithcode.com/model/res2net">Res2Net: A New Multi-scale Backbone Architecture</a></td>
        <td><details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>58</td>
        <td>ResNeXt152_vd_32x4d</td>
        <td><a href="https://paperswithcode.com/model/resnext">Aggregated Residual Transformations for Deep Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>59</td>
        <td>ResNeXt152_vd_64x4d</td>
        <td><a href="https://paperswithcode.com/model/resnext">Aggregated Residual Transformations for Deep Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>60</td>
        <td>DenseNet121</td>
        <td><a href="https://paperswithcode.com/model/resnext">Aggregated Residual Transformations for Deep Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>61</td>
        <td>DenseNet161</td>
        <td><a href="https://paperswithcode.com/paper/densely-connected-convolutional-networks">Densely Connected Convolutional Networks</a></td>
        <td><details><summary>Abstract</summary><div>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL . </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>62</td>
        <td>DenseNet169</td>
        <td><a href="https://paperswithcode.com/paper/densely-connected-convolutional-networks">Densely Connected Convolutional Networks</a></td>
        <td><details><summary>Abstract</summary><div>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL . </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>63</td>
        <td>DenseNet201</td>
        <td><a href="https://paperswithcode.com/paper/densely-connected-convolutional-networks">Densely Connected Convolutional Networks</a></td>
        <td><details><summary>Abstract</summary><div>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL . </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>64</td>
        <td>DenseNet264</td>
        <td><a href="https://paperswithcode.com/paper/densely-connected-convolutional-networks">Densely Connected Convolutional Networks</a></td>
        <td><details><summary>Abstract</summary><div>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL . </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>65</td>
        <td>DPN68</td>
        <td><a href="https://paperswithcode.com/paper/dual-path-networks">Dual Path Networks</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>66</td>
        <td>DPN92</td>
        <td><a href="https://paperswithcode.com/paper/dual-path-networks">Dual Path Networks</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>67</td>
        <td>DPN98</td>
        <td><a href="https://paperswithcode.com/paper/dual-path-networks">Dual Path Networks</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>68</td>
        <td>DPN107</td>
        <td><a href="https://paperswithcode.com/paper/dual-path-networks">Dual Path Networks</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>69</td>
        <td>DPN131</td>
        <td><a href="https://paperswithcode.com/paper/dual-path-networks">Dual Path Networks</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>70</td>
        <td>VGG11</td>
        <td><a href="https://paperswithcode.com/model/vgg"></a></td>
        <td><details><summary>Abstract</summary><div>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>71</td>
        <td>VGG13</td>
        <td><a href="https://paperswithcode.com/model/vgg"></a></td>
        <td><details><summary>Abstract</summary><div>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>72</td>
        <td>VGG16</td>
        <td><a href="https://paperswithcode.com/model/vgg"></a></td>
        <td><details><summary>Abstract</summary><div>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>73</td>
        <td>VGG19</td>
        <td><a href="https://paperswithcode.com/model/vgg"></a></td>
        <td><details><summary>Abstract</summary><div>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>74</td>
        <td>AlexNet</td>
        <td><a href="https://paperswithcode.com/model/alexnet">ImageNet Classification with Deep Convolutional Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry</div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>75</td>
        <td>Xception41</td>
        <td><a href="https://paperswithcode.com/model/xception?variant=xception-1">Xception: Deep Learning with Depthwise Separable Convolutions</a></td>
        <td><details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>76</td>
        <td>Xception65</td>
        <td><a href="https://paperswithcode.com/model/xception?variant=xception-1">Xception: Deep Learning with Depthwise Separable Convolutions</a></td>
        <td><details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>77</td>
        <td>Xception71</td>
        <td><a href="https://paperswithcode.com/model/xception?variant=xception-1">Xception: Deep Learning with Depthwise Separable Convolutions</a></td>
        <td><details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>78</td>
        <td>Xception41_deeplab</td>
        <td><a href="https://paperswithcode.com/model/xception?variant=xception-1">Xception: Deep Learning with Depthwise Separable Convolutions</a></td>
        <td><details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>79</td>
        <td>Xception65_deeplab</td>
        <td><a href="https://paperswithcode.com/model/xception?variant=xception-1">Xception: Deep Learning with Depthwise Separable Convolutions</a></td>
        <td><details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>80</td>
        <td>DarkNet53</td>
        <td><a href="https://paperswithcode.com/method/darknet-53">YOLOv3: An Incremental Improvement</a></td>
        <td><details><summary>Abstract</summary><div>We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL</div></details></td>
        <td>ImageNet </td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>81</td>
        <td>EfficientNetB0</td>
        <td><a href="https://paperswithcode.com/method/efficientnet">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>82</td>
        <td>EfficientNetB1</td>
        <td><a href="https://paperswithcode.com/method/efficientnet">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>83</td>
        <td>EfficientNetB2</td>
        <td><a href="https://paperswithcode.com/method/efficientnet">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>84</td>
        <td>EfficientNetB3</td>
        <td><a href="https://paperswithcode.com/method/efficientnet">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>85</td>
        <td>EfficientNetB4</td>
        <td><a href="https://paperswithcode.com/method/efficientnet">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>86</td>
        <td>EfficientNetB5</td>
        <td><a href="https://paperswithcode.com/method/efficientnet">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>87</td>
        <td>EfficientNetB6</td>
        <td><a href="https://paperswithcode.com/method/efficientnet">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>88</td>
        <td>EfficientNetB7</td>
        <td><a href="https://paperswithcode.com/method/efficientnet">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>89</td>
        <td>SqueezeNet1_0</td>
        <td><a href="https://paperswithcode.com/method/squeezenet">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size</a></td>
        <td><details><summary>Abstract</summary><div>Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).The SqueezeNet architecture is available for download here: this https URL</div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>90</td>
        <td>SqueezeNet1_1</td>
        <td><a href="https://paperswithcode.com/method/squeezenet">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size</a></td>
        <td><details><summary>Abstract</summary><div>Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).The SqueezeNet architecture is available for download here: this https URL</div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>91</td>
        <td>MobileNetV1</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv1">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td>
        <td><details><summary>Abstract</summary><div> We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>92</td>
        <td>MobileNetV1_x0_25</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv1">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td>
        <td><details><summary>Abstract</summary><div> We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>93</td>
        <td>MobileNetV1_x0_5</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv1">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td>
        <td><details><summary>Abstract</summary><div> We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>94</td>
        <td>MobileNetV1_x0_75</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv1">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></td>
        <td><details><summary>Abstract</summary><div> We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>95</td>
        <td>MobileNetV2</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv2">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></td>
        <td><details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>96</td>
        <td>MobileNetV2_x0_25</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv2">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></td>
        <td><details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>97</td>
        <td>MobileNetV2_x0_5</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv2">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></td>
        <td><details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>98</td>
        <td>MobileNetV2_x0_75</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv2">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></td>
        <td><details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>99</td>
        <td>MobileNetV2_x1_5</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv2">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></td>
        <td><details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>100</td>
        <td>MobileNetV2_x2_0</td>
        <td><a href="https://paperswithcode.com/method/mobilenetv2">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></td>
        <td><details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>101</td>
        <td>MobileNetV3_large_x0_</br>35</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>102</td>
        <td>MobileNetV3_large_x0_</br>5</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>103</td>
        <td>MobileNetV3_large_x0_</br>75</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>104</td>
        <td>MobileNetV3_large_x1_</br>0</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>105</td>
        <td>MobileNetV3_large_x1_</br>0-FPGM</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv4">Searching for MobileNetV4</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>106</td>
        <td>MobileNetV3_large_x1_</br>0-PACT</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv5">Searching for MobileNetV5</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>107</td>
        <td>MobileNetV3_large_x1_</br>0-KL</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv6">Searching for MobileNetV6</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>108</td>
        <td>MobileNetV3_large_x1_</br>25</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>109</td>
        <td>MobileNetV3_small_x0_</br>35</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>110</td>
        <td>MobileNetV3_small_x0_</br>5</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>111</td>
        <td>MobileNetV3_small_x0_</br>75</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>112</td>
        <td>MobileNetV3_small_x1_</br>0</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>113</td>
        <td>MobileNetV3_small_x1_</br>25</td>
        <td><a href="https://paperswithcode.com/paper/searching-for-mobilenetv3">Searching for MobileNetV3</a></td>
        <td><details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>114</td>
        <td>ShuffleNetV2_swish</td>
        <td><a href="https://paperswithcode.com/method/shufflenet-v2">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></td>
        <td><details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>115</td>
        <td>ShuffleNetV2_x0_25</td>
        <td><a href="https://paperswithcode.com/method/shufflenet-v2">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></td>
        <td><details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>116</td>
        <td>ShuffleNetV2_x0_33</td>
        <td><a href="https://paperswithcode.com/method/shufflenet-v2">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></td>
        <td><details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>117</td>
        <td>ShuffleNetV2_x0_5</td>
        <td><a href="https://paperswithcode.com/method/shufflenet-v2">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></td>
        <td><details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>118</td>
        <td>ShuffleNetV2_x1_0</td>
        <td><a href="https://paperswithcode.com/method/shufflenet-v2">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></td>
        <td><details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>119</td>
        <td>ShuffleNetV2_x1_5</td>
        <td><a href="https://paperswithcode.com/method/shufflenet-v2">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></td>
        <td><details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>120</td>
        <td>ShuffleNetV2_x2_0</td>
        <td><a href="https://paperswithcode.com/method/shufflenet-v2">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></td>
        <td><details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>121</td>
        <td>CSPDarkNet53</td>
        <td><a href="https://paperswithcode.com/model/csp-resnet?variant=cspresnet50">CSPNet: A New Backbone that can Enhance Learning Capability of CNN</a></td>
        <td><details><summary>Abstract</summary><div>Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at this https URL. </div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>122</td>
        <td>GhostNet_x0_5</td>
        <td><a href="https://paperswithcode.com/method/ghostnet">GhostNet: More Features from Cheap Operations</a></td>
        <td><details><summary>Abstract</summary><div>Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at this https URL</div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>123</td>
        <td>GhostNet_x1_0</td>
        <td><a href="https://paperswithcode.com/method/ghostnet">GhostNet: More Features from Cheap Operations</a></td>
        <td><details><summary>Abstract</summary><div>Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at this https URL</div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>124</td>
        <td>GhostNet_x1_3</td>
        <td><a href="https://paperswithcode.com/method/ghostnet">GhostNet: More Features from Cheap Operations</a></td>
        <td><details><summary>Abstract</summary><div>Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at this https URL</div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>125</td>
        <td>RegNet</td>
        <td><a href="https://paperswithcode.com/paper/regnet-self-regulated-network-for-image">RegNet: Self-Regulated Network for Image Classification</a></td>
        <td><details><summary>Abstract</summary><div>The ResNet and its variants have achieved remarkable successes in various computer vision tasks. Despite its success in making gradient flow through building blocks, the simple shortcut connection mechanism limits the ability of re-exploring new potentially complementary features due to the additive function. To address this issue, in this paper, we propose to introduce a regulator module as a memory mechanism to extract complementary features, which are further fed to the ResNet. In particular, the regulator module is composed of convolutional RNNs (e.g., Convolutional LSTMs or Convolutional GRUs), which are shown to be good at extracting Spatio-temporal information. We named the new regulated networks as RegNet. The regulator module can be easily implemented and appended to any ResNet architecture. We also apply the regulator module for improving the Squeeze-and-Excitation ResNet to show the generalization ability of our method. Experimental results on three image classification datasets have demonstrated the promising performance of the proposed architecture compared with the standard ResNet, SE-ResNet, and other state-of-the-art architectures.</div></details></td>
        <td>ImageNet</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>126</td>
        <td>DLA169</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>127</td>
        <td>DLA60x_c</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>128</td>
        <td>DLA102x2</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>129</td>
        <td>DLA102</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>130</td>
        <td>DLA60x</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>131</td>
        <td>DLA60</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>132</td>
        <td>DLA46_c</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>133</td>
        <td>DLA34</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>134</td>
        <td>DLA102x</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>135</td>
        <td>DLA46x_c</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Layer+Aggregation">Deep Layer Aggregation</a></td>
        <td><details><summary>Abstract</summary><div>Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>136</td>
        <td>ReXNet_1_5</td>
        <td><a href="https://paperswithcode.com/paper/rexnet-diminishing-representational#code">Rethinking Channel Dimensions for Efficient Model Design</a></td>
        <td><details><summary>Abstract</summary><div>Designing an efficient model within the limited computational cost is challenging. We argue the accuracy of a lightweight model has been further limited by the design convention: a stage-wise configuration of the channel dimensions, which looks like a piecewise linear function of the network stage. In this paper, we study an effective channel dimension configuration towards better performance than the convention. To this end, we empirically study how to design a single layer properly by analyzing the rank of the output feature. We then investigate the channel configuration of a model by searching network architectures concerning the channel configuration under the computational cost restriction. Based on the investigation, we propose a simple yet effective channel configuration that can be parameterized by the layer index. As a result, our proposed model following the channel parameterization achieves remarkable performance on ImageNet classification and transfer learning tasks including COCO object detection, COCO instance segmentation, and fine-grained classifications. Code and ImageNet pretrained models are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>137</td>
        <td>ReXNet_1_0</td>
        <td><a href="https://paperswithcode.com/paper/rexnet-diminishing-representational#code">Rethinking Channel Dimensions for Efficient Model Design</a></td>
        <td><details><summary>Abstract</summary><div>Designing an efficient model within the limited computational cost is challenging. We argue the accuracy of a lightweight model has been further limited by the design convention: a stage-wise configuration of the channel dimensions, which looks like a piecewise linear function of the network stage. In this paper, we study an effective channel dimension configuration towards better performance than the convention. To this end, we empirically study how to design a single layer properly by analyzing the rank of the output feature. We then investigate the channel configuration of a model by searching network architectures concerning the channel configuration under the computational cost restriction. Based on the investigation, we propose a simple yet effective channel configuration that can be parameterized by the layer index. As a result, our proposed model following the channel parameterization achieves remarkable performance on ImageNet classification and transfer learning tasks including COCO object detection, COCO instance segmentation, and fine-grained classifications. Code and ImageNet pretrained models are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>138</td>
        <td>ReXNet_3_0</td>
        <td><a href="https://paperswithcode.com/paper/rexnet-diminishing-representational#code">Rethinking Channel Dimensions for Efficient Model Design</a></td>
        <td><details><summary>Abstract</summary><div>Designing an efficient model within the limited computational cost is challenging. We argue the accuracy of a lightweight model has been further limited by the design convention: a stage-wise configuration of the channel dimensions, which looks like a piecewise linear function of the network stage. In this paper, we study an effective channel dimension configuration towards better performance than the convention. To this end, we empirically study how to design a single layer properly by analyzing the rank of the output feature. We then investigate the channel configuration of a model by searching network architectures concerning the channel configuration under the computational cost restriction. Based on the investigation, we propose a simple yet effective channel configuration that can be parameterized by the layer index. As a result, our proposed model following the channel parameterization achieves remarkable performance on ImageNet classification and transfer learning tasks including COCO object detection, COCO instance segmentation, and fine-grained classifications. Code and ImageNet pretrained models are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>139</td>
        <td>ReXNet_2_0</td>
        <td><a href="https://paperswithcode.com/paper/rexnet-diminishing-representational#code">Rethinking Channel Dimensions for Efficient Model Design</a></td>
        <td><details><summary>Abstract</summary><div>Designing an efficient model within the limited computational cost is challenging. We argue the accuracy of a lightweight model has been further limited by the design convention: a stage-wise configuration of the channel dimensions, which looks like a piecewise linear function of the network stage. In this paper, we study an effective channel dimension configuration towards better performance than the convention. To this end, we empirically study how to design a single layer properly by analyzing the rank of the output feature. We then investigate the channel configuration of a model by searching network architectures concerning the channel configuration under the computational cost restriction. Based on the investigation, we propose a simple yet effective channel configuration that can be parameterized by the layer index. As a result, our proposed model following the channel parameterization achieves remarkable performance on ImageNet classification and transfer learning tasks including COCO object detection, COCO instance segmentation, and fine-grained classifications. Code and ImageNet pretrained models are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>140</td>
        <td>ReXNet_1_3</td>
        <td><a href="https://paperswithcode.com/paper/rexnet-diminishing-representational#code">Rethinking Channel Dimensions for Efficient Model Design</a></td>
        <td><details><summary>Abstract</summary><div>Designing an efficient model within the limited computational cost is challenging. We argue the accuracy of a lightweight model has been further limited by the design convention: a stage-wise configuration of the channel dimensions, which looks like a piecewise linear function of the network stage. In this paper, we study an effective channel dimension configuration towards better performance than the convention. To this end, we empirically study how to design a single layer properly by analyzing the rank of the output feature. We then investigate the channel configuration of a model by searching network architectures concerning the channel configuration under the computational cost restriction. Based on the investigation, we propose a simple yet effective channel configuration that can be parameterized by the layer index. As a result, our proposed model following the channel parameterization achieves remarkable performance on ImageNet classification and transfer learning tasks including COCO object detection, COCO instance segmentation, and fine-grained classifications. Code and ImageNet pretrained models are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>141</td>
        <td>TNT_small</td>
        <td><a href="https://paperswithcode.com/method/tnt">Transformer in Transformer</a></td>
        <td><details><summary>Abstract</summary><div>Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16×16) as "visual sentences" and present to further divide them into smaller patches (e.g., 4×4) as "visual words". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at this https URL, and the MindSpore code is available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>142</td>
        <td>MixNet_L</td>
        <td><a href="https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet">MixConv: Mixed Depthwise Convolutional Kernels</a></td>
        <td><details><summary>Abstract</summary><div>     Depthwise convolution is becoming increasingly popular in modern efficient ConvNets, but its kernel size is often overlooked. In this paper, we systematically study the impact of different kernel sizes, and observe that combining the benefits of multiple kernel sizes can lead to better accuracy and efficiency. Based on this observation, we propose a new mixed depthwise convolution (MixConv), which naturally mixes up multiple kernel sizes in a single convolution. As a simple drop-in replacement of vanilla depthwise convolution, our MixConv improves the accuracy and efficiency for existing MobileNets on both ImageNet classification and COCO object detection. To demonstrate the effectiveness of MixConv, we integrate it into AutoML search space and develop a new family of models, named as MixNets, which outperform previous mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy +4.2%), ShuffleNetV2 [16] (+3.5%), MnasNet [26] (+1.3%), ProxylessNAS [2] (+2.2%), and FBNet [27] (+2.0%). In particular, our MixNet-L achieves a new state-of-the-art 78.9% ImageNet top-1 accuracy under typical mobile settings (<600M FLOPS). Code is at this https URL tensorflow/tpu/tree/master/models/official/mnasnet/mixnet </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>143</td>
        <td>MixNet_S</td>
        <td><a href="https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet">MixConv: Mixed Depthwise Convolutional Kernels</a></td>
        <td><details><summary>Abstract</summary><div>     Depthwise convolution is becoming increasingly popular in modern efficient ConvNets, but its kernel size is often overlooked. In this paper, we systematically study the impact of different kernel sizes, and observe that combining the benefits of multiple kernel sizes can lead to better accuracy and efficiency. Based on this observation, we propose a new mixed depthwise convolution (MixConv), which naturally mixes up multiple kernel sizes in a single convolution. As a simple drop-in replacement of vanilla depthwise convolution, our MixConv improves the accuracy and efficiency for existing MobileNets on both ImageNet classification and COCO object detection. To demonstrate the effectiveness of MixConv, we integrate it into AutoML search space and develop a new family of models, named as MixNets, which outperform previous mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy +4.2%), ShuffleNetV2 [16] (+3.5%), MnasNet [26] (+1.3%), ProxylessNAS [2] (+2.2%), and FBNet [27] (+2.0%). In particular, our MixNet-L achieves a new state-of-the-art 78.9% ImageNet top-1 accuracy under typical mobile settings (<600M FLOPS). Code is at this https URL tensorflow/tpu/tree/master/models/official/mnasnet/mixnet </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>144</td>
        <td>MixNet_M</td>
        <td><a href="https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet">MixConv: Mixed Depthwise Convolutional Kernels</a></td>
        <td><details><summary>Abstract</summary><div>     Depthwise convolution is becoming increasingly popular in modern efficient ConvNets, but its kernel size is often overlooked. In this paper, we systematically study the impact of different kernel sizes, and observe that combining the benefits of multiple kernel sizes can lead to better accuracy and efficiency. Based on this observation, we propose a new mixed depthwise convolution (MixConv), which naturally mixes up multiple kernel sizes in a single convolution. As a simple drop-in replacement of vanilla depthwise convolution, our MixConv improves the accuracy and efficiency for existing MobileNets on both ImageNet classification and COCO object detection. To demonstrate the effectiveness of MixConv, we integrate it into AutoML search space and develop a new family of models, named as MixNets, which outperform previous mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy +4.2%), ShuffleNetV2 [16] (+3.5%), MnasNet [26] (+1.3%), ProxylessNAS [2] (+2.2%), and FBNet [27] (+2.0%). In particular, our MixNet-L achieves a new state-of-the-art 78.9% ImageNet top-1 accuracy under typical mobile settings (<600M FLOPS). Code is at this https URL tensorflow/tpu/tree/master/models/official/mnasnet/mixnet </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>145</td>
        <td>ResNeSt50</td>
        <td><a href="https://github.com/zhanghang1989/ResNeSt">ResNeSt: Split-Attention Networks</a></td>
        <td><details><summary>Abstract</summary><div>While image classification models have recently continuedto advance, most downstream applications such as object detection andsemantic segmentation still employ ResNet variants as the backbone net-work due to their simple and modular structure. We present a modularSplit-Attention block that enables attention across feature-map groups.By stacking these Split-Attention blocks ResNet-style, we obtain a newResNet variant which we call ResNeSt. Our network preserves the over-all ResNet structure to be used in downstream tasks straightforwardlywithout introducing additional computational costs.ResNeSt models outperform other networks with similar model com-plexities. For example, ResNeSt-50 achieves 81.13% top-1 accuracy onImageNet using a single crop-size of 224 × 224, outperforming previ-ous best ResNet variant by more than 1% accuracy. This improvementalso helps downstream tasks including object detection, instance segmen-tation and semantic segmentation. For example, by simply replace theResNet-50 backbone with ResNeSt-50, we improve the mAP of Faster-RCNN on MS-COCO from 39.3% to 42.3% and the mIoU for DeeplabV3on ADE20K from 42.1% to 45.1%1</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>146</td>
        <td>ResNeSt50_fast_1s1x64</br>d</td>
        <td><a href="https://github.com/zhanghang1989/ResNeSt">ResNeSt: Split-Attention Networks</a></td>
        <td><details><summary>Abstract</summary><div>While image classification models have recently continuedto advance, most downstream applications such as object detection andsemantic segmentation still employ ResNet variants as the backbone net-work due to their simple and modular structure. We present a modularSplit-Attention block that enables attention across feature-map groups.By stacking these Split-Attention blocks ResNet-style, we obtain a newResNet variant which we call ResNeSt. Our network preserves the over-all ResNet structure to be used in downstream tasks straightforwardlywithout introducing additional computational costs.ResNeSt models outperform other networks with similar model com-plexities. For example, ResNeSt-50 achieves 81.13% top-1 accuracy onImageNet using a single crop-size of 224 × 224, outperforming previ-ous best ResNet variant by more than 1% accuracy. This improvementalso helps downstream tasks including object detection, instance segmen-tation and semantic segmentation. For example, by simply replace theResNet-50 backbone with ResNeSt-50, we improve the mAP of Faster-RCNN on MS-COCO from 39.3% to 42.3% and the mIoU for DeeplabV3on ADE20K from 42.1% to 45.1%1</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>148</td>
        <td>RedNet152</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=RedNet">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>149</td>
        <td>RedNet38</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=RedNet">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>150</td>
        <td>RedNet101</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=RedNet">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>151</td>
        <td>RedNet26</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=RedNet">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>152</td>
        <td>RedNet50</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=RedNet">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at this https URL. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>153</td>
        <td>LeViT_128S</td>
        <td><a href="https://paperswithcode.com/method/levit">LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference</a></td>
        <td><details><summary>Abstract</summary><div>We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at this https URL</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>154</td>
        <td>LeViT_256</td>
        <td><a href="https://paperswithcode.com/method/levit">LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference</a></td>
        <td><details><summary>Abstract</summary><div>We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at this https URL</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>155</td>
        <td>LeViT_192</td>
        <td><a href="https://paperswithcode.com/method/levit">LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference</a></td>
        <td><details><summary>Abstract</summary><div>We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at this https URL</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>156</td>
        <td>LeViT_128</td>
        <td><a href="https://paperswithcode.com/method/levit">LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference</a></td>
        <td><details><summary>Abstract</summary><div>We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at this https URL</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>157</td>
        <td>LeViT_384</td>
        <td><a href="https://paperswithcode.com/method/levit">LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference</a></td>
        <td><details><summary>Abstract</summary><div>We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at this https URL</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>158</td>
        <td>alt_gvt_large</td>
        <td><a href="https://paperswithcode.com/paper/twins-revisiting-spatial-attention-design-in#code">Twins: Revisiting the Design of Spatial Attention in Vision Transformers</a></td>
        <td><details><summary>Abstract</summary><div>Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at this https URL . </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>159</td>
        <td>pcpvt_large</td>
        <td><a href="https://paperswithcode.com/paper/twins-revisiting-spatial-attention-design-in#code">Twins: Revisiting the Design of Spatial Attention in Vision Transformers</a></td>
        <td><details><summary>Abstract</summary><div>Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at this https URL . </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>160</td>
        <td>alt_gvt_small</td>
        <td><a href="https://paperswithcode.com/paper/twins-revisiting-spatial-attention-design-in#code">Twins: Revisiting the Design of Spatial Attention in Vision Transformers</a></td>
        <td><details><summary>Abstract</summary><div>Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at this https URL . </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>161</td>
        <td>pcpvt_base</td>
        <td><a href="https://paperswithcode.com/paper/twins-revisiting-spatial-attention-design-in#code">Twins: Revisiting the Design of Spatial Attention in Vision Transformers</a></td>
        <td><details><summary>Abstract</summary><div>Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at this https URL . </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>162</td>
        <td>pcpvt_small</td>
        <td><a href="https://paperswithcode.com/paper/twins-revisiting-spatial-attention-design-in#code">Twins: Revisiting the Design of Spatial Attention in Vision Transformers</a></td>
        <td><details><summary>Abstract</summary><div>Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at this https URL . </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>163</td>
        <td>alt_gvt_base</td>
        <td><a href="https://paperswithcode.com/paper/twins-revisiting-spatial-attention-design-in#code">Twins: Revisiting the Design of Spatial Attention in Vision Transformers</a></td>
        <td><details><summary>Abstract</summary><div>Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at this https URL . </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>164</td>
        <td>ESNet_x0_5</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=PicoDet">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices arXiv:2111.00902v1</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been achallenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural net- work architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve la- bel assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on ob- ject detection for mobile devices. Our models achieve bet- ter trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M param- eters achieves 30.6% mAP, which is an absolute 4.8% im- provement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an ab- solute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state- of-the-art results for lightweight object detection. Code and pre-trained models are available at PaddleDetection1.1. Introduction Object detection is widely adopted in numerous com-puter vision tasks, including autonomous driving, robot vi- sion, intelligent transportation, industrial quality inspec- tion, object tracking, etc. Two-stage models normally lead to higher performance. However, this type of resource- 1https://github.com/PaddlePaddle/PaddleDetectionFigure</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>165</td>
        <td>ESNet_x0_75</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=PicoDet">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices arXiv:2111.00902v1</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been achallenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural net- work architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve la- bel assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on ob- ject detection for mobile devices. Our models achieve bet- ter trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M param- eters achieves 30.6% mAP, which is an absolute 4.8% im- provement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an ab- solute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state- of-the-art results for lightweight object detection. Code and pre-trained models are available at PaddleDetection1.1. Introduction Object detection is widely adopted in numerous com-puter vision tasks, including autonomous driving, robot vi- sion, intelligent transportation, industrial quality inspec- tion, object tracking, etc. Two-stage models normally lead to higher performance. However, this type of resource- 1https://github.com/PaddlePaddle/PaddleDetectionFigure</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>166</td>
        <td>ESNet_x1_0</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=PicoDet">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices arXiv:2111.00902v1</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been achallenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural net- work architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve la- bel assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on ob- ject detection for mobile devices. Our models achieve bet- ter trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M param- eters achieves 30.6% mAP, which is an absolute 4.8% im- provement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an ab- solute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state- of-the-art results for lightweight object detection. Code and pre-trained models are available at PaddleDetection1.1. Introduction Object detection is widely adopted in numerous com-puter vision tasks, including autonomous driving, robot vi- sion, intelligent transportation, industrial quality inspec- tion, object tracking, etc. Two-stage models normally lead to higher performance. However, this type of resource- 1https://github.com/PaddlePaddle/PaddleDetectionFigure</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>167</td>
        <td>ESNet_x0_25</td>
        <td><a href="https://paperswithcode.com/search?q_meta=&q_type=&q=PicoDet">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices arXiv:2111.00902v1</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been achallenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural net- work architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve la- bel assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on ob- ject detection for mobile devices. Our models achieve bet- ter trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M param- eters achieves 30.6% mAP, which is an absolute 4.8% im- provement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an ab- solute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state- of-the-art results for lightweight object detection. Code and pre-trained models are available at PaddleDetection1.1. Introduction Object detection is widely adopted in numerous com-puter vision tasks, including autonomous driving, robot vi- sion, intelligent transportation, industrial quality inspec- tion, object tracking, etc. Two-stage models normally lead to higher performance. However, this type of resource- 1https://github.com/PaddlePaddle/PaddleDetectionFigure</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>168</td>
        <td>HarDNet68_ds</td>
        <td><a href="https://paperswithcode.com/paper/hardnet-a-low-memory-traffic-network">HarDNet: A Low Memory Traffic Network</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>169</td>
        <td>HarDNet85</td>
        <td><a href="https://paperswithcode.com/paper/hardnet-a-low-memory-traffic-network">HarDNet: A Low Memory Traffic Network</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>170</td>
        <td>HarDNet68</td>
        <td><a href="https://paperswithcode.com/paper/hardnet-a-low-memory-traffic-network">HarDNet: A Low Memory Traffic Network</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>171</td>
        <td>HarDNet39_ds</td>
        <td><a href="https://paperswithcode.com/paper/hardnet-a-low-memory-traffic-network">HarDNet: A Low Memory Traffic Network</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>172</td>
        <td>ViT_base_patch16_224</td>
        <td><a href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
        <td><details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>173</td>
        <td>ViT_base_patch16_384</td>
        <td><a href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
        <td><details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>174</td>
        <td>ViT_base_patch32_384</td>
        <td><a href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
        <td><details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>175</td>
        <td>ViT_huge_patch16_224</td>
        <td><a href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
        <td><details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>176</td>
        <td>ViT_huge_patch32_384</td>
        <td><a href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
        <td><details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>177</td>
        <td>ViT_large_patch16_224</td>
        <td><a href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
        <td><details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>178</td>
        <td>ViT_large_patch16_384</td>
        <td><a href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
        <td><details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>179</td>
        <td>ViT_large_patch32_384</td>
        <td><a href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
        <td><details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>180</td>
        <td>ViT_small_patch16_224</td>
        <td><a href="https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
        <td><details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>181</td>
        <td>DeiT_base_patch16_224</td>
        <td><a href="https://paperswithcode.com/method/deit">Training data-efficient image transformers & distillation through attention</a></td>
        <td><details><summary>Abstract</summary><div>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>182</td>
        <td>DeiT_base_patch16_384</td>
        <td><a href="https://paperswithcode.com/method/deit">Training data-efficient image transformers & distillation through attention</a></td>
        <td><details><summary>Abstract</summary><div>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>183</td>
        <td>DeiT_small_patch16_22</br>4</td>
        <td><a href="https://paperswithcode.com/method/deit">Training data-efficient image transformers & distillation through attention</a></td>
        <td><details><summary>Abstract</summary><div>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>184</td>
        <td>DeiT_tiny_patch16_224</td>
        <td><a href="https://paperswithcode.com/method/deit">Training data-efficient image transformers & distillation through attention</a></td>
        <td><details><summary>Abstract</summary><div>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>185</td>
        <td>SwinTransformer_base_</br>patch4_window12_384</td>
        <td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
        <td><details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>186</td>
        <td>SwinTransformer_base_</br>patch4_window7_224</td>
        <td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
        <td><details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>187</td>
        <td>SwinTransformer_large</br>_patch4_window12_384</td>
        <td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
        <td><details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>188</td>
        <td>SwinTransformer_large</br>_patch4_window7_224</td>
        <td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
        <td><details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>189</td>
        <td>SwinTransformer_small</br>_patch4_window7_224</td>
        <td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
        <td><details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>190</td>
        <td>SwinTransformer_tiny_</br>patch4_window7_224</td>
        <td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
        <td><details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/algorithm_introduction/ImageNet_models.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>190</td>
        <td>ch_ppocr_mobile_v2.0_</br>det</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>191</td>
        <td>ch_ppocr_mobile_v2.0_</br>det_FPGM</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>192</td>
        <td>ch_ppocr_mobile_v2.0_</br>det_PACT</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>193</td>
        <td>ch_ppocr_mobile_v2.0_</br>det_KL</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>194</td>
        <td>ch_ppocr_mobile_v2.0_</br>rec</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>195</td>
        <td>ch_ppocr_mobile_v2.0_</br>rec_FPGM</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>196</td>
        <td>ch_ppocr_mobile_v2.0_</br>rec_PACT</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>197</td>
        <td>ch_ppocr_mobile_v2.0_</br>rec_KL</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>198</td>
        <td>ch_ppocr_mobile_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>199</td>
        <td>ch_ppocr_server_v2.0_</br>det</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>200</td>
        <td>ch_ppocr_server_v2.0_</br>rec</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>201</td>
        <td>ch_ppocr_server_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr">PP-OCR: A Practical Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>202</td>
        <td>ch_PP-OCRv2_det	</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>203</td>
        <td>ch_PP-OCRv2_det_PACT</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>204</td>
        <td>ch_PP-OCRv2_det_KL</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>205</td>
        <td>ch_PP-OCRv2_rec</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>206</td>
        <td>ch_PP-OCRv2_rec_PACT</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>207</td>
        <td>ch_PP-OCRv2_rec_KL</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>208</td>
        <td>ch_PP-OCRv2</td>
        <td><a href="https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System</a></td>
        <td><details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details></td>
        <td>-</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>209</td>
        <td>det_mv3_db_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/real-time-scene-text-detection-with">Real-time Scene Text Detection with Differentiable Binarization</a></td>
        <td><details><summary>Abstract</summary><div>Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: this https URL</div></details></td>
        <td>icdar2015 / hmean / 75.12%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>210</td>
        <td>det_r50_vd_db_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/real-time-scene-text-detection-with">Real-time Scene Text Detection with Differentiable Binarization</a></td>
        <td><details><summary>Abstract</summary><div>Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: this https URL</div></details></td>
        <td>icdar2015 / hmean / 82.38%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>211</td>
        <td>det_mv3_east_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/east-an-efficient-and-accurate-scene-text">EAST: an efficient and accurate scene text detector</a></td>
        <td><details><summary>Abstract</summary><div>Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.</div></details></td>
        <td>icdar2015 / hmean / 80.03%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>212</td>
        <td>det_r50_vd_east_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/east-an-efficient-and-accurate-scene-text">EAST: an efficient and accurate scene text detector</a></td>
        <td><details><summary>Abstract</summary><div>Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.</div></details></td>
        <td>icdar2015 / hmean / 86.25%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>213</td>
        <td>det_r50_vd_sast_icdar</br>15_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/a-single-shot-arbitrarily-shaped-text">A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning</a></td>
        <td><details><summary>Abstract</summary><div>Detecting scene text of arbitrary shapes has been a challenging task over the past years. In this paper, we propose a novel segmentation-based text detector, namely SAST, which employs a context attended multi-task learning framework based on a Fully Convolutional Network (FCN) to learn various geometric properties for the reconstruction of polygonal representation of text regions. Taking sequential characteristics of text into consideration, a Context Attention Block is introduced to capture long-range dependencies of pixel information to obtain a more reliable segmentation. In post-processing, a Point-to-Quad assignment method is proposed to cluster pixels into text instances by integrating both high-level object knowledge and low-level pixel information in a single shot. Moreover, the polygonal representation of arbitrarily-shaped text can be extracted with the proposed geometric properties much more effectively. Experiments on several benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves better or comparable performance in terms of accuracy. Furthermore, the proposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0% on a single NVIDIA Titan Xp graphics card, surpassing most of the existing segmentation-based methods.</div></details></td>
        <td>icdar2015 / hmean / 87.42%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>214</td>
        <td>det_r50_vd_sast_total</br>text_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/a-single-shot-arbitrarily-shaped-text">A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning</a></td>
        <td><details><summary>Abstract</summary><div>Detecting scene text of arbitrary shapes has been a challenging task over the past years. In this paper, we propose a novel segmentation-based text detector, namely SAST, which employs a context attended multi-task learning framework based on a Fully Convolutional Network (FCN) to learn various geometric properties for the reconstruction of polygonal representation of text regions. Taking sequential characteristics of text into consideration, a Context Attention Block is introduced to capture long-range dependencies of pixel information to obtain a more reliable segmentation. In post-processing, a Point-to-Quad assignment method is proposed to cluster pixels into text instances by integrating both high-level object knowledge and low-level pixel information in a single shot. Moreover, the polygonal representation of arbitrarily-shaped text can be extracted with the proposed geometric properties much more effectively. Experiments on several benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves better or comparable performance in terms of accuracy. Furthermore, the proposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0% on a single NVIDIA Titan Xp graphics card, surpassing most of the existing segmentation-based methods.</div></details></td>
        <td>total-text / hmean / 83.66%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>215</td>
        <td>det_r50_vd_pse_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/shape-robust-text-detection-with-progressive-1">Shape Robust Text Detection with Progressive Scale Expansion Network</a></td>
        <td><details><summary>Abstract</summary><div>Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape. On the other hand, two text instances which are close to each other may lead to a false detection which covers both instances. Traditionally, the segmentation-based approach can relieve the first problem but usually fail to solve the second challenge. To address these two challenges, in this paper, we propose a novel Progressive Scale Expansion Network (PSENet), which can precisely detect text instances with arbitrary shapes. More specifically, PSENet generates the different scale of kernels for each text instance, and gradually expands the minimal scale kernel to the text instance with the complete shape. Due to the fact that there are large geometrical margins among the minimal scale kernels, our method is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances. Extensive experiments on CTW1500, Total-Text, ICDAR 2015 and ICDAR 2017 MLT validate the effectiveness of PSENet. Notably, on CTW1500, a dataset full of long curve texts, PSENet achieves a F-measure of 74.3% at 27 FPS, and our best F-measure (82.2%) outperforms state-of-art algorithms by 6.6%. </div></details></td>
        <td>icdar2015 / hmean / 82.55%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>216</td>
        <td>det_mv3_pse_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/shape-robust-text-detection-with-progressive-1">Shape Robust Text Detection with Progressive Scale Expansion Network</a></td>
        <td><details><summary>Abstract</summary><div>Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape. On the other hand, two text instances which are close to each other may lead to a false detection which covers both instances. Traditionally, the segmentation-based approach can relieve the first problem but usually fail to solve the second challenge. To address these two challenges, in this paper, we propose a novel Progressive Scale Expansion Network (PSENet), which can precisely detect text instances with arbitrary shapes. More specifically, PSENet generates the different scale of kernels for each text instance, and gradually expands the minimal scale kernel to the text instance with the complete shape. Due to the fact that there are large geometrical margins among the minimal scale kernels, our method is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances. Extensive experiments on CTW1500, Total-Text, ICDAR 2015 and ICDAR 2017 MLT validate the effectiveness of PSENet. Notably, on CTW1500, a dataset full of long curve texts, PSENet achieves a F-measure of 74.3% at 27 FPS, and our best F-measure (82.2%) outperforms state-of-art algorithms by 6.6%. </div></details></td>
        <td>icdar2015 / hmean / 75.89%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>217</td>
        <td>rec_mv3_none_bilstm_c</br>tc_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</a></td>
        <td><details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 79.97%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>218</td>
        <td>rec_r34_vd_none_bilst</br>m_ctc_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</a></td>
        <td><details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 82.76%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>219</td>
        <td>rec_mv3_none_none_ctc</br>_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</a></td>
        <td><details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 78.05%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>220</td>
        <td>rec_r34_vd_none_none_</br>ctc_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</a></td>
        <td><details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 80.9%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>221</td>
        <td>rec_mv3_tps_bilstm_at</br>t_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</a></td>
        <td><details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 82.5%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>222</td>
        <td>rec_r34_vd_tps_bilstm</br>_att_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</a></td>
        <td><details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 83.6%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>223</td>
        <td>rec_mv3_tps_bilstm_ct</br>c_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</a></td>
        <td><details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 81.42%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>224</td>
        <td>rec_r34_vd_tps_bilstm</br>_ctc_v2.0</td>
        <td><a href="https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition">What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</a></td>
        <td><details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 84.44%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>225</td>
        <td>rec_r50_vd_srn</td>
        <td><a href="https://paperswithcode.com/paper/towards-accurate-scene-text-recognition-with">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks</a></td>
        <td><details><summary>Abstract</summary><div>Scene text image contains two levels of contents: visual texture and semantic information. Although the previous scene text recognition methods have made great progress over the past few years, the research on mining semantic information to assist text recognition attracts less attention, only RNN-like structures are explored to implicitly model semantic information. However, we observe that RNN based methods have some obvious shortcomings, such as time-dependent decoding manner and one-way serial transmission of semantic context, which greatly limit the help of semantic information and the computation efficiency. To mitigate these limitations, we propose a novel end-to-end trainable framework named semantic reasoning network (SRN) for accurate scene text recognition, where a global semantic reasoning module (GSRM) is introduced to capture global semantic context through multi-way parallel transmission. The state-of-the-art results on 7 public benchmarks, including regular text, irregular text and non-Latin long text, verify the effectiveness and robustness of the proposed method. In addition, the speed of SRN has significant advantages over the RNN based methods, demonstrating its value in practical use.</div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 88.52%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>226</td>
        <td>rec_mtb_nrtr</td>
        <td><a href="https://paperswithcode.com/paper/nrtr-a-no-recurrence-sequence-to-sequence">NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Scene text recognition has attracted a great many researches due to its importance to various applications. Existing methods mainly adopt recurrence or convolution based networks. Though have obtained good performance, these methods still suffer from two limitations: slow training speed due to the internal recurrence of RNNs, and high complexity due to stacked convolutional layers for long-term feature extraction. This paper, for the first time, proposes a no-recurrence sequence-to-sequence text recognizer, named NRTR, that dispenses with recurrences and convolutions entirely. NRTR follows the encoder-decoder paradigm, where the encoder uses stacked self-attention to extract image features, and the decoder applies stacked self-attention to recognize texts based on encoder output. NRTR relies solely on self-attention mechanism thus could be trained with more parallelization and less complexity. Considering scene image has large variation in text and background, we further design a modality-transform block to effectively transform 2D input images to 1D sequences, combined with the encoder to extract more discriminative features. NRTR achieves state-of-the-art or highly competitive performance on both regular and irregular benchmarks, while requires only a small fraction of training time compared to the best model from the literature (at least 8 times faster). </div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 84.3%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>227</td>
        <td>rec_r31_sar</td>
        <td><a href="https://paperswithcode.com/paper/show-attend-and-read-a-simple-and-strong">Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Recognizing irregular text in natural scene images is challenging due to the large variance in text appearance, such as curvature, orientation and distortion. Most existing approaches rely heavily on sophisticated model designs and/or extra fine-grained annotations, which, to some extent, increase the difficulty in algorithm implementation and data collection. In this work, we propose an easy-to-implement strong baseline for irregular scene text recognition, using off-the-shelf neural network components and only word-level annotations. It is composed of a -layer ResNet, an LSTM-based encoder-decoder framework and a 2-dimensional attention module. Despite its simplicity, the proposed method is robust and achieves state-of-the-art performance on both regular and irregular scene text recognition benchmarks. </div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 87.2%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>228</td>
        <td>rec_resnet_stn_bilstm</br>_att</td>
        <td><a href="https://paperswithcode.com/paper/seed-semantics-enhanced-encoder-decoder">SEED: Semantics Enhanced Encoder-Decoder Framework for Scene TextRecognition</a></td>
        <td><details><summary>Abstract</summary><div>Scene text recognition is a hot research topic in computer vision. Recently, many recognition methods based on the encoder-decoder framework have been proposed, and they can handle scene texts of perspective distortion and curve shape. Nevertheless, they still face lots of challenges like image blur, uneven illumination, and incomplete characters. We argue that most encoder-decoder methods are based on local visual features without explicit global semantic information. In this work, we propose a semantics enhanced encoder-decoder framework to robustly recognize low-quality scene texts. The semantic information is used both in the encoder module for supervision and in the decoder module for initializing. In particular, the state-of-the art ASTER method is integrated into the proposed framework as an exemplar. Extensive experiments demonstrate that the proposed framework is more robust for low-quality text images, and achieves state-of-the-art results on several benchmark datasets. </div></details></td>
        <td>IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE / avg_acc / 85.2%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_ch/algorithm_overview.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>229</td>
        <td>en_server_pgnetA</td>
        <td><a href="https://paperswithcode.com/paper/pgnet-real-time-arbitrarily-shaped-text">PGNet: Real-time Arbitrarily-Shaped Text Spottingwith Point Gathering Network</a></td>
        <td><details><summary>Abstract</summary><div>The reading of arbitrarily-shaped text has received increasing research attention. However, existing text spotters are mostly built on two-stage frameworks or character-based methods, which suffer from either Non-Maximum Suppression (NMS), Region-of-Interest (RoI) operations, or character-level annotations. In this paper, to address the above problems, we propose a novel fully convolutional Point Gathering Network (PGNet) for reading arbitrarily-shaped text in real-time. The PGNet is a single-shot text spotter, where the pixel-level character classification map is learned with proposed PG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC decoder, we gather high-level character classification vectors from two-dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees high efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and improve the end-to-end performance. Experiments prove that the proposed method achieves competitive accuracy, meanwhile significantly improving the running speed. In particular, in Total-Text, it runs at 46.7 FPS, surpassing the previous spotters with a large margin.</div></details></td>
        <td>total-text / e2e_f_score / 60.03%</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.1/doc/doc_ch/pgnet.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>230</td>
        <td>PP-Structure-layout</td>
        <td><a href="-">-</a></td>
        <td><details><summary>Abstract</summary><div>-</div></details></td>
        <td>-</td>
        <td><a href="-">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>231</td>
        <td>PP-Structure-chartrec</td>
        <td><a href="-">-</a></td>
        <td><details><summary>Abstract</summary><div>-</div></details></td>
        <td>-</td>
        <td><a href="-">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>232</td>
        <td>PP-Structure</td>
        <td><a href="-">-</a></td>
        <td><details><summary>Abstract</summary><div>-</div></details></td>
        <td>-</td>
        <td><a href="-">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>233</td>
        <td>ppyolo_mbv3_small_coc</br>o</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div>Object detection is one of the most important areas in computer vision, which plays a key role in various practical scenarios. Due to limitation of hardware, it is often necessary to sacrifice accuracy to ensure the infer speed of the detector in practice. Therefore, the balance between effectiveness and efficiency of object detector must be considered. The goal of this paper is to implement an object detector with relatively balanced effectiveness and efficiency that can be directly applied in actual application scenarios, rather than propose a novel detection model. Considering that YOLOv3 has been widely used in practice, we develop a new object detector based on YOLOv3. We mainly try to combine various existing tricks that almost not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Since all experiments in this paper are conducted based on PaddlePaddle, we call it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better balance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing the existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source code is at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>234</td>
        <td>ppyolo_r18vd_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div>Object detection is one of the most important areas in computer vision, which plays a key role in various practical scenarios. Due to limitation of hardware, it is often necessary to sacrifice accuracy to ensure the infer speed of the detector in practice. Therefore, the balance between effectiveness and efficiency of object detector must be considered. The goal of this paper is to implement an object detector with relatively balanced effectiveness and efficiency that can be directly applied in actual application scenarios, rather than propose a novel detection model. Considering that YOLOv3 has been widely used in practice, we develop a new object detector based on YOLOv3. We mainly try to combine various existing tricks that almost not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Since all experiments in this paper are conducted based on PaddlePaddle, we call it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better balance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing the existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source code is at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>235</td>
        <td>ppyolo_tiny_650e_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div>Object detection is one of the most important areas in computer vision, which plays a key role in various practical scenarios. Due to limitation of hardware, it is often necessary to sacrifice accuracy to ensure the infer speed of the detector in practice. Therefore, the balance between effectiveness and efficiency of object detector must be considered. The goal of this paper is to implement an object detector with relatively balanced effectiveness and efficiency that can be directly applied in actual application scenarios, rather than propose a novel detection model. Considering that YOLOv3 has been widely used in practice, we develop a new object detector based on YOLOv3. We mainly try to combine various existing tricks that almost not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Since all experiments in this paper are conducted based on PaddlePaddle, we call it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better balance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing the existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source code is at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>236</td>
        <td>ppyolov2_r101vd_dcn_3</br>65e_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLOv2: A Practical Object Detector</a></td>
        <td><details><summary>Abstract</summary><div>Being effective and efficient is essential to an object detector for practical use. To meet these two concerns, we comprehensively evaluate a collection of existing refinements to improve the performance of PP-YOLO while almost keep the infer time unchanged. This paper will analyze a collection of refinements and empirically evaluate their impact on the final model performance through incremental ablation study. Things we tried that didn't work will also be discussed. By combining multiple effective refinements, we boost PP-YOLO's performance from 45.9% mAP to 49.5% mAP on COCO2017 test-dev. Since a significant margin of performance has been made, we present PP-YOLOv2. In terms of speed, PP-YOLOv2 runs in 68.9FPS at 640x640 input size. Paddle inference engine with TensorRT, FP16-precision, and batch size = 1 further improves PP-YOLOv2's infer speed, which achieves 106.5 FPS. Such a performance surpasses existing object detectors with roughly the same amount of parameters (i.e., YOLOv4-CSP, YOLOv5l). Besides, PP-YOLOv2 with ResNet101 achieves 50.3% mAP on COCO2017 test-dev. Source code is at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>237</td>
        <td>picodet_s_320_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been a challenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural network architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve label assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on object detection for mobile devices. Our models achieve better trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M parameters achieves 30.6% mAP, which is an absolute 4.8% improvement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an absolute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state-of-the-art results for lightweight object detection. Code and pre-trained models are available at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>238</td>
        <td>picodet_m_416_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been a challenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural network architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve label assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on object detection for mobile devices. Our models achieve better trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M parameters achieves 30.6% mAP, which is an absolute 4.8% improvement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an absolute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state-of-the-art results for lightweight object detection. Code and pre-trained models are available at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>239</td>
        <td>picodet_l_640_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been a challenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural network architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve label assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on object detection for mobile devices. Our models achieve better trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M parameters achieves 30.6% mAP, which is an absolute 4.8% improvement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an absolute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state-of-the-art results for lightweight object detection. Code and pre-trained models are available at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>240</td>
        <td>picodet_lcnet_1_5x_41</br>6_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been a challenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural network architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve label assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on object detection for mobile devices. Our models achieve better trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M parameters achieves 30.6% mAP, which is an absolute 4.8% improvement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an absolute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state-of-the-art results for lightweight object detection. Code and pre-trained models are available at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>241</td>
        <td>picodet_mobilenetv3_l</br>arge_1x_416_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been a challenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural network architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve label assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on object detection for mobile devices. Our models achieve better trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M parameters achieves 30.6% mAP, which is an absolute 4.8% improvement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an absolute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state-of-the-art results for lightweight object detection. Code and pre-trained models are available at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>242</td>
        <td>picodet_r18_640_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been a challenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural network architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve label assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on object detection for mobile devices. Our models achieve better trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M parameters achieves 30.6% mAP, which is an absolute 4.8% improvement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an absolute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state-of-the-art results for lightweight object detection. Code and pre-trained models are available at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>243</td>
        <td>picodet_shufflenetv2_</br>1x_416_coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices</a></td>
        <td><details><summary>Abstract</summary><div>The better accuracy and efficiency trade-off has been a challenging problem in object detection. In this work, we are dedicated to studying key optimizations and neural network architecture choices for object detection to improve accuracy and efficiency. We investigate the applicability of the anchor-free strategy on lightweight object detection models. We enhance the backbone structure and design the lightweight structure of the neck, which improves the feature extraction ability of the network. We improve label assignment strategy and loss function to make training more stable and efficient. Through these optimizations, we create a new family of real-time object detectors, named PP-PicoDet, which achieves superior performance on object detection for mobile devices. Our models achieve better trade-offs between accuracy and latency compared to other popular models. PicoDet-S with only 0.99M parameters achieves 30.6% mAP, which is an absolute 4.8% improvement in mAP while reducing mobile CPU inference latency by 55% compared to YOLOX-Nano, and is an absolute 7.1% improvement in mAP compared to NanoDet. It reaches 123 FPS (150 FPS using Paddle Lite) on mobile ARM CPU when the input size is 320. PicoDet-L with only 3.3M parameters achieves 40.9% mAP, which is an absolute 3.7% improvement in mAP and 44% faster than YOLOv5s. As shown in Figure 1, our models far outperform the state-of-the-art results for lightweight object detection. Code and pre-trained models are available at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>244</td>
        <td>tinypose_128x96</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">无</a></td>
        <td><details><summary>Abstract</summary><div>无</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>245</td>
        <td>ssdlite_mobilenet_v1_</br>300_coco</td>
        <td><a href="https://github.com/weiliu89/caffe/tree/ssd">SSD: Single Shot MultiBox Detector</a></td>
        <td><details><summary>Abstract</summary><div>We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For 300\times 300 input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for 500\times 500 input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL .</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>246</td>
        <td>faster_rcnn_r50_fpn_1</br>x_coco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>247</td>
        <td>faster_rcnn_swin_tiny</br>_fpn_1x_coco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>248</td>
        <td>faster_rcnn_r34_fpn_1</br>x_coco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>249</td>
        <td>faster_rcnn_r34_vd_fp</br>n_1x_coco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>250</td>
        <td>faster_rcnn_r50_1x_co</br>co</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>251</td>
        <td>faster_rcnn_r50_vd_1x</br>_coco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>252</td>
        <td>faster_rcnn_r50_vd_fp</br>n_1x_coco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>253</td>
        <td>faster_rcnn_r101_1x_c</br>oco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>254</td>
        <td>faster_rcnn_r101_fpn_</br>1x_coco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>255</td>
        <td>faster_rcnn_r101_vd_f</br>pn_1x_coco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>256</td>
        <td>faster_rcnn_x101_vd_6</br>4x4d_fpn_1x_coco</td>
        <td><a href="">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>257</td>
        <td>fcos_r50_fpn_1x_coco</td>
        <td><a href="https://github.com/tianzhi0549/FCOS/">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
        <td><details><summary>Abstract</summary><div>We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: this https URL</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>258</td>
        <td>fcos_dcn_r50_fpn_1x_c</br>oco</td>
        <td><a href="https://github.com/tianzhi0549/FCOS/">FCOS: Fully Convolutional One-Stage Object Detection</a></td>
        <td><details><summary>Abstract</summary><div>We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: this https URL</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>259</td>
        <td>yolov3_mobilenet_v1_2</br>70e_coco</td>
        <td><a href="https://pjreddie.com/darknet/yolo/">YOLOv3: An Incremental Improvement</a></td>
        <td><details><summary>Abstract</summary><div>We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: this https URL</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>260</td>
        <td>yolov3_mobilenet_v3_l</br>arge_270e_coco</td>
        <td><a href="https://pjreddie.com/darknet/yolo/">YOLOv3: An Incremental Improvement</a></td>
        <td><details><summary>Abstract</summary><div>We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: this https URL</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>261</td>
        <td>yolov3_r34_270e_coco</td>
        <td><a href="https://pjreddie.com/darknet/yolo/">YOLOv3: An Incremental Improvement</a></td>
        <td><details><summary>Abstract</summary><div>We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: this https URL</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>262</td>
        <td>yolov3_r50vd_dcn_270e</br>_coco</td>
        <td><a href="https://pjreddie.com/darknet/yolo/">YOLOv3: An Incremental Improvement</a></td>
        <td><details><summary>Abstract</summary><div>We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: this https URL</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>263</td>
        <td>ttfnet_darknet53_1x_c</br>oco</td>
        <td><a href="https://github.com/ZJULearning/ttfnet">Training-Time-Friendly Network for Real-Time Object Detection</a></td>
        <td><details><summary>Abstract</summary><div>Modern object detectors can rarely achieve short training time, fast inference speed, and high accuracy at the same time. To strike a balance among them, we propose the Training-Time-Friendly Network (TTFNet). In this work, we start with light-head, single-stage, and anchor-free designs, which enable fast inference speed. Then, we focus on shortening training time. We notice that encoding more training samples from annotated boxes plays a similar role as increasing batch size, which helps enlarge the learning rate and accelerate the training process. To this end, we introduce a novel approach using Gaussian kernels to encode training samples. Besides, we design the initiative sample weights for better information utilization. Experiments on MS COCO show that our TTFNet has great advantages in balancing training time, inference speed, and accuracy. It has reduced training time by more than seven times compared to previous real-time detectors while maintaining state-of-the-art performances. In addition, our super-fast version of TTFNet-18 and TTFNet-53 can outperform SSD300 and YOLOv3 by less than one-tenth of their training time, respectively. The code has been made available at \url{this https URL}.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>264</td>
        <td>cascade_rcnn_r50_fpn_</br>1x_coco</td>
        <td><a href="https://github.com/zhaoweicai/cascade-rcnn">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
        <td><details><summary>Abstract</summary><div>In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code will be made available at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>265</td>
        <td>cascade_rcnn_r50_vd_f</br>pn_ssld_1x_coco</td>
        <td><a href="https://github.com/zhaoweicai/cascade-rcnn">Cascade R-CNN: Delving into High Quality Object Detection</a></td>
        <td><details><summary>Abstract</summary><div>In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code will be made available at this https URL.</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>266</td>
        <td>cascade_mask_rcnn_r50</br>_fpn_1x_coco</td>
        <td><a href="https://github.com/zhaoweicai/Detectron-Cascade-RCNN">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>In object detection, the intersection over union (IoU) threshold is frequently used to define positives/negatives. The threshold used to train a detector defines its \textit{quality}. While the commonly used threshold of 0.5 leads to noisy (low-quality) detections, detection performance frequently degrades for larger thresholds. This paradox of high-quality detection has two causes: 1) overfitting, due to vanishing positive samples for large thresholds, and 2) inference-time quality mismatch between detector and test hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, composed of a sequence of detectors trained with increasing IoU thresholds, is proposed to address these problems. The detectors are trained sequentially, using the output of a detector as training set for the next. This resampling progressively improves hypotheses quality, guaranteeing a positive training set of equivalent size for all detectors and minimizing overfitting. The same cascade is applied at inference, to eliminate quality mismatches between hypotheses and detectors. An implementation of the Cascade R-CNN without bells or whistles achieves state-of-the-art performance on the COCO dataset, and significantly improves high-quality detection on generic and specific object detection datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally, the Cascade R-CNN is generalized to instance segmentation, with nontrivial improvements over the Mask R-CNN. To facilitate future research, two implementations are made available at \url{this https URL} (Caffe) and \url{this https URL} (Detectron).</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>267</td>
        <td>cascade_mask_rcnn_r50</br>_vd_fpn_ssld_1x_coco</td>
        <td><a href="https://github.com/zhaoweicai/Detectron-Cascade-RCNN">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>In object detection, the intersection over union (IoU) threshold is frequently used to define positives/negatives. The threshold used to train a detector defines its \textit{quality}. While the commonly used threshold of 0.5 leads to noisy (low-quality) detections, detection performance frequently degrades for larger thresholds. This paradox of high-quality detection has two causes: 1) overfitting, due to vanishing positive samples for large thresholds, and 2) inference-time quality mismatch between detector and test hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, composed of a sequence of detectors trained with increasing IoU thresholds, is proposed to address these problems. The detectors are trained sequentially, using the output of a detector as training set for the next. This resampling progressively improves hypotheses quality, guaranteeing a positive training set of equivalent size for all detectors and minimizing overfitting. The same cascade is applied at inference, to eliminate quality mismatches between hypotheses and detectors. An implementation of the Cascade R-CNN without bells or whistles achieves state-of-the-art performance on the COCO dataset, and significantly improves high-quality detection on generic and specific object detection datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally, the Cascade R-CNN is generalized to instance segmentation, with nontrivial improvements over the Mask R-CNN. To facilitate future research, two implementations are made available at \url{this https URL} (Caffe) and \url{this https URL} (Detectron).</div></details></td>
        <td>COCO mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>268</td>
        <td>blazeface_1000e</td>
        <td><a href="">BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs</a></td>
        <td><details><summary>Abstract</summary><div>We present BlazeFace, a lightweight and well-performing face detector tailored for mobile GPU inference. It runs at a speed of 200-1000+ FPS on flagship devices. This super-realtime performance enables it to be applied to any augmented reality pipeline that requires an accurate facial region of interest as an input for task-specific models, such as 2D/3D facial keypoint or geometry estimation, facial features or expression classification, and face region segmentation. Our contributions include a lightweight feature extraction network inspired by, but distinct from MobileNetV1/V2, a GPU-friendly anchor scheme modified from Single Shot MultiBox Detector (SSD), and an improved tie resolution strategy alternative to non-maximum suppression.</div></details></td>
        <td>wider face</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>269</td>
        <td>blazeface_fpn_ssh_100</br>0e</td>
        <td><a href="">BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs</a></td>
        <td><details><summary>Abstract</summary><div>We present BlazeFace, a lightweight and well-performing face detector tailored for mobile GPU inference. It runs at a speed of 200-1000+ FPS on flagship devices. This super-realtime performance enables it to be applied to any augmented reality pipeline that requires an accurate facial region of interest as an input for task-specific models, such as 2D/3D facial keypoint or geometry estimation, facial features or expression classification, and face region segmentation. Our contributions include a lightweight feature extraction network inspired by, but distinct from MobileNetV1/V2, a GPU-friendly anchor scheme modified from Single Shot MultiBox Detector (SSD), and an improved tie resolution strategy alternative to non-maximum suppression.</div></details></td>
        <td>wider face</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>270</td>
        <td>s2anet_conv_2x_spine</td>
        <td><a href="https://github.com/csuhan/s2anet">Align Deep Features for Oriented Object Detection</a></td>
        <td><details><summary>Abstract</summary><div>The past decade has witnessed significant progress on detecting objects in aerial images that are often distributed with large scale variations and arbitrary orientations. However most of existing methods rely on heuristically defined anchors with different scales, angles and aspect ratios and usually suffer from severe misalignment between anchor boxes and axis-aligned convolutional features, which leads to the common inconsistency between the classification score and localization accuracy. To address this issue, we propose a Single-shot Alignment Network (S2A-Net) consisting of two modules: a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The FAM can generate high-quality anchors with an Anchor Refinement Network and adaptively align the convolutional features according to the anchor boxes with a novel Alignment Convolution. The ODM first adopts active rotating filters to encode the orientation information and then produces orientation-sensitive and orientation-invariant features to alleviate the inconsistency between classification score and localization accuracy. Besides, we further explore the approach to detect objects in large-size images, which leads to a better trade-off between speed and accuracy. Extensive experiments demonstrate that our method can achieve state-of-the-art performance on two commonly used aerial objects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The code is available at this https URL.</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>271</td>
        <td>s2anet_alignconv_2x_s</br>pine</td>
        <td><a href="https://github.com/csuhan/s2anet">Align Deep Features for Oriented Object Detection</a></td>
        <td><details><summary>Abstract</summary><div>The past decade has witnessed significant progress on detecting objects in aerial images that are often distributed with large scale variations and arbitrary orientations. However most of existing methods rely on heuristically defined anchors with different scales, angles and aspect ratios and usually suffer from severe misalignment between anchor boxes and axis-aligned convolutional features, which leads to the common inconsistency between the classification score and localization accuracy. To address this issue, we propose a Single-shot Alignment Network (S2A-Net) consisting of two modules: a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The FAM can generate high-quality anchors with an Anchor Refinement Network and adaptively align the convolutional features according to the anchor boxes with a novel Alignment Convolution. The ODM first adopts active rotating filters to encode the orientation information and then produces orientation-sensitive and orientation-invariant features to alleviate the inconsistency between classification score and localization accuracy. Besides, we further explore the approach to detect objects in large-size images, which leads to a better trade-off between speed and accuracy. Extensive experiments demonstrate that our method can achieve state-of-the-art performance on two commonly used aerial objects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The code is available at this https URL.</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>272</td>
        <td>s2anet_1x_spine</td>
        <td><a href="https://github.com/csuhan/s2anet">Align Deep Features for Oriented Object Detection</a></td>
        <td><details><summary>Abstract</summary><div>The past decade has witnessed significant progress on detecting objects in aerial images that are often distributed with large scale variations and arbitrary orientations. However most of existing methods rely on heuristically defined anchors with different scales, angles and aspect ratios and usually suffer from severe misalignment between anchor boxes and axis-aligned convolutional features, which leads to the common inconsistency between the classification score and localization accuracy. To address this issue, we propose a Single-shot Alignment Network (S2A-Net) consisting of two modules: a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The FAM can generate high-quality anchors with an Anchor Refinement Network and adaptively align the convolutional features according to the anchor boxes with a novel Alignment Convolution. The ODM first adopts active rotating filters to encode the orientation information and then produces orientation-sensitive and orientation-invariant features to alleviate the inconsistency between classification score and localization accuracy. Besides, we further explore the approach to detect objects in large-size images, which leads to a better trade-off between speed and accuracy. Extensive experiments demonstrate that our method can achieve state-of-the-art performance on two commonly used aerial objects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The code is available at this https URL.</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>273</td>
        <td>solov2_r50_fpn_1x_coc</br>o</td>
        <td><a href="https://git.io/AdelaiDet">SOLOv2: Dynamic, Faster and Stronger</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we aim at building a simple, direct, and fast instance segmentation framework with strong performance. We follow the principle of the SOLO method of Wang et al. "SOLO: segmenting objects by locations". Importantly, we take one step further by dynamically learning the mask head of the object segmenter such that the mask head is conditioned on the location. Specifically, the mask branch is decoupled into a mask kernel branch and mask feature branch, which are responsible for learning the convolution kernel and the convolved features respectively. Moreover, we propose Matrix NMS (non maximum suppression) to significantly reduce the inference time overhead due to NMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate a simple direct instance segmentation system, outperforming a few state-of-the-art methods in both speed and accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1% AP. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential to serve as a new strong baseline for many instance-level recognition tasks besides instance segmentation. Code is available at: this https URL</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>274</td>
        <td>solov2_r50_enhance_co</br>co</td>
        <td><a href="https://git.io/AdelaiDet">SOLOv2: Dynamic, Faster and Stronger</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we aim at building a simple, direct, and fast instance segmentation framework with strong performance. We follow the principle of the SOLO method of Wang et al. "SOLO: segmenting objects by locations". Importantly, we take one step further by dynamically learning the mask head of the object segmenter such that the mask head is conditioned on the location. Specifically, the mask branch is decoupled into a mask kernel branch and mask feature branch, which are responsible for learning the convolution kernel and the convolved features respectively. Moreover, we propose Matrix NMS (non maximum suppression) to significantly reduce the inference time overhead due to NMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate a simple direct instance segmentation system, outperforming a few state-of-the-art methods in both speed and accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1% AP. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential to serve as a new strong baseline for many instance-level recognition tasks besides instance segmentation. Code is available at: this https URL</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>275</td>
        <td>solov2_r101_vd_fpn_3x</br>_coco</td>
        <td><a href="https://git.io/AdelaiDet">SOLOv2: Dynamic, Faster and Stronger</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we aim at building a simple, direct, and fast instance segmentation framework with strong performance. We follow the principle of the SOLO method of Wang et al. "SOLO: segmenting objects by locations". Importantly, we take one step further by dynamically learning the mask head of the object segmenter such that the mask head is conditioned on the location. Specifically, the mask branch is decoupled into a mask kernel branch and mask feature branch, which are responsible for learning the convolution kernel and the convolved features respectively. Moreover, we propose Matrix NMS (non maximum suppression) to significantly reduce the inference time overhead due to NMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate a simple direct instance segmentation system, outperforming a few state-of-the-art methods in both speed and accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1% AP. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential to serve as a new strong baseline for many instance-level recognition tasks besides instance segmentation. Code is available at: this https URL</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>276</td>
        <td>mask_rcnn_r50_fpn_1x_</br>coco</td>
        <td><a href="https://github.com/facebookresearch/Detectron">Mask R-CNN</a></td>
        <td><details><summary>Abstract</summary><div>We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>277</td>
        <td>mask_rcnn_r50_1x_coco</td>
        <td><a href="https://github.com/facebookresearch/Detectron">Mask R-CNN</a></td>
        <td><details><summary>Abstract</summary><div>We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>278</td>
        <td>mask_rcnn_r50_vd_fpn_</br>1x_coco</td>
        <td><a href="https://github.com/facebookresearch/Detectron">Mask R-CNN</a></td>
        <td><details><summary>Abstract</summary><div>We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>279</td>
        <td>mask_rcnn_r101_fpn_1x</br>_coco</td>
        <td><a href="https://github.com/facebookresearch/Detectron">Mask R-CNN</a></td>
        <td><details><summary>Abstract</summary><div>We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>280</td>
        <td>mask_rcnn_r101_vd_fpn</br>_1x_coco</td>
        <td><a href="https://github.com/facebookresearch/Detectron">Mask R-CNN</a></td>
        <td><details><summary>Abstract</summary><div>We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>281</td>
        <td>mask_rcnn_x101_vd_64x</br>4d_fpn_1x_coco</td>
        <td><a href="https://github.com/facebookresearch/Detectron">Mask R-CNN</a></td>
        <td><details><summary>Abstract</summary><div>We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>282</td>
        <td>hrnet_w32_256x192</td>
        <td><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">Deep High-Resolution Representation Learning for Human Pose Estimation</a></td>
        <td><details><summary>Abstract</summary><div>This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at \url{this https URL}.</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>283</td>
        <td>dark_hrnet_w32_256x19</br>2</td>
        <td><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">Deep High-Resolution Representation Learning for Human Pose Estimation</a></td>
        <td><details><summary>Abstract</summary><div>This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at \url{this https URL}.</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>284</td>
        <td>dark_hrnet_w48_256x19</br>2</td>
        <td><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">Deep High-Resolution Representation Learning for Human Pose Estimation</a></td>
        <td><details><summary>Abstract</summary><div>This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at \url{this https URL}.</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>285</td>
        <td>higherhrnet_hrnet_w32</br>_512</td>
        <td><a href="https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation">HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation</a></td>
        <td><details><summary>Abstract</summary><div>Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene. The code and models are available at this https URL.</div></details></td>
        <td>dota mAP</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>286</td>
        <td>fairmot_dla34_30e_576</br>x320</td>
        <td><a href="https://github.com/ifzhang/FairMOT">FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking</a></td>
        <td><details><summary>Abstract</summary><div>Multi-object tracking (MOT) is an important problem in computer vision which has a wide range of applications. Formulating MOT as multi-task learning of object detection and re-ID in a single network is appealing since it allows joint optimization of the two tasks and enjoys high computation efficiency. However, we find that the two tasks tend to compete with each other which need to be carefully addressed. In particular, previous works usually treat re-ID as a secondary task whose accuracy is heavily affected by the primary detection task. As a result, the network is biased to the primary detection task which is not fair to the re-ID task. To solve the problem, we present a simple yet effective approach termed as FairMOT based on the anchor-free object detection architecture CenterNet. Note that it is not a naive combination of CenterNet and re-ID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. The resulting approach achieves high accuracy for both detection and tracking. The approach outperforms the state-of-the-art methods by a large margin on several public datasets. The source code and pre-trained models are released at this https URL.</div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>287</td>
        <td>fairmot_hrnetv2_w18_d</br>lafpn_30e_576x320</td>
        <td><a href="https://github.com/ifzhang/FairMOT">FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking</a></td>
        <td><details><summary>Abstract</summary><div>Multi-object tracking (MOT) is an important problem in computer vision which has a wide range of applications. Formulating MOT as multi-task learning of object detection and re-ID in a single network is appealing since it allows joint optimization of the two tasks and enjoys high computation efficiency. However, we find that the two tasks tend to compete with each other which need to be carefully addressed. In particular, previous works usually treat re-ID as a secondary task whose accuracy is heavily affected by the primary detection task. As a result, the network is biased to the primary detection task which is not fair to the re-ID task. To solve the problem, we present a simple yet effective approach termed as FairMOT based on the anchor-free object detection architecture CenterNet. Note that it is not a naive combination of CenterNet and re-ID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. The resulting approach achieves high accuracy for both detection and tracking. The approach outperforms the state-of-the-art methods by a large margin on several public datasets. The source code and pre-trained models are released at this https URL.</div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>288</td>
        <td>jde_darknet53_30e_576</br>x320</td>
        <td><a href="https://github.com/Zhongdao/Towards-Realtime-MOT">Towards Real-Time Multi-Object Tracking</a></td>
        <td><details><summary>Abstract</summary><div>Modern multiple object tracking (MOT) systems usually follow the \emph{tracking-by-detection} paradigm. It has 1) a detection model for target localization and 2) an appearance embedding model for data association. Having the two models separately executed might lead to efficiency problems, as the running time is simply a sum of the two steps without investigating potential structures that can be shared between them. Existing research efforts on real-time MOT usually focus on the association step, so they are essentially real-time association methods but not real-time MOT system. In this paper, we propose an MOT system that allows target detection and appearance embedding to be learned in a shared model. Specifically, we incorporate the appearance embedding model into a single-shot detector, such that the model can simultaneously output detections and the corresponding embeddings. We further propose a simple and fast association method that works in conjunction with the joint model. In both components the computation cost is significantly reduced compared with former MOT systems, resulting in a neat and fast baseline for future follow-ups on real-time MOT algorithm design. To our knowledge, this work reports the first (near) real-time MOT system, with a running speed of 22 to 40 FPS depending on the input resolution. Meanwhile, its tracking accuracy is comparable to the state-of-the-art trackers embodying separate detection and embedding (SDE) learning (64.4% MOTA \vs 66.1% MOTA on MOT-16 challenge). Code and models are available at \url{this https URL}.</div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>289</td>
        <td>yolov3_darknet53_270e</br>_coco</td>
        <td><a href="https://pjreddie.com/darknet/yolo/">YOLOv3: An Incremental Improvement</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>290</td>
        <td>yolov3_darknet53_270e</br>_coco_FPGM</td>
        <td><a href="https://pjreddie.com/darknet/yolo/">YOLOv3: An Incremental Improvement</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>291</td>
        <td>yolov3_darknet53_270e</br>_coco_PACT</td>
        <td><a href="https://pjreddie.com/darknet/yolo/">YOLOv3: An Incremental Improvement</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>292</td>
        <td>yolov3_darknet53_270e</br>_coco_KL</td>
        <td><a href="https://pjreddie.com/darknet/yolo/">YOLOv3: An Incremental Improvement</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>293</td>
        <td>ppyolo_mbv3_large_coc</br>o</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>294</td>
        <td>ppyolo_mbv3_large_coc</br>o_FPGM</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>295</td>
        <td>ppyolo_mbv3_large_coc</br>o_PACT</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>296</td>
        <td>ppyolo_mbv3_large_coc</br>o_KL</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>297</td>
        <td>ppyolo_r50vd_dcn_1x_c</br>oco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>298</td>
        <td>ppyolo_r50vd_dcn_1x_c</br>oco_FPGM</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>299</td>
        <td>ppyolo_r50vd_dcn_1x_c</br>oco_PACT</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>300</td>
        <td>ppyolo_r50vd_dcn_1x_c</br>oco_KL</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>301</td>
        <td>ppyolov2_r50vd_dcn_36</br>5e_coco</td>
        <td><a href="">PP-YOLOv2: A Practical Object Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td>支持 Paddle Inference</td>
    </tr>
    <tr>
        <td>302</td>
        <td>Deformable DETR</td>
        <td><a href="https://github.com/fundamentalvision/Deformable-DETR">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>303</td>
        <td>DETR</td>
        <td><a href="https://github.com/facebookresearch/detr">DETR: End-to-End Object Detection with Transformers</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>304</td>
        <td>Sparse R-CNN</td>
        <td><a href="https://github.com/PeizeSun/SparseR-CNN">Sparse R-CNN: End-to-End Object Detection with Learnable Proposals</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>305</td>
        <td>RetinaNet</td>
        <td><a href="https://github.com/facebookresearch/Detectron">Focal Loss for Dense Object Detection</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>306</td>
        <td>CornerNetLite</td>
        <td><a href="">CornerNet: Detecting Objects as Paired Keypoints</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>307</td>
        <td>EfficientDet</td>
        <td><a href="https://github.com/google/automl/tree/master/efficientdet">EfficientDet: Scalable and Efficient Object Detection</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>308</td>
        <td>Faceboxes</td>
        <td><a href="">FaceBoxes: A CPU Real-time Face Detector with High Accuracy</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>309</td>
        <td>Libra R-CNN</td>
        <td><a href="">Libra R-CNN: Towards Balanced Learning for Object Detection</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>310</td>
        <td>PyramidBox</td>
        <td><a href="">PyramidBox: A Context-assisted Single Shot Face Detector</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MOT  mota</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>311</td>
        <td>PP-HumanSeg-Server (D</br>eepLabv3p_resnet50)</td>
        <td><a href="https://paperswithcode.com/paper/encoder-decoder-with-atrous-separable">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>Spatial pyramid pooling module or encode-decoder structureare used in deep neural networks for semantic segmentation task. Theformer networks are able to encode multi-scale contextual information byprobing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networkscan capture sharper object boundaries by gradually recovering the spatialinformation. In this work, we propose to combine the advantages fromboth methods. Specifically, our proposed model, DeepLabv3+, extendsDeepLabv3 by adding a simple yet effective decoder module to refine thesegmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolutionto both Atrous Spatial Pyramid Pooling and decoder modules, resultingin a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapesdatasets, achieving the test set performance of 89.0% and 82.1% withoutany post-processing. Our paper is accompanied with a publicly availablereference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>312</td>
        <td>PP-HumanSeg-Lite</td>
        <td><a href="无">无</a></td>
        <td><details><summary>Abstract</summary><div>无</div></details></td>
        <td>Cityscapes/mIoU</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>313</td>
        <td>PP-HumanMatting</td>
        <td><a href="https://paperswithcode.com/paper/is-a-green-screen-really-necessary-for-real">Is a Green Screen Really Necessary for Real-Time Portrait Matting?</a></td>
        <td><details><summary>Abstract</summary><div>For portrait matting without the green screen, existing works either require auxiliary inputs that are costly to obtain or use multiple models that are computationally expensive. Consequently, they are unavailable in real-time applications. In contrast, we present a light-weight matting objective decomposition network (MODNet), which can process portrait matting from a single input image in real time. The design of MODNet benefits from optimizing a series of correlated sub-objectives simultaneously via explicit constraints. Moreover, since trimap-free methods usually suffer from the domain shift problem in practice, we introduce (1) a self-supervised strategy based on sub-objectives consistency to adapt MODNet to real-world data and (2) a one-frame delay trick to smooth the results when applying MODNet to portrait video sequence. MODNet is easy to be trained in an end-to-end style. It is much faster than contemporaneous matting methods and runs at 63 frames per second. On a carefully designed portrait matting benchmark newly proposed in this work, MODNet greatly outperforms prior trimap-free methods. More importantly, our method achieves remarkable results in daily photos and videos. Now, do you really need a green screen for real-time portrait matting?</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/contrib/Matting/README_CN.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>314</td>
        <td>U-Net</td>
        <td><a href="https://paperswithcode.com/paper/u-net-convolutional-networks-for-biomedical">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the stronguse of data augmentation to use the available annotated samples moreefficiently. The architecture consists of a contracting path to capturecontext and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from veryfew images and outperforms the prior best method (a sliding-windowconvolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrastand DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentationof a 512x512 image takes less than a second on a recent GPU. The fullimplementation (based on Caffe) and the trained networks are availableat http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>315</td>
        <td>ICNet</td>
        <td><a href="">非动态图模型，已废弃</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>316</td>
        <td>PSPNet</td>
        <td><a href="https://paperswithcode.com/paper/pyramid-scene-parsing-network">Pyramid Scene Parsing Network</a></td>
        <td><details><summary>Abstract</summary><div>Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>317</td>
        <td>PP-HumanSeg-mobile (H</br>RNet_W18_small)</td>
        <td><a href="https://paperswithcode.com/paper/190807919">Deep High-Resolution Representation Learning for Visual Recognition</a></td>
        <td><details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>318</td>
        <td>Fast-SCNN</td>
        <td><a href="https://paperswithcode.com/paper/fast-scnn-fast-semantic-segmentation-network">Fast-SCNN: Fast Semantic Segmentation Network</a></td>
        <td><details><summary>Abstract</summary><div>The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024x2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our `learning to downsample' module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>319</td>
        <td>OCRNet_HRNetW18</td>
        <td><a href="https://paperswithcode.com/paper/object-contextual-representations-for">Object-Contextual Representations for Semantic Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, % the representation similarity we compute the relation between each pixel and each object region and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Our submission "HRNet + OCR + SegFix" achieves 1-st place on the Cityscapes leaderboard by the time of submission. Code is available at: https://git.io/openseg and https://git.io/HRNet.OCR. We rephrase the object-contextual representation scheme using the Transformer encoder-decoder framework. The details are presented in~Section3.3.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>320</td>
        <td>ANN</td>
        <td><a href="https://paperswithcode.com/paper/asymmetric-non-local-neural-networks-for">Asymmetric Non-local Neural Networks for Semantic Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: https://github.com/MendelXu/ANN.git</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>321</td>
        <td>Deeplabv3</td>
        <td><a href="https://paperswithcode.com/paper/rethinking-atrous-convolution-for-semantic">Rethinking Atrous Convolution for Semantic Image Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>322</td>
        <td>GCNet</td>
        <td><a href="https://paperswithcode.com/paper/gcnet-non-local-networks-meet-squeeze">GCNet: Non-local networks meet squeeze-excitation networks and beyond</a></td>
        <td><details><summary>Abstract</summary><div>The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks. The code and configurations are released at https://github.com/xvjiarui/GCNet</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>323</td>
        <td>BiSeNetv2</td>
        <td><a href="https://paperswithcode.com/paper/bisenet-v2-bilateral-network-with-guided">BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>The low-level details and high-level semantics are both essential to the semantic segmentation task. However, to speed up the model inference, current approaches almost always sacrifice the low-level details, which leads to a considerable accuracy decrease. We propose to treat these spatial details and categorical semantics separately to achieve high accuracy and high efficiency for realtime semantic segmentation. To this end, we propose an efficient and effective architecture with a good trade-off between speed and accuracy, termed Bilateral Segmentation Network (BiSeNet V2). This architecture involves: (i) a Detail Branch, with wide channels and shallow layers to capture low-level details and generate high-resolution feature representation; (ii) a Semantic Branch, with narrow channels and deep layers to obtain high-level semantic context. The Semantic Branch is lightweight due to reducing the channel capacity and a fast-downsampling strategy. Furthermore, we design a Guided Aggregation Layer to enhance mutual connections and fuse both types of feature representation. Besides, a booster training strategy is designed to improve the segmentation performance without any extra inference cost. Extensive quantitative and qualitative evaluations demonstrate that the proposed architecture performs favourably against a few state-of-the-art real-time semantic segmentation approaches. Specifically, for a 2,048x1,024 input, we achieve 72.6% Mean IoU on the Cityscapes test set with a speed of 156 FPS on one NVIDIA GeForce GTX 1080 Ti card, which is significantly faster than existing methods, yet we achieve better segmentation accuracy</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>324</td>
        <td>DANet</td>
        <td><a href="https://paperswithcode.com/paper/dual-attention-network-for-scene-segmentation">Dual Attention Network for Scene Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the selfattention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data. We make the code and trained model publicly available at https://github.com/junfu1115/DANet</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>325</td>
        <td>HarDNet</td>
        <td><a href="https://paperswithcode.com/paper/hardnet-a-low-memory-traffic-network">HarDNet: A Low Memory Traffic Network</a></td>
        <td><details><summary>Abstract</summary><div>State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>326</td>
        <td>DecoupledSegNet</td>
        <td><a href="https://paperswithcode.com/paper/improving-semantic-segmentation-via-decoupled">Improving Semantic Segmentation via Decoupled Body and Edge Supervision</a></td>
        <td><details><summary>Abstract</summary><div>Existing semantic segmentation approaches either aim to improve the object's inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires \textit{explicitly} modeling the object \textit{body} and \textit{edge}, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including \textit{Cityscapes}, \textit{CamVid}, \textit{KIITI} and \textit{BDD} show that our proposed approach establishes new state of the art while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU \% on Cityscape with only fine-annotated data. Code and models are made available to foster any further research (\url{https://github.com/lxtGH/DecoupleSegNets}).</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>327</td>
        <td>ISANet</td>
        <td><a href="https://paperswithcode.com/paper/interlaced-sparse-self-attention-for-semantic">Interlaced Sparse Self-Attention for Semantic Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we present a so-called interlaced sparse self-attention approach to improve the efficiency of the \emph{self-attention} mechanism for semantic segmentation. The main idea is that we factorize the dense affinity matrix as the product of two sparse affinity matrices. There are two successive attention modules each estimating a sparse affinity matrix. The first attention module is used to estimate the affinities within a subset of positions that have long spatial interval distances and the second attention module is used to estimate the affinities within a subset of positions that have short spatial interval distances. These two attention modules are designed so that each position is able to receive the information from all the other positions. In contrast to the original self-attention module, our approach decreases the computation and memory complexity substantially especially when processing high-resolution feature maps. We empirically verify the effectiveness of our approach on six challenging semantic segmentation benchmarks</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>328</td>
        <td>SFNet</td>
        <td><a href="https://paperswithcode.com/paper/semantic-flow-for-fast-and-accurate-scene">Semantic Flow for Fast and Accurate Scene Parsing</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used -- atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4\% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at \url{https://github.com/lxtGH/SFSegNets}</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>329</td>
        <td>DNLNet</td>
        <td><a href="https://paperswithcode.com/paper/disentangled-non-local-neural-networks">Disentangled Non-Local Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>330</td>
        <td>GSCNN</td>
        <td><a href="https://paperswithcode.com/paper/gated-scnn-gated-shape-cnns-for-semantic">Gated-scnn: Gated shape cnns for semantic segmentation</a></td>
        <td><details><summary>Abstract</summary><div>Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. Here, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch, i.e. shape stream, that processes information in parallel to the classical stream. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>331</td>
        <td>EMANet</td>
        <td><a href="https://paperswithcode.com/paper/expectation-maximization-attention-networks">Expectation-Maximization Attention Networks for Semantic Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>Self-attention mechanism has been widely used for various tasks. It is designed to compute the representation of each position by a weighted sum of the features at all positions. Thus, it can capture long-range relations for computer vision tasks. However, it is computationally consuming. Since the attention maps are computed w.r.t all other positions. In this paper, we formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed. By a weighted summation upon these bases, the resulting representation is low-rank and deprecates noisy information from the input. The proposed Expectation-Maximization Attention (EMA) module is robust to the variance of input and is also friendly in memory and computation. Moreover, we set up the bases maintenance and normalization methods to stabilize its training procedure. We conduct extensive experiments on popular semantic segmentation benchmarks including PASCAL VOC, PASCAL Context and COCO Stuff, on which we set new records.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>332</td>
        <td>Attention-UNet</td>
        <td><a href="https://paperswithcode.com/paper/attention-u-net-learning-where-to-look-for">Attention U-Net: Learning Where to Look for the Pancreas</a></td>
        <td><details><summary>Abstract</summary><div>We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available. </div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>333</td>
        <td>U^2-Net</td>
        <td><a href="无">U2-Net: Going deeper with nested U-structure for salient object detection</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we design a simple yet powerful deep network architecture, U2-Net, for salient object detection (SOD). The architecture of our U2-Net is a two-level nested U-structure. The design has the following advantages: (1) it is able to capture more contextual information from different scales thanks to the mixture of receptive fields of different sizes in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the whole architecture without significantly increasing the computational cost because of the pooling operations used in these RSU blocks. This architecture enables us to train a deep network from scratch without using backbones from image classification tasks. We instantiate two models of the proposed architecture, U2-Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and U2-Net† (4.7 MB, 40 FPS), to facilitate the usage in different environments. Both models achieve competitive performance on six SOD datasets. The code is available: https://github.com/NathanUA/U-2-Net</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>334</td>
        <td>UNet++</td>
        <td><a href="https://paperswithcode.com/paper/unet-a-nested-u-net-architecture-for-medical">A Nested U-Net Architecture for Medical Image Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>335</td>
        <td>UNet+++</td>
        <td><a href="https://paperswithcode.com/paper/unet-3-a-full-scale-connected-unet-for">UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>Recently, a growing interest has been seen in deep learning-based semantic segmentation. UNet, which is one of deep learning networks with an encoder-decoder architecture, is widely used in medical image segmentation. Combining multi-scale features is one of important factors for accurate segmentation. UNet++ was developed as a modified Unet by designing an architecture with nested and dense skip connections. However, it does not explore sufficient information from full scales and there is still a large room for improvement. In this paper, we propose a novel UNet 3+, which takes advantage of full-scale skip connections and deep supervisions. The full-scale skip connections incorporate low-level details with high-level semantics from feature maps in different scales; while the deep supervision learns hierarchical representations from the full-scale aggregated feature maps. The proposed method is especially benefiting for organs that appear at varying scales. In addition to accuracy improvements, the proposed UNet 3+ can reduce the network parameters to improve the computation efficiency. We further propose a hybrid loss function and devise a classification-guided module to enhance the organ boundary and reduce the over-segmentation in a non-organ image, yielding more accurate segmentation results. The effectiveness of the proposed method is demonstrated on two datasets. The code is available at: github.com/ZJUGiveLab/UNet-Version</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>336</td>
        <td>SETR</td>
        <td><a href="https://paperswithcode.com/paper/rethinking-semantic-segmentation-from-a">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</a></td>
        <td><details><summary>Abstract</summary><div>Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>337</td>
        <td>SwinTransformer</td>
        <td><a href="https://paperswithcode.com/paper/swin-transformer-hierarchical-vision">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
        <td><details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{https://github.com/microsoft/Swin-Transformer}.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/tree/develop/contrib/AutoNUE">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>338</td>
        <td>SegFormer_B0</td>
        <td><a href="https://paperswithcode.com/paper/segformer-simple-and-efficient-design-for">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</a></td>
        <td><details><summary>Abstract</summary><div>We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>339</td>
        <td>STDC_STDC1</td>
        <td><a href="https://paperswithcode.com/paper/rethinking-bisenet-for-real-time-semantic">Rethinking BiSeNet For Real-time Semantic Segmentation</a></td>
        <td><details><summary>Abstract</summary><div>BiSeNet has been proved to be a popular two-stream network for real-time segmentation. However, its principle of adding an extra path to encode spatial information is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classification, may be inefficient for image segmentation due to the deficiency of task-specific design. To handle these problems, we propose a novel and efficient structure named Short-Term Dense Concatenate network (STDC network) by removing structure redundancy. Specifically, we gradually reduce the dimension of feature maps and use the aggregation of them for image representation, which forms the basic module of STDC network. In the decoder, we propose a Detail Aggregation module by integrating the learning of spatial information into low-level layers in single-stream manner. Finally, the low-level features and deep features are fused to predict the final segmentation results. Extensive experiments on Cityscapes and CamVid dataset demonstrate the effectiveness of our method by achieving promising trade-off between segmentation accuracy and inference speed. On Cityscapes, we achieve 71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti, which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0 FPS while inferring on higher resolution images.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>340</td>
        <td>PP-MSVSR</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>341</td>
        <td>Pix2Pix</td>
        <td><a href="https://paperswithcode.com/paper/image-to-image-translation-with-conditional">Image-to-Image Translation with Conditional Adversarial Networks</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>facades</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/pix2pix_cyclegan.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>342</td>
        <td>CycleGAN</td>
        <td><a href="https://paperswithcode.com/paper/unpaired-image-to-image-translation-using">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>cityscapes</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/pix2pix_cyclegan.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>343</td>
        <td>PSGAN</td>
        <td><a href="https://paperswithcode.com/paper/psgan-pose-robust-spatial-aware-gan-for">PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>MT, landmarks</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/psgan.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>344</td>
        <td>Wav2Lip</td>
        <td><a href="https://paperswithcode.com/paper/a-lip-sync-expert-is-all-you-need-for-speech">A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>LRS2</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/wav2lip.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>345</td>
        <td>LESRCNN</td>
        <td><a href="https://paperswithcode.com/paper/lightweight-image-super-resolution-with-2">Lightweight image super-resolution with enhanced CNN</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>DIV2K</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/single_image_super_resolution.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>346</td>
        <td>ESRGAN</td>
        <td><a href="https://paperswithcode.com/paper/esrgan-enhanced-super-resolution-generative">Esrgan: Enhanced super-resolution generative adversarial networks</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>DIV2K</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/single_image_super_resolution.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>347</td>
        <td>RealSR</td>
        <td><a href="https://paperswithcode.com/paper/real-world-super-resolution-via-kernel">Real-World Super-Resolution via Kernel Estimation and Noise Injection</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>DIV2K</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/single_image_super_resolution.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>348</td>
        <td>StyleGAN2</td>
        <td><a href="https://paperswithcode.com/paper/analyzing-and-improving-the-image-quality-of">Analyzing and Improving the Image Quality of StyleGAN</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>ffhq</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/styleganv2.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>349</td>
        <td>U-GAT-IT</td>
        <td><a href="https://paperswithcode.com/paper/u-gat-it-unsupervised-generative-attentional">U-GAT-IT: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>Selfie2anime</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/ugatit.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>350</td>
        <td>AnimeGAN2</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>351</td>
        <td>Photo2Cartoon</td>
        <td><a href="https://paperswithcode.com/paper/u-gat-it-unsupervised-generative-attentional">U-GAT-IT: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>photo2cartoon</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/photo2cartoon.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>352</td>
        <td>DRN</td>
        <td><a href="https://paperswithcode.com/paper/closed-loop-matters-dual-regression-networks">Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>DIV2K</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/single_image_super_resolution.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>353</td>
        <td>starGAN2</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>354</td>
        <td>FOM</td>
        <td><a href="https://paperswithcode.com/paper/first-order-motion-model-for-image-animation-1">First Order Motion Model for Image Animation</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>VoxCeleb, fashion</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/motion_driving.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>355</td>
        <td>EDVR</td>
        <td><a href="https://paperswithcode.com/paper/edvr-video-restoration-with-enhanced">EDVR: Video Restoration with Enhanced Deformable Convolutional Networks</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>REDS</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/video_super_resolution.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>356</td>
        <td>BasicVSR</td>
        <td><a href="https://paperswithcode.com/paper/basicvsr-the-search-for-essential-components">BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>REDS</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/video_super_resolution.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>357</td>
        <td>LapStyle</td>
        <td><a href="https://paperswithcode.com/paper/drafting-and-revision-laplacian-pyramid">Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>coco</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/lap_style.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>358</td>
        <td>DCGAN</td>
        <td><a href="https://paperswithcode.com/method/dcgan">Deep Convolutional GAN</a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>mnist</td>
        <td><a href="暂无">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>359</td>
        <td>CGAN</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>360</td>
        <td>DSSM</td>
        <td><a href="https://paperswithcode.com/paper/learning-deep-structured-semantic-models-for">Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</a></td>
        <td><details><summary>Abstract</summary><div>Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper</div></details></td>
        <td>BQ</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>361</td>
        <td>Match-Pyramid</td>
        <td><a href="https://paperswithcode.com/paper/text-matching-as-image-recognition">Text Matching as Image Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines.</div></details></td>
        <td>Letor07</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>362</td>
        <td>MultiView-Simnet</td>
        <td><a href="https://paperswithcode.com/paper/a-multi-view-deep-learning-approach-for-cross">A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems</a></td>
        <td><details><summary>Abstract</summary><div>Recent online services rely heavily on automatic personalization to recommend relevant content to a large number of users. This requires systems to scale promptly to accommodate the stream of new users visiting the online services for the first time. In this work, we propose a content-based recommendation system to address both the recommendation quality and the system scalability. We propose to use a rich feature set to represent users, according to their web browsing history and search queries. We use a Deep Learning approach to map users and items to a latent space where the similarity between users and their preferred items is maximized. We extend the model to jointly learn from features of items from different domains and user features by introducing a multi-view Deep Learning model. We show how to make this rich-feature based user representation scalable by reducing the dimension of the inputs and the amount of training data. The rich user feature representation allows the model to learn relevant user behavior patterns and give useful recommendations for users who do not have any interaction with the service, given that they have adequate search and browsing history. The combination of different domains into a single model for learning helps improve the recommendation quality across all the domains, as well as having a more compact and a semantically richer user latent feature vector. We experiment with our approach on three real-world recommendation systems acquired from different sources of Microsoft products: Windows Apps recommendation, News recommendation, and Movie/TV recommendation. Results indicate that our approach is significantly better than the state-of-the-art algorithms (up to 49% enhancement on existing users and 115% enhancement on new users). In addition, experiments on a publicly open data set also indicate the superiority of our method in comparison with transitional generative topic models, for modeling cross-domain recommender systems. Scalability analysis show that our multi-view DNN model can easily scale to encompass millions of users and billions of item entries. Experimental results also confirm that combining features from all domains produces much better performance than building separate models for each domain.</div></details></td>
        <td>BQ</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>363</td>
        <td>DeepWalk</td>
        <td><a href="https://paperswithcode.com/paper/deepwalk-online-learning-of-social">DeepWalk: Online Learning of Social Representations</a></td>
        <td><details><summary>Abstract</summary><div>We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk’s latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk’s representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk’s representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection</div></details></td>
        <td>BlogCatalog</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>364</td>
        <td>Mind</td>
        <td><a href="https://paperswithcode.com/paper/multi-interest-network-with-dynamic-routing">Multi-Interest Network with Dynamic Routing for Recommendation at Tmall</a></td>
        <td><details><summary>Abstract</summary><div>Industrial recommender systems usually consist of the matching stage and the ranking stage, in order to handle the billion-scale of users and items. The matching stage retrieves candidate items relevant to user interests, while the ranking stage sorts candidate items by user interests. Thus, the most critical ability is to model and represent user interests for either stage. Most of the existing deep learning-based models represent one user as a single vector which is insufficient to capture the varying nature of user’s interests. In this paper, we approach this problem from a different view, to represent one user with multiple vectors encoding the different aspects of the user’s interests. We propose the Multi-Interest Network with Dynamic routing (MIND) for dealing with user’s diverse interests in the matching stage. Specifically, we design a multi-interest extractor layer based on capsule routing mechanism, which is applicable for clustering historical behaviors and extracting diverse interests. Furthermore, we develop a technique named label-aware attention to help learn a user representation with multiple vectors. Through extensive experiments on several public benchmarks and one largescale industrial dataset from Tmall, we demonstrate that MIND can achieve superior performance than state-of-the-art methods for recommendation. Currently, MIND has been deployed for handling major online traffic at the homepage on Mobile Tmall App.</div></details></td>
        <td>AmazonBook</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>365</td>
        <td>NCF</td>
        <td><a href="https://paperswithcode.com/paper/neural-collaborative-filtering">Neural Collaborative Filtering</a></td>
        <td><details><summary>Abstract</summary><div>In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation — collaborative filtering — on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering — the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural networkbased Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user–item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance</div></details></td>
        <td>movielens</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>366</td>
        <td>Word2vec</td>
        <td><a href="https://paperswithcode.com/paper/distributed-representations-of-words-and-1">Distributed Representations of Words and Phrases and their Compositionality</a></td>
        <td><details><summary>Abstract</summary><div>The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</div></details></td>
        <td>one_billion</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>367</td>
        <td>Fasttext</td>
        <td><a href="https://paperswithcode.com/paper/bag-of-tricks-for-efficient-text">Bag of Tricks for Efficient Text Classification</a></td>
        <td><details><summary>Abstract</summary><div>This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>368</td>
        <td>GraphNeuralNetwork</td>
        <td><a href="https://paperswithcode.com/paper/session-based-recommendation-with-graph">Session-based Recommendation with Graph Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.</div></details></td>
        <td>DIGINETICA和Yoochoose</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>369</td>
        <td>GRU4Rec</td>
        <td><a href="https://paperswithcode.com/paper/session-based-recommendations-with-recurrent">Session-based Recommendations with Recurrent Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.</div></details></td>
        <td>RSC15</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>370</td>
        <td>RALM</td>
        <td><a href="https://paperswithcode.com/paper/real-time-attention-based-look-alike-model">Real-time Attention Based Look-alike Model for Recommender System</a></td>
        <td><details><summary>Abstract</summary><div>Recently, deep learning models play more and more important roles in contents recommender systems. However, although the performance of recommendations is greatly improved, the "Matthew effect" becomes increasingly evident. While the head contents get more and more popular, many competitive long-tail contents are difficult to achieve timely exposure because of lacking behavior features. This issue has badly impacted the quality and diversity of recommendations. To solve this problem, look-alike algorithm is a good choice to extend audience for high quality long-tail contents. But the traditional look-alike models which widely used in online advertising are not suitable for recommender systems because of the strict requirement of both real-time and effectiveness. This paper introduces a real-time attention based look-alike model (RALM) for recommender systems, which tackles the challenge of conflict between real-time and effectiveness. RALM realizes real-time lookalike audience extension benefiting from seeds-to-user similarity prediction and improves the effectiveness through optimizing user representation learning and look-alike learning modeling. For user representation learning, we propose a novel neural network structure named attention merge layer to replace the concatenation layer, which significantly improves the expressive ability of multifields feature learning. On the other hand, considering the various members of seeds, we design global attention unit and local attention unit to learn robust and adaptive seeds representation with respect to a certain target user. At last, we introduce seeds clustering mechanism which not only reduces the time complexity of attention units prediction but also minimizes the loss of seeds information at the same time. According to our experiments, RALM shows superior effectiveness and performance than popular lookalike models. RALM has been successfully deployed in "Top Stories" Recommender System of WeChat, leading to great improvement on diversity and quality of recommendations. As far as we know this is the first real-time look-alike model applied in recommender systems</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>371</td>
        <td>SSR</td>
        <td><a href="">Multtti-Rate Deep Learning for Temporal Recommendation</a></td>
        <td><details><summary>Abstract</summary><div>Modeling temporal behavior in recommendation systems is an important and challenging problem. Its challenges come from the fact that temporal modeling increases the cost of parameter estimation and inference, while requiring large amount of data to reliably learn the model with the additional time dimensions. Therefore, it is often difficult to model temporal behavior in large-scale real-world recommendation systems. In this work, we propose a novel deep neural network based architecture that models the combination of long-term static and short-term temporal user preferences to improve the recommendation performance. To train the model efficiently for large-scale applications, we propose a novel pre-train method to reduce the number of free parameters significantly. The resulted model is applied to a real-world data set from a commercial News recommendation system. We compare to a set of established baselines and the experimental results show that our method outperforms the state-of-the-art significantly.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>372</td>
        <td>Youtube_dnn</td>
        <td><a href="https://paperswithcode.com/paper/deep-neural-networks-for-youtube">Deep Neural Networks for YouTube Recommendations</a></td>
        <td><details><summary>Abstract</summary><div>YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>373</td>
        <td>BST</td>
        <td><a href="https://paperswithcode.com/paper/behavior-sequence-transformer-for-e-commerce">Behavior Sequence Transformer for E-commerce Recommendation in Alibaba</a></td>
        <td><details><summary>Abstract</summary><div>Deep learning based methods have been widely used in industrial recommendation systems (RSs). Previous works adopt an Embedding&MLP paradigm: raw features are embedded into lowdimensional vectors, which are then fed on to MLP for final recommendations. However, most of these works just concatenate different features, ignoring the sequential nature of users’ behaviors. In this paper, we propose to use the powerful Transformer model to capture the sequential signals underlying users’ behavior sequences for recommendation in Alibaba. Experimental results demonstrate the superiority of the proposed model, which is then deployed online at Taobao and obtain significant improvements in online Click-Through-Rate (CTR) comparing to two baselines.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>374</td>
        <td>DCN</td>
        <td><a href="https://paperswithcode.com/paper/deep-cross-network-for-ad-click-predictions">Deep & Cross Network for Ad Click Predictions</a></td>
        <td><details><summary>Abstract</summary><div>Feature engineering has been the key to the success of many prediction models. However, the process is nontrivial and oen requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily ecient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benets of a DNN model, and beyond that, it introduces a novel cross network that is more ecient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classication dataset, in terms of both model accuracy and memory usage.</div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>375</td>
        <td>DeepFM</td>
        <td><a href="https://paperswithcode.com/paper/deepfm-a-factorization-machine-based-neural">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and highorder feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its “wide” and “deep” parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.</div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>376</td>
        <td>DMR</td>
        <td><a href="https://paperswithcode.com/paper/deep-match-to-rank-model-for-personalized">Deep Match to Rank Model for Personalized Click-Through Rate Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Deep Match to Rank Model for Personalized Click-Through Rate Prediction</div></details></td>
        <td>Ali_Display_Ad_Click</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>377</td>
        <td>DNN</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>378</td>
        <td>FFM</td>
        <td><a href="https://paperswithcode.com/paper/field-aware-factorization-machines-for-ctr">Field-aware Factorization Machines for CTR Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, field-aware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for classifying large sparse data including those from CTR prediction. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use.</div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>379</td>
        <td>FM</td>
        <td><a href="">Factorization machines</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models. Index Terms—factorization machine; sparse data; tensor factorization; support vector machine</div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>380</td>
        <td>GateNet</td>
        <td><a href="https://paperswithcode.com/paper/gatenet-gating-enhanced-deep-network-for">GateNet: Gating-Enhanced Deep Network for Click-Through Rate Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Advertising and feed ranking are essential to many Internet companies such as Facebook. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. In recent years, many neural network based CTR models have been proposed and achieved success such as Factorization-Machine Supported Neural Networks, DeepFM and xDeepFM. Many of them contain two commonly used components: embedding layer and MLP hidden layers. On the other side, gating mechanism is also widely applied in many research fields such as computer vision(CV) and natural language processing(NLP). Some research has proved that gating mechanism improves the trainability of non-convex deep neural networks. Inspired by these observations, we propose a novel model named GateNet which introduces either the feature embedding gate or the hidden gate to the embedding layer or hidden layers of DNN CTR models, respectively. The feature embedding gate provides a learnable feature gating module to select salient latent information from the feature-level. The hidden gate helps the model to implicitly capture the high-order interaction more effectively. Extensive experiments conducted on three real-world datasets demonstrate its effectiveness to boost the performance of various state-of-the-art models such as FM, DeepFM and xDeepFM on all datasets.</div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>381</td>
        <td>Logistic_regression</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>382</td>
        <td>Naml</td>
        <td><a href="https://paperswithcode.com/paper/neural-news-recommendation-with-attentive">Neural News Recommendation with Attentive Multi-View Learning</a></td>
        <td><details><summary>Abstract</summary><div>Neural News Recommendation with Attentive Multi-View Learning</div></details></td>
        <td>microsoft news dataset</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>383</td>
        <td>Wide&Deep</td>
        <td><a href="https://paperswithcode.com/paper/wide-deep-learning-for-recommender-systems">Wide & Deep Learning for Recommender Systems</a></td>
        <td><details><summary>Abstract</summary><div>Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning—jointly trained wide linear models and deep neural networks—to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.</div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>384</td>
        <td>XDeepFM</td>
        <td><a href="https://paperswithcode.com/paper/xdeepfm-combining-explicit-and-implicit">xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems</a></td>
        <td><details><summary>Abstract</summary><div>Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at https://github.com/Leavingseason/xDeepFM.</div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>385</td>
        <td>AutoInt</td>
        <td><a href="https://paperswithcode.com/paper/autoint-automatic-feature-interaction">AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks</a></td>
        <td><details><summary>Abstract</summary><div>Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on highorder combinatorial features (a.k.a. cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding lowdimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the AutoInt to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multihead self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the lowdimensional space. With different layers of the multi-head selfattentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: https://github.com/DeepGraphLearning/RecommenderSystems.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>386</td>
        <td>AFM</td>
        <td><a href="https://paperswithcode.com/paper/attentional-factorization-machines-learning">Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks</a></td>
        <td><details><summary>Abstract</summary><div>Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep [Cheng et al., 2016] and DeepCross[Shan et al., 2016] with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: https://github. com/hexiangnan/attentional factorization machine</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>387</td>
        <td>DeepCross</td>
        <td><a href="https://paperswithcode.com/paper/deep-crossing-web-scale-modeling-without">Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features</a></td>
        <td><details><summary>Abstract</summary><div>Manually crafted combinatorial features have been the “secret sauce” behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units. Deep Crossing is implemented with a modeling tool called the Computational Network Tool Kit (CNTK), powered by a multi-GPU platform. It was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models. This demonstrates the potential of using Deep Crossing as a general modeling paradigm to improve existing products, as well as to speed up the development of new models with a fraction of the investment in feature engineering and acquisition of deep domain knowledge.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>388</td>
        <td>DIEN</td>
        <td><a href="https://paperswithcode.com/paper/deep-interest-evolution-network-for-click">Deep Interest Evolution Network for Click-Through Rate Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Click-through rate (CTR) prediction, whose goal is to estimate the probability of a user clicking on the item, has become one of the core tasks in the advertising system. For CTR prediction model, it is necessary to capture the latent user interest behind the user behavior data. Besides, considering the changing of the external environment and the internal cognition, user interest evolves over time dynamically. There are several CTR prediction methods for interest modeling, while most of them regard the representation of behavior as the interest directly, and lack specially modeling for latent interest behind the concrete behavior. Moreover, little work considers the changing trend of the interest. In this paper, we propose a novel model, named Deep Interest Evolution Network (DIEN), for CTR prediction. Specifically, we design interest extractor layer to capture temporal interests from history behavior sequence. At this layer, we introduce an auxiliary loss to supervise interest extracting at each step. As user interests are diverse, especially in the e-commerce system, we propose interest evolving layer to capture interest evolving process that is relative to the target item. At interest evolving layer, attention mechanism is embedded into the sequential structure novelly, and the effects of relative interests are strengthened during interest evolution. In the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. Notably, DIEN has been deployed in the display advertisement system of Taobao, and obtained 20.7% improvement on CTR.</div></details></td>
        <td>amazon eletronics</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>389</td>
        <td>DIN</td>
        <td><a href="https://paperswithcode.com/paper/deep-interest-network-for-click-through-rate">Deep Interest Network for Click-Through Rate Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.</div></details></td>
        <td>amazon eletronics</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>390</td>
        <td>FGCNN</td>
        <td><a href="https://paperswithcode.com/paper/feature-generation-by-convolutional-neural">Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Click-Through Rate prediction is an important task in recommender systems, which aims to estimate the probability of a user to click on a given item. Recently, many deep models have been proposed to learn low-order and high-order feature interactions from original features. However, since useful interactions are always sparse, it is difficult for DNN to learn them effectively under a large number of parameters. In real scenarios, artificial features are able to improve the performance of deep models (such as Wide & Deep Learning), but feature engineering is expensive and requires domain knowledge, making it impractical in different scenarios. Therefore, it is necessary to augment feature space automatically.In this paper, We propose a novel Feature Generation by Convolutional Neural Network (FGCNN) model with two components: Feature Generation and Deep Classifier. Feature Generation leverages the strength of CNN to generate local patterns and recombine them to generate new features. Deep Classifier adopts the structure of IPNN to learn interactions from the augmented feature space. Experimental results on three large-scale datasets show that FGCNN significantly outperforms nine state-of-the-art models. Moreover, when applying some state-of-the-art models as Deep Classifier, better performance is always achieved, showing the great compatibility of our FGCNN model. This work explores a novel direction for CTR predictions: it is quite useful to reduce the learning difficulties of DNN by automatically identifying important features.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>391</td>
        <td>Fibinet</td>
        <td><a href="https://paperswithcode.com/paper/fibinet-combining-feature-importance-and">FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two realworld datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM)</div></details></td>
        <td>Criteo</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>392</td>
        <td>FLEN</td>
        <td><a href="https://paperswithcode.com/paper/flen-leveraging-field-for-scalable-ctr">FLEN: Leveraging Field for Scalable CTR Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Click-Through Rate (CTR) prediction systems are usually based on multi-field categorical features, i.e., every feature is categorical and belongs to one and only one field. Modeling feature conjunctions is crucial for CTR prediction accuracy. However, it usually requires a massive number of parameters to explicitly model all feature conjunctions, which is not scalable for real-world production systems. In this paper, we describe a novel Field-Leveraged Embedding Network (FLEN) which has been deployed in the commercial recommender systems in Meitu and serves the main traffic. FLEN devises a field-wise bi-interaction pooling technique. By suitably exploiting field information, the field-wise bi-interaction pooling layer captures both inter-field and intra-field feature conjunctions with a small number of model parameters and an acceptable time complexity for industrial applications. We show that some classic shallow CTR models can be regarded as special cases of this technique, i.e., MF, FM and FwFM. We identify a unique challenge in this technique, i.e., the FM module in our model may suffer from the coupled gradient issue, which will damage the performance of the model. To solve this challenge, we develop Dicefactor: a novel dropout method to prevent independent latent features from co-adapting. Extensive experiments, including offline evaluations and online A/B testing on real production systems, demonstrate the effectiveness and efficiency of FLEN against the state-of-the-art models. In particular, compared to the previous version deployed on the system (i.e. NFM), FLEN has obtained 5.19% improvement on CTR with 1/6 of memory usage and computation time.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>393</td>
        <td>FNN</td>
        <td><a href="https://paperswithcode.com/paper/deep-learning-over-multi-field-categorical">Deep Learning over Multi-field Categorical Data</a></td>
        <td><details><summary>Abstract</summary><div>Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users’ ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>394</td>
        <td>NFM</td>
        <td><a href="https://paperswithcode.com/paper/neural-factorization-machines-for-sparse">Neural Factorization Machines for Sparse Predictive Analytics</a></td>
        <td><details><summary>Abstract</summary><div>Many predictive tasks of web applications need to model categorical variables, such as user IDs and demographics like genders and occupations. To apply standard machine learning techniques, these categorical predictors are always converted to a set of binary features via one-hot encoding, making the resultant feature vector highly sparse. To learn from such sparse data effectively, it is crucial to account for the interactions between features.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>395</td>
        <td>PNN</td>
        <td><a href="https://paperswithcode.com/paper/product-based-neural-networks-for-user">Product-based Neural Networks for User Response Prediction</a></td>
        <td><details><summary>Abstract</summary><div>Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfield categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>396</td>
        <td>ESMM</td>
        <td><a href="https://paperswithcode.com/paper/entire-space-multi-task-model-an-effective">Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate</a></td>
        <td><details><summary>Abstract</summary><div>Estimating post-click conversion rate (CVR) accurately is crucial for ranking systems in industrial applications such as recommendation and advertising. Conventional CVR modeling applies popular deep learning methods and achieves state-of-the-art performance. However it encounters several task-specific problems in practice, making CVR modeling challenging. For example, conventional CVR models are trained with samples of clicked impressions while utilized to make inference on the entire space with samples of all impressions. This causes a sample selection bias problem. Besides, there exists an extreme data sparsity problem, making the model fitting rather difficult. In this paper, we model CVR in a brand-new perspective by making good use of sequential pattern of user actions, i.e., impression -> click -> conversion. The proposed Entire Space Multi-task Model (ESMM) can eliminate the two problems simultaneously by i) modeling CVR directly over the entire space, ii) employing a feature representation transfer learning strategy. Experiments on dataset gathered from Taobao's recommender system demonstrate that ESMM significantly outperforms competitive methods. We also release a sampling version of this dataset to enable future research. To the best of our knowledge, this is the first public dataset which contains samples with sequential dependence of click and conversion labels for CVR modeling.</div></details></td>
        <td>Alibaba Click and Conversion Prediction</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>397</td>
        <td>MMOE</td>
        <td><a href="https://paperswithcode.com/paper/modeling-task-relationships-in-multi-task">Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts</a></td>
        <td><details><summary>Abstract</summary><div>Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google.</div></details></td>
        <td>Census-income</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>398</td>
        <td>PLE</td>
        <td><a href="https://paperswithcode.com/paper/progressive-layered-extraction-ple-a-novel">Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations</a></td>
        <td><details><summary>Abstract</summary><div>Multi-task learning (MTL) has been successfully applied to many recommendation applications. However, MTL models often suffer from performance degeneration with negative transfer due to the complex and competing task correlation in real-world recommender systems. Moreover, through extensive experiments across SOTA MTL models, we have observed an interesting seesaw phenomenon that performance of one task is often improved by hurting the performance of some other tasks. To address these issues, we propose a Progressive Layered Extraction (PLE) model with a novel sharing structure design. PLE separates shared components and task-specific components explicitly and adopts a progressive routing mechanism to extract and separate deeper semantic knowledge gradually, improving efficiency of joint representation learning and information routing across tasks in a general setup. We apply PLE to both complicatedly correlated and normally correlated tasks, ranging from two-task cases to multi-task cases on a real-world Tencent video recommendation dataset with 1 billion samples, and results show that PLE outperforms state-of-the-art MTL models significantly under different task correlations and task-group size. Furthermore, online evaluation of PLE on a large-scale content recommendation platform at Tencent manifests 2.23% increase in view-count and 1.84% increase in watch time compared to SOTA MTL models, which is a significant improvement and demonstrates the effectiveness of PLE. Finally, extensive offline experiments on public benchmark datasets demonstrate that PLE can be applied to a variety of scenarios besides recommendations to eliminate the seesaw phenomenon. PLE now has been deployed to the online video recommender system in Tencent successfully.</div></details></td>
        <td>Census-income</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>399</td>
        <td>ShareBottom</td>
        <td><a href="https://paperswithcode.com/paper/multitask-learning-a-knowledge-based-source">Multitask learning</a></td>
        <td><details><summary>Abstract</summary><div>Multitask Learning is an approach to inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. In this thesis we demonstrate multitask learning for a dozen problems. We explain how multitask learning works and show that there are many opportunities for multitask learning in real domains. We show that in some cases features that would normally be used as inputs work better if used as multitask outputs instead. We present suggestions for how to get the most out of multitask learning in articial neural nets, present an algorithm for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Multitask learning improves generalization performance, can be applied in many dierent kinds of domains, and can be used with dierent learning algorithms. We conjecture there will be many opportunities for its use on real-world problems.</div></details></td>
        <td>Census-income</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>400</td>
        <td>Maml</td>
        <td><a href="https://paperswithcode.com/paper/model-agnostic-meta-learning-for-fast">Model-agnostic meta-learning for fast adaptation of deep networks</a></td>
        <td><details><summary>Abstract</summary><div>We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.</div></details></td>
        <td>Omniglot</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>401</td>
        <td>Listwise</td>
        <td><a href="https://paperswithcode.com/paper/sequential-evaluation-and-generation">Sequential Evaluation and Generation Framework for Combinatorial Recommender System</a></td>
        <td><details><summary>Abstract</summary><div>to the user at one time in the result page, where the correlations among the items have impact on the user behavior. In this work, we model the combinatorial recommendation as the problem of generating a sequence(ordered list) of items from a candidate set, with the target of maximizing the expected overall utility(e.g. total clicks) of the sequence. Toward solving this problem, we propose the Evaluation-Generation framework. On the one hand of this framework, an evaluation model is trained to evaluate the expected overall utility, by fully considering the user, item information and the correlations among the co-exposed items. On the other hand, generation policies based on heuristic searching or reinforcement learning are devised to generate potential high-quality sequences, from which the evaluation model select one to expose. We propose effective model architectures and learning metrics under this framework. We also offer series of offline tests to thoroughly investigate the performance of the proposed framework, as supplements to the online experiments. Our results show obvious increase in performance compared with the previous solutions.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>402</td>
        <td>TDM</td>
        <td><a href="">Learning Tree-based Deep Model for Recommender Systems</a></td>
        <td><details><summary>Abstract</summary><div>Model-based methods for recommender systems have been studied extensively in recent years. In systems with large corpus, however, the calculation cost for the learnt model to predict all useritem preferences is tremendous, which makes full corpus retrieval extremely difficult. To overcome the calculation barriers, models such as matrix factorization resort to inner product form (i.e., model user-item preference as the inner product of user, item latent factors) and indexes to facilitate efficient approximate k-nearest neighbor searches. However, it still remains challenging to incorporate more expressive interaction forms between user and item features, e.g., interactions through deep neural networks, because of the calculation cost. In this paper, we focus on the problem of introducing arbitrary advanced models to recommender systems with large corpus. We propose a novel tree-based method which can provide logarithmic complexity w.r.t. corpus size even with more expressive models such as deep neural networks. Our main idea is to predict user interests from coarse to fine by traversing tree nodes in a top-down fashion and making decisions for each user-node pair. We also show that the tree structure can be jointly learnt towards better compatibility with users’ interest distribution and hence facilitate both training and prediction. Experimental evaluations with two large-scale real-world datasets show that the proposed method significantly outperforms traditional methods. Online A/B test results in Taobao display advertising platform also demonstrate the effectiveness of the proposed method in production environments.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>403</td>
        <td>Tagspace</td>
        <td><a href="https://paperswithcode.com/paper/tagspace-semantic-embeddings-from-hashtags">TagSpace: Semantic Embeddings from Hashtags</a></td>
        <td><details><summary>Abstract</summary><div>We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal. The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags. As well as strong performance on the hashtag prediction task itself, we show that its learned representation of text (ignoring the hashtag labels) is useful for other tasks as well. To that end, we present results on a document recommendation task, where it also outperforms a number of baselines.</div></details></td>
        <td>ag_news</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>404</td>
        <td>Textcnn</td>
        <td><a href="">Convolutional neural networks for sentence classication</a></td>
        <td><details><summary>Abstract</summary><div>We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.</div></details></td>
        <td>Senta</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>405</td>
        <td>conformer offline/onl</br>ine</td>
        <td><a href="https://paperswithcode.com/paper/unified-streaming-and-non-streaming-two-pass">Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42% CER with 640ms latency in a streaming ASR system</div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>406</td>
        <td>transformer offline/o</br>nline</td>
        <td><a href="https://paperswithcode.com/paper/unified-streaming-and-non-streaming-two-pass">Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42% CER with 640ms latency in a streaming ASR system</div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>407</td>
        <td>deepspeech2 offline/o</br>nline</td>
        <td><a href="https://paperswithcode.com/paper/deep-speech-2-end-to-end-speech-recognition">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a></td>
        <td><details><summary>Abstract</summary><div>We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.</div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>408</td>
        <td>fastspeech2/fastpitch</td>
        <td><a href="https://paperswithcode.com/paper/fastspeech-2-fast-and-high-quality-end-to-end">FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a></td>
        <td><details><summary>Abstract</summary><div>Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al.,2019) can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more informationas input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speechvariations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated andtime-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit thevoice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTSby 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech(e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directlytake them as conditional inputs in training and use predicted values in inference.We further design FastSpeech 2s, which is the first attempt to directly generatespeech waveform from text in parallel, enjoying the benefit of fully end-to-endinference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inferencespeed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available athttps://speechresearch.github.io/fastspeech2/.</div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>409</td>
        <td>speedyspeech</td>
        <td><a href="https://paperswithcode.com/paper/speedyspeech-efficient-neural-speech">SpeedySpeech: Efficient Neural Speech Synthesis</a></td>
        <td><details><summary>Abstract</summary><div>While recent neural sequence-to-sequence models have greatly improved the quality of speech synthesis, there has not been a system capable of fast training, fast inference and high-quality audio synthesis at the same time. We propose a student-teacher network capable of high-quality faster-than-real-time spectrogram synthesis, with low requirements on computational resources and fast training time. We show that self-attention layers are not necessary for generation of high quality audio. We utilize simple convolutional blocks with residual connections in both student and teacher networks and use only a single attention layer in the teacher model. Coupled with a MelGAN vocoder, our model's voice quality was rated significantly higher than Tacotron 2. Our model can be efficiently trained on a single GPU and can run in real time even on a CPU. We provide both our source code and audio samples in our GitHub repository. </div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>410</td>
        <td>transformer_tts</td>
        <td><a href="https://paperswithcode.com/paper/neural-speech-synthesis-with-transformer">Neural Speech Synthesis with Transformer Network</a></td>
        <td><details><summary>Abstract</summary><div>Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-the- art performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we intro- duce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mecha- nism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves training efficiency. Meanwhile, any two inputs at different times are connected directly by a self-attention mechanism, which solves the long range de- pendency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spec- trograms, followed by a WaveNet vocoder to output the fi- nal audio results. Experiments are conducted to test the ef- ficiency and performance of our new network. For the effi- ciency, our Transformer TTS network can speed up the train- ing about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our pro- posed model achieves state-of-the-art performance (outper- forms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).</div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>411</td>
        <td>PP-Waveflow</td>
        <td><a href="https://paperswithcode.com/paper/waveflow-a-compact-flow-based-model-for-raw-1">WaveFlow: A Compact Flow-based Model for Raw Audio</a></td>
        <td><details><summary>Abstract</summary><div>In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood. It handles the long-range structure of 1-D waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions. WaveFlow provides a unified view of likelihood-based models for 1-D data, including WaveNet and WaveGlow as special cases. It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms with hundreds of thousands of time-steps. Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis. Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15× smaller than WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6× faster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered inference kernels. </div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>412</td>
        <td>Parallel WaveGAN</td>
        <td><a href="https://paperswithcode.com/paper/parallel-wavegan-a-fast-waveform-generation">PARALLEL WAVEGAN: A FAST WAVEFORM GENERATION MODEL BASED ON GENERATIVE ADVERSARIAL NETWORKS WITH MULTI-RESOLUTION SPECTROGRAM</a></td>
        <td><details><summary>Abstract</summary><div>We propose Parallel WaveGAN, a distillation-free, fast, and small- footprint waveform generation method using a generative adver- sarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectro- gram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained. Furthermore, our model is able to generate high- fidelity speech even with its compact architecture. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real- time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet sys- tem.</div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>413</td>
        <td>GE2E</td>
        <td><a href="https://paperswithcode.com/paper/generalized-end-to-end-loss-for-speaker">Generalized End-to-End Loss for Speaker Verification</a></td>
        <td><details><summary>Abstract</summary><div>In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e. "OK Google" and "Hey Google") as well as multiple dialects. </div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>414</td>
        <td>VoiceCloning</td>
        <td><a href="https://paperswithcode.com/paper/transfer-learning-from-speaker-verification">Transfer Learning from Speaker Verification toMultispeaker Text-To-Speech Synthesis</a></td>
        <td><details><summary>Abstract</summary><div>We describe a neural network-based system for text-to-speech (TTS) synthesis thatis able to generate speech audio in the voice of different speakers, including thoseunseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using anindependent dataset of noisy speech without transcripts from thousands of speakers,to generate a fixed-dimensional embedding vector from only seconds of referencespeech from a target speaker; (2) a sequence-to-sequence synthesis network basedon Tacotron 2 that generates a mel spectrogram from text, conditioned on thespeaker embedding; (3) an auto-regressive WaveNet-based vocoder network thatconverts the mel spectrogram into time domain waveform samples. We demonstratethat the proposed model is able to transfer the knowledge of speaker variabilitylearned by the discriminatively-trained speaker encoder to the multispeaker TTStask, and is able to synthesize natural speech from speakers unseen during training.We quantify the importance of training the speaker encoder on a large and diversespeaker set in order to obtain the best generalization performance. Finally, we showthat randomly sampled speaker embeddings can be used to synthesize speech inthe voice of novel speakers dissimilar from those used in training, indicating thatthe model has learned a high quality speaker representation.</div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>415</td>
        <td>tacotron2</td>
        <td><a href="https://paperswithcode.com/paper/neural-speech-synthesis-with-transformer">Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions</a></td>
        <td><details><summary>Abstract</summary><div>This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.</div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>416</td>
        <td>Attention LSTM</td>
        <td><a href="https://paperswithcode.com/paper/beyond-short-snippets-deep-networks-for-video">Beyond Short Snippets: Deep Networks for Video Classification</a></td>
        <td><details><summary>Abstract</summary><div>Convolutional neural networks (CNNs) have been exten- sively applied for image recognition problems giving state- of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image infor- mation across a video over longer time periods than previ- ously attempted. We propose two methods capable of han- dling full length videos. The first method explores various convolutional temporal feature pooling architectures, ex- amining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improve- ments over previously published results on the Sports 1 mil- lion dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 73.0%). </div></details></td>
        <td>Youtube8M, Hit@1: 89.0</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/attention_lstm.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>417</td>
        <td>TSM</td>
        <td><a href="https://paperswithcode.com/paper/temporal-shift-module-for-efficient-video">TSM: Temporal Shift Module for Efficient Video Understanding</a></td>
        <td><details><summary>Abstract</summary><div>Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.</div></details></td>
        <td>Top-1: 71.06</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/tsm.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>418</td>
        <td>PP-TSM</td>
        <td><a href="https://paperswithcode.com/paper/temporal-shift-module-for-efficient-video">TSM: Temporal Shift Module for Efficient Video Understanding</a></td>
        <td><details><summary>Abstract</summary><div>Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.</div></details></td>
        <td>k400, uniform, Top-1: 74.54</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/pp-tsm.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>419</td>
        <td>TSN</td>
        <td><a href="https://paperswithcode.com/paper/temporal-segment-networks-for-action">Temporal Segment Networks for Action Recognition in Video</a></td>
        <td><details><summary>Abstract</summary><div>Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.</div></details></td>
        <td>Top-1: 69.81</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/tsn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>420</td>
        <td>PP-TSN</td>
        <td><a href="https://paperswithcode.com/paper/temporal-segment-networks-for-action">Temporal Segment Networks for Action Recognition in Video</a></td>
        <td><details><summary>Abstract</summary><div>Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.</div></details></td>
        <td>Top-1: 75.06</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/pp-tsn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>421</td>
        <td>SlowFast</td>
        <td><a href="https://paperswithcode.com/paper/slowfast-networks-for-video-recognition">SlowFast Networks for Video Recognition</a></td>
        <td><details><summary>Abstract</summary><div>We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast</div></details></td>
        <td>k400, Top-1: 74.35</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/slowfast.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>422</td>
        <td>TimeSformer</td>
        <td><a href="https://paperswithcode.com/paper/is-space-time-attention-all-you-need-for">Is Space-Time Attention All You Need for Video Understanding?</a></td>
        <td><details><summary>Abstract</summary><div>We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.</div></details></td>
        <td>Top-1: 77.29</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/timesformer.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>423</td>
        <td>ST-GCN</td>
        <td><a href="https://paperswithcode.com/paper/spatial-temporal-graph-convolutional-networks-1">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</a></td>
        <td><details><summary>Abstract</summary><div>Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.</div></details></td>
        <td>ntu-rgbd, Top-1: 82.28</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/stgcn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>424</td>
        <td>AGCN</td>
        <td><a href="https://paperswithcode.com/paper/skeleton-based-action-recognition-with-multi">Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks</a></td>
        <td><details><summary>Abstract</summary><div>Graph convolutional networks (GCNs), which generalize CNNs to more generic non-Euclidean structures, have achieved remarkable performance for skeleton-based action recognition. However, there still exist several issues in the previous GCN-based models. First, the topology of the graph is set heuristically and fixed over all the model layers and input data. This may not be suitable for the hierarchy of the GCN model and the diversity of the data in action recognition tasks. Second, the second-order information of the skeleton data, i.e., the length and orientation of the bones, is rarely investigated, which is naturally more informative and discriminative for the human action recognition. In this work, we propose a novel multi-stream attention-enhanced adaptive graph convolutional neural network (MS-AAGCN) for skeleton-based action recognition. The graph topology in our model can be either uniformly or individually learned based on the input data in an end-to-end manner. This data-driven approach increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Besides, the proposed adaptive graph convolutional layer is further enhanced by a spatial-temporal-channel attention module, which helps the model pay more attention to important joints, frames and features. Moreover, the information of both the joints and bones, together with their motion information, are simultaneously modeled in a multi-stream framework, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.</div></details></td>
        <td>ntu-rgbd, Top-1: 83.27</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/agcn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>425</td>
        <td>BMN</td>
        <td><a href="https://paperswithcode.com/paper/bmn-boundary-matching-network-for-temporal">BMN: Boundary-Matching Network for Temporal Action Proposal Generation</a></td>
        <td><details><summary>Abstract</summary><div>Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.</div></details></td>
        <td>ActivityNet, AUC: 67.23</td>
        <td><a href="https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/localization/bmn.md">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>426</td>
        <td>IGSQL</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>427</td>
        <td>RAT-SQL</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>428</td>
        <td>BiGRU-CRF</td>
        <td><a href="https://paperswithcode.com/paper/chinese-lexical-analysis-with-deep-bi-gru-crf#code">Chinese Lexical Analysis with Deep Bi-GRU-CRF Network</a></td>
        <td><details><summary>Abstract</summary><div>Lexical analysis is believed to be a crucial step towards natural language understanding and has been widely studied. Recent years, end-to-end lexical analysis models with recurrent neural networks have gained increasing attention. In this report, we introduce a deep Bi-GRU-CRF network that jointly models word segmentation, part-of-speech tagging and named entity recognition tasks. We trained the model using several massive corpus pre-tagged by our best Chinese lexical analysis tool, together with a small, yet high-quality human annotated corpus. We conducted balanced sampling between different corpora to guarantee the influence of human annotations, and fine-tune the CRF decoding layer regularly during the training progress. As evaluated by linguistic experts, the model achieved a 95.5% accuracy on the test set, roughly 13% relative error reduction over our (previously) best Chinese lexical analysis tool. The model is computationally efficient, achieving the speed of 2.3K characters per second with one thread.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/lexical_analysis">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>429</td>
        <td>Deep Biaffine Attenti</br>on</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>430</td>
        <td>ERNIE-CSC</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>431</td>
        <td>PLATO-2</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>432</td>
        <td>Seq2Seq</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>433</td>
        <td>Transformer</td>
        <td><a href="https://paperswithcode.com/paper/attention-is-all-you-need">attention is all you need</a></td>
        <td><details><summary>Abstract</summary><div>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</div></details></td>
        <td></td>
        <td><a href="https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>434</td>
        <td>STACL</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>435</td>
        <td>SKEP</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>436</td>
        <td>SimNet</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>437</td>
        <td>Sentence-Transformer</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>438</td>
        <td>EFL</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>439</td>
        <td>PET</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>440</td>
        <td>P-Tuning</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>441</td>
        <td>Pointer Generator Net</br>work</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>442</td>
        <td>ERNIE</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>443</td>
        <td>ERNIE-DOC</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>444</td>
        <td>ERNIE-GEN</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>445</td>
        <td>ERNIE-GRAM</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>446</td>
        <td>RoFormer</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>447</td>
        <td>BART</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>448</td>
        <td>ALBERT</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>449</td>
        <td>BERT</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>450</td>
        <td>BigBird</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>451</td>
        <td>DistilBert</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>452</td>
        <td>ELECTRA</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>453</td>
        <td>GPT</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>454</td>
        <td>NeZha</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>455</td>
        <td>RoBERTa</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>
    <tr>
        <td>456</td>
        <td>MiniLMv2</td>
        <td><a href=""></a></td>
        <td><details><summary>Abstract</summary><div></div></details></td>
        <td></td>
        <td><a href="">快速开始</a></td>
        <td></td>
    </tr>

