 | 序号 | 模型简称 | 论文名称(链接) | 摘要 | 快速开始  |
 | --- | --- | --- | --- | --- |
| 1 | PP-ShiTu_mainbody_de</br>t | [PP-ShiTu: A Practical Lightweight Image Recognition System](https://paperswithcode.com/search?q_meta=&q_type=&q=ShiTu) | <details><summary>Abstract</summary><div>In recent years, image recognition applications have developed rapidly. A large number of studies and techniques have emerged in different fields, such as face recognition, pedestrian and vehicle re-identification, landmark retrieval, and product recognition. In this paper, we propose a practical lightweight image recognition system, named PP-ShiTu, consisting of the following 3 modules, mainbody detection, feature extraction and vector search. We introduce popular strategies including metric learning, deep hash, knowledge distillation and model quantization to improve accuracy and inference speed. With strategies above, PP-ShiTu works well in different scenarios with a set of models trained on a mixed dataset. Experiments on different datasets and benchmarks show that the system is widely effective in different domains of image recognition. All the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleClas on PaddlePaddle</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/quick_start/quick_start_recognition.md) | 
| 2 | PP-ShiTu_general_rec</br> | [PP-ShiTu: A Practical Lightweight Image Recognition System](https://paperswithcode.com/search?q_meta=&q_type=&q=ShiTu) | <details><summary>Abstract</summary><div>In recent years, image recognition applications have developed rapidly. A large number of studies and techniques have emerged in different fields, such as face recognition, pedestrian and vehicle re-identification, landmark retrieval, and product recognition. In this paper, we propose a practical lightweight image recognition system, named PP-ShiTu, consisting of the following 3 modules, mainbody detection, feature extraction and vector search. We introduce popular strategies including metric learning, deep hash, knowledge distillation and model quantization to improve accuracy and inference speed. With strategies above, PP-ShiTu works well in different scenarios with a set of models trained on a mixed dataset. Experiments on different datasets and benchmarks show that the system is widely effective in different domains of image recognition. All the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleClas on PaddlePaddle</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/quick_start/quick_start_recognition.md) | 
| 3 | PPLCNet_x0_25 | [PP-LCNet: A Lightweight CPU Convolutional Neural Networ](https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet) | <details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/models/PP-LCNet.md) | 
| 4 | PPLCNet_x0_35 | [PP-LCNet: A Lightweight CPU Convolutional Neural Networ](https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet) | <details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details> | [快速开始]() | 
| 5 | PPLCNet_x0_5 | [PP-LCNet: A Lightweight CPU Convolutional Neural Networ](https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet) | <details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details> | [快速开始]() | 
| 6 | PPLCNet_x0_75 | [PP-LCNet: A Lightweight CPU Convolutional Neural Networ](https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet) | <details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details> | [快速开始]() | 
| 7 | PPLCNet_x1_0 | [PP-LCNet: A Lightweight CPU Convolutional Neural Networ](https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet) | <details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details> | [快速开始]() | 
| 8 | PPLCNet_x1_5 | [PP-LCNet: A Lightweight CPU Convolutional Neural Networ](https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet) | <details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details> | [快速开始]() | 
| 9 | PPLCNet_x2_0 | [PP-LCNet: A Lightweight CPU Convolutional Neural Networ](https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet) | <details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details> | [快速开始]() | 
| 10 | PPLCNet_x2_5 | [PP-LCNet: A Lightweight CPU Convolutional Neural Networ](https://paperswithcode.com/search?q_meta=&q_type=&q=LCNet) | <details><summary>Abstract</summary><div>We propose a lightweight CPU network based on theMKLDNN acceleration strategy, named PP-LCNet, whichimproves the performance of lightweight models on multi-ple tasks. This paper lists technologies which can improvenetwork accuracy while the latency is almost constant. Withthese improvements, the accuracy of PP-LCNet can greatlysurpass the previous network structure with the same infer-ence time for classification. As shown in Figure 1, it outper-forms the most state-of-the-art models. And for downstreamtasks of computer vision, it also performs very well, such asobject detection, semantic segmentation, etc. All our exper-iments are implemented based on PaddlePaddle1. Code andpretrained models are available at PaddleClas2</div></details> | [快速开始]() | 
| 11 | SE_ResNeXt50_vd_32x4</br>d | [Squeeze-and-Excitation Networks](https://paperswithcode.com/model/seresnext?variant=seresnext50-32x4d) | <details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 12 | SE_ResNeXt50_32x4d | [Squeeze-and-Excitation Networks](https://paperswithcode.com/model/seresnext) | <details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 13 | SE_ResNet18_vd | [Squeeze-and-Excitation Networks](https://paperswithcode.com/paper/squeeze-and-excitation-networks) | <details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 14 | SE_ResNet34_vd | [Squeeze-and-Excitation Networks](https://paperswithcode.com/paper/squeeze-and-excitation-networks) | <details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 15 | SE_ResNet50_vd | [Squeeze-and-Excitation Networks](https://paperswithcode.com/paper/squeeze-and-excitation-networks) | <details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 16 | ResNet50_vd | [Squeeze-and-Excitation Networks](https://paperswithcode.com/paper/squeeze-and-excitation-networks) | <details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 17 | HRNet_W18_C | [Deep High-Resolution Representation Learning for Visual Recognition](https://paperswithcode.com/method/hrnet) | <details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 18 | HRNet_W30_C | [Deep High-Resolution Representation Learning for Visual Recognition](https://paperswithcode.com/method/hrnet) | <details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 19 | HRNet_W32_C | [Deep High-Resolution Representation Learning for Visual Recognition](https://paperswithcode.com/method/hrnet) | <details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 20 | HRNet_W40_C | [Deep High-Resolution Representation Learning for Visual Recognition](https://paperswithcode.com/method/hrnet) | <details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 21 | HRNet_W44_C | [Deep High-Resolution Representation Learning for Visual Recognition](https://paperswithcode.com/method/hrnet) | <details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 22 | HRNet_W48_C | [Deep High-Resolution Representation Learning for Visual Recognition](https://paperswithcode.com/method/hrnet) | <details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 23 | HRNet_W64_C | [Deep High-Resolution Representation Learning for Visual Recognition](https://paperswithcode.com/method/hrnet) | <details><summary>Abstract</summary><div>High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{this https URL}}. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 24 | SE_ResNeXt101_32x4d | [Squeeze-and-Excitation Networks](https://paperswithcode.com/paper/squeeze-and-excitation-networks) | <details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 25 | SENet154_vd | [Squeeze-and-Excitation Networks](https://paperswithcode.com/paper/squeeze-and-excitation-networks) | <details><summary>Abstract</summary><div>The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 26 | GoogLeNet | [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://paperswithcode.com/model/inception-v4?variant=inception-v4-1) | <details><summary>Abstract</summary><div>Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 27 | InceptionV3 | [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://paperswithcode.com/model/inception-v4?variant=inception-v4-1) | <details><summary>Abstract</summary><div>Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 28 | InceptionV4 | [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://paperswithcode.com/model/inception-v4?variant=inception-v4-1) | <details><summary>Abstract</summary><div>Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 29 | ResNet18 | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 30 | ResNet18_vd | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 31 | ResNet34 | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 32 | ResNet34_vd | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 33 | ResNet50 | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 34 | ResNet50_vd | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 35 | ResNet50_vd-FPGM | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 36 | ResNet50_vd-PACT | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 37 | ResNet50_vd-KL | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 38 | ResNet101 | [Adaptively Connected Neural Networks](https://paperswithcode.com/paper/adaptively-connected-neural-networks) | <details><summary>Abstract</summary><div> This paper presents a novel adaptively connected neural network (ACNet) to improve the traditional convolutional neural networks (CNNs) {in} two aspects. First, ACNet employs a flexible way to switch global and local inference in processing the internal feature representations by adaptively determining the connection status among the feature nodes (e.g., pixels of the feature maps) \footnote{In a computer vision domain, a node refers to a pixel of a feature map{, while} in {the} graph domain, a node denotes a graph node.}. We can show that existing CNNs, the classical multilayer perceptron (MLP), and the recently proposed non-local network (NLN) \cite{nonlocalnn17} are all special cases of ACNet. Second, ACNet is also capable of handling non-Euclidean data. Extensive experimental analyses on {a variety of benchmarks (i.e.,} ImageNet-1k classification, COCO 2017 detection and segmentation, CUHK03 person re-identification, CIFAR analysis, and Cora document categorization) demonstrate that {ACNet} cannot only achieve state-of-the-art performance but also overcome the limitation of the conventional MLP and CNN \footnote{Corresponding author: Liang Lin (linliang@ieee.org)}. The code is available at \url{this https URL}. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 39 | ResNet101_vd | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 40 | ResNet152 | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 41 | ResNet152_vd | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 42 | ResNet200_vd | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 43 | Res2Net50_26w_4s | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 44 | Res2Net50_14w_8s | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 45 | Res2Net50_vd_26w_4s | [Deep Residual Learning for Image Recognition](https://paperswithcode.com/model/resnet) | <details><summary>Abstract</summary><div>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 46 | Res2Net101_vd_26w_4s</br> | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 47 | Res2Net200_vd_26w_4s</br> | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 48 | ResNeXt50_32x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 49 | ResNeXt50_64x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 50 | ResNeXt50_vd_32x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 51 | ResNeXt50_vd_64x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 52 | ResNeXt101_32x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 53 | ResNeXt101_64x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 54 | ResNeXt101_vd_32x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 55 | ResNeXt101_vd_64x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 56 | ResNeXt152_32x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 57 | ResNeXt152_64x4d | [Res2Net: A New Multi-scale Backbone Architecture](﻿﻿﻿https://paperswithcode.com/model/res2net) | <details><summary>Abstract</summary><div>﻿    Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 58 | ResNeXt152_vd_32x4d | [Aggregated Residual Transformations for Deep Neural Networks](https://paperswithcode.com/model/resnext) | <details><summary>Abstract</summary><div>We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 59 | ResNeXt152_vd_64x4d | [Aggregated Residual Transformations for Deep Neural Networks](https://paperswithcode.com/model/resnext) | <details><summary>Abstract</summary><div>We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 60 | DenseNet121 | [Aggregated Residual Transformations for Deep Neural Networks](https://paperswithcode.com/model/resnext) | <details><summary>Abstract</summary><div>We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 61 | DenseNet161 | [Densely Connected Convolutional Networks](https://paperswithcode.com/paper/densely-connected-convolutional-networks) | <details><summary>Abstract</summary><div>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL . </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 62 | DenseNet169 | [Densely Connected Convolutional Networks](https://paperswithcode.com/paper/densely-connected-convolutional-networks) | <details><summary>Abstract</summary><div>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL . </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 63 | DenseNet201 | [Densely Connected Convolutional Networks](https://paperswithcode.com/paper/densely-connected-convolutional-networks) | <details><summary>Abstract</summary><div>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL . </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 64 | DenseNet264 | [Densely Connected Convolutional Networks](https://paperswithcode.com/paper/densely-connected-convolutional-networks) | <details><summary>Abstract</summary><div>Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL . </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 65 | DPN68 | [Dual Path Networks](https://paperswithcode.com/paper/dual-path-networks) | <details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 66 | DPN92 | [Dual Path Networks](https://paperswithcode.com/paper/dual-path-networks) | <details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 67 | DPN98 | [Dual Path Networks](https://paperswithcode.com/paper/dual-path-networks) | <details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 68 | DPN107 | [Dual Path Networks](https://paperswithcode.com/paper/dual-path-networks) | <details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 69 | DPN131 | [Dual Path Networks](https://paperswithcode.com/paper/dual-path-networks) | <details><summary>Abstract</summary><div>In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 70 | VGG11 | [](https://paperswithcode.com/model/vgg) | <details><summary>Abstract</summary><div>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 71 | VGG13 | [](https://paperswithcode.com/model/vgg) | <details><summary>Abstract</summary><div>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 72 | VGG16 | [](https://paperswithcode.com/model/vgg) | <details><summary>Abstract</summary><div>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 73 | VGG19 | [](https://paperswithcode.com/model/vgg) | <details><summary>Abstract</summary><div>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 74 | AlexNet | [ImageNet Classification with Deep Convolutional Neural Networks](https://paperswithcode.com/model/alexnet) | <details><summary>Abstract</summary><div>We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 75 | Xception41 | [Xception: Deep Learning with Depthwise Separable Convolutions](https://paperswithcode.com/model/xception?variant=xception-1) | <details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 76 | Xception65 | [Xception: Deep Learning with Depthwise Separable Convolutions](https://paperswithcode.com/model/xception?variant=xception-1) | <details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 77 | Xception71 | [Xception: Deep Learning with Depthwise Separable Convolutions](https://paperswithcode.com/model/xception?variant=xception-1) | <details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 78 | Xception41_deeplab | [Xception: Deep Learning with Depthwise Separable Convolutions](https://paperswithcode.com/model/xception?variant=xception-1) | <details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 79 | Xception65_deeplab | [Xception: Deep Learning with Depthwise Separable Convolutions](https://paperswithcode.com/model/xception?variant=xception-1) | <details><summary>Abstract</summary><div>We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 80 | DarkNet | [YOLOv3: An Incremental Improvement](https://paperswithcode.com/method/darknet-53) | <details><summary>Abstract</summary><div>We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 81 | EfficientNetB0 | [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://paperswithcode.com/method/efficientnet) | <details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 82 | EfficientNetB1 | [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://paperswithcode.com/method/efficientnet) | <details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 83 | EfficientNetB2 | [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://paperswithcode.com/method/efficientnet) | <details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 84 | EfficientNetB3 | [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://paperswithcode.com/method/efficientnet) | <details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 85 | EfficientNetB4 | [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://paperswithcode.com/method/efficientnet) | <details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 86 | EfficientNetB5 | [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://paperswithcode.com/method/efficientnet) | <details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 87 | EfficientNetB6 | [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://paperswithcode.com/method/efficientnet) | <details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 88 | EfficientNetB7 | [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://paperswithcode.com/method/efficientnet) | <details><summary>Abstract</summary><div>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 89 | SqueezeNet1_0 | [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](https://paperswithcode.com/method/squeezenet) | <details><summary>Abstract</summary><div>Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).The SqueezeNet architecture is available for download here: this https URL</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 90 | SqueezeNet1_1 | [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](https://paperswithcode.com/method/squeezenet) | <details><summary>Abstract</summary><div>Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).The SqueezeNet architecture is available for download here: this https URL</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 91 | MobileNetV1 | [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://paperswithcode.com/method/mobilenetv1) | <details><summary>Abstract</summary><div> We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 92 | MobileNetV1_x0_25 | [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://paperswithcode.com/method/mobilenetv1) | <details><summary>Abstract</summary><div> We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 93 | MobileNetV1_x0_5 | [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://paperswithcode.com/method/mobilenetv1) | <details><summary>Abstract</summary><div> We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 94 | MobileNetV1_x0_75 | [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://paperswithcode.com/method/mobilenetv1) | <details><summary>Abstract</summary><div> We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 95 | MobileNetV2 | [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://paperswithcode.com/method/mobilenetv2) | <details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 96 | MobileNetV2_x0_25 | [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://paperswithcode.com/method/mobilenetv2) | <details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 97 | MobileNetV2_x0_5 | [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://paperswithcode.com/method/mobilenetv2) | <details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 98 | MobileNetV2_x0_75 | [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://paperswithcode.com/method/mobilenetv2) | <details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 99 | MobileNetV2_x1_5 | [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://paperswithcode.com/method/mobilenetv2) | <details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 100 | MobileNetV2_x2_0 | [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://paperswithcode.com/method/mobilenetv2) | <details><summary>Abstract</summary><div>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 101 | MobileNetV3_large_x0</br>_35 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 102 | MobileNetV3_large_x0</br>_5 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 103 | MobileNetV3_large_x0</br>_75 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 104 | MobileNetV3_large_x1</br>_0 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleClas/blob/release/2.3/docs/zh_CN/ImageNet_models_cn.md) | 
| 105 | MobileNetV3_large_x1</br>_0-FPGM | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 106 | MobileNetV3_large_x1</br>_0-PACT | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 107 | MobileNetV3_large_x1</br>_0-KL | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 108 | MobileNetV3_large_x1</br>_25 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始]() | 
| 109 | MobileNetV3_small_x0</br>_35 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始]() | 
| 110 | MobileNetV3_small_x0</br>_5 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始]() | 
| 111 | MobileNetV3_small_x0</br>_75 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始]() | 
| 112 | MobileNetV3_small_x1</br>_0 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始]() | 
| 113 | MobileNetV3_small_x1</br>_25 | [Searching for MobileNetV3](https://paperswithcode.com/paper/searching-for-mobilenetv3) | <details><summary>Abstract</summary><div>We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation. </div></details> | [快速开始]() | 
| 114 | ShuffleNetV2_swish | [ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://paperswithcode.com/method/shufflenet-v2) | <details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details> | [快速开始]() | 
| 115 | ShuffleNetV2_x0_25 | [ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://paperswithcode.com/method/shufflenet-v2) | <details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details> | [快速开始]() | 
| 116 | ShuffleNetV2_x0_33 | [ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://paperswithcode.com/method/shufflenet-v2) | <details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details> | [快速开始]() | 
| 117 | ShuffleNetV2_x0_5 | [ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://paperswithcode.com/method/shufflenet-v2) | <details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details> | [快速开始]() | 
| 118 | ShuffleNetV2_x1_0 | [ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://paperswithcode.com/method/shufflenet-v2) | <details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details> | [快速开始]() | 
| 119 | ShuffleNetV2_x1_5 | [ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://paperswithcode.com/method/shufflenet-v2) | <details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details> | [快速开始]() | 
| 120 | ShuffleNetV2_x2_0 | [ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://paperswithcode.com/method/shufflenet-v2) | <details><summary>Abstract</summary><div>Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff. </div></details> | [快速开始]() | 
| 121 | CSPDarkNet53 | [CSPNet: A New Backbone that can Enhance Learning Capability of CNN](https://paperswithcode.com/model/csp-resnet?variant=cspresnet50) | <details><summary>Abstract</summary><div>Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at this https URL. </div></details> | [快速开始]() | 
| 122 | GhostNet_x0_5 | [GhostNet: More Features from Cheap Operations](https://paperswithcode.com/method/ghostnet) | <details><summary>Abstract</summary><div>Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at this https URL</div></details> | [快速开始]() | 
| 123 | GhostNet_x1_0 | [GhostNet: More Features from Cheap Operations](https://paperswithcode.com/method/ghostnet) | <details><summary>Abstract</summary><div>Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at this https URL</div></details> | [快速开始]() | 
| 124 | GhostNet_x1_3 | [GhostNet: More Features from Cheap Operations](https://paperswithcode.com/method/ghostnet) | <details><summary>Abstract</summary><div>Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at this https URL</div></details> | [快速开始]() | 
| 125 | RegNet | [RegNet: Self-Regulated Network for Image Classification](https://paperswithcode.com/paper/regnet-self-regulated-network-for-image) | <details><summary>Abstract</summary><div>The ResNet and its variants have achieved remarkable successes in various computer vision tasks. Despite its success in making gradient flow through building blocks, the simple shortcut connection mechanism limits the ability of re-exploring new potentially complementary features due to the additive function. To address this issue, in this paper, we propose to introduce a regulator module as a memory mechanism to extract complementary features, which are further fed to the ResNet. In particular, the regulator module is composed of convolutional RNNs (e.g., Convolutional LSTMs or Convolutional GRUs), which are shown to be good at extracting Spatio-temporal information. We named the new regulated networks as RegNet. The regulator module can be easily implemented and appended to any ResNet architecture. We also apply the regulator module for improving the Squeeze-and-Excitation ResNet to show the generalization ability of our method. Experimental results on three image classification datasets have demonstrated the promising performance of the proposed architecture compared with the standard ResNet, SE-ResNet, and other state-of-the-art architectures.</div></details> | [快速开始]() | 
| 126 | ViT_base_patch16_224</br> | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1) | <details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details> | [快速开始]() | 
| 127 | ViT_base_patch16_384</br> | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1) | <details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details> | [快速开始]() | 
| 128 | ViT_base_patch32_384</br> | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1) | <details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details> | [快速开始]() | 
| 129 | ViT_huge_patch16_224</br> | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1) | <details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details> | [快速开始]() | 
| 130 | ViT_huge_patch32_384</br> | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1) | <details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details> | [快速开始]() | 
| 131 | ViT_large_patch16_22</br>4 | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1) | <details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details> | [快速开始]() | 
| 132 | ViT_large_patch16_38</br>4 | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1) | <details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details> | [快速开始]() | 
| 133 | ViT_large_patch32_38</br>4 | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1) | <details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details> | [快速开始]() | 
| 134 | ViT_small_patch16_22</br>4 | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1) | <details><summary>Abstract</summary><div>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</div></details> | [快速开始]() | 
| 135 | DeiT_base_patch16_22</br>4 | [Training data-efficient image transformers & distillation through attention](https://paperswithcode.com/method/deit) | <details><summary>Abstract</summary><div>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.</div></details> | [快速开始]() | 
| 136 | DeiT_base_patch16_38</br>4 | [Training data-efficient image transformers & distillation through attention](https://paperswithcode.com/method/deit) | <details><summary>Abstract</summary><div>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.</div></details> | [快速开始]() | 
| 137 | DeiT_small_patch16_2</br>24 | [Training data-efficient image transformers & distillation through attention](https://paperswithcode.com/method/deit) | <details><summary>Abstract</summary><div>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.</div></details> | [快速开始]() | 
| 138 | DeiT_tiny_patch16_22</br>4 | [Training data-efficient image transformers & distillation through attention](https://paperswithcode.com/method/deit) | <details><summary>Abstract</summary><div>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.</div></details> | [快速开始]() | 
| 139 | SwinTransformer_base</br>_patch4_window12_384 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://paperswithcode.com/paper/swin-transformer-hierarchical-vision) | <details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details> | [快速开始]() | 
| 140 | SwinTransformer_base</br>_patch4_window7_224 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://paperswithcode.com/paper/swin-transformer-hierarchical-vision) | <details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details> | [快速开始]() | 
| 141 | SwinTransformer_larg</br>e_patch4_window12_384 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://paperswithcode.com/paper/swin-transformer-hierarchical-vision) | <details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details> | [快速开始]() | 
| 142 | SwinTransformer_larg</br>e_patch4_window7_224 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://paperswithcode.com/paper/swin-transformer-hierarchical-vision) | <details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details> | [快速开始]() | 
| 143 | SwinTransformer_smal</br>l_patch4_window7_224 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://paperswithcode.com/paper/swin-transformer-hierarchical-vision) | <details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details> | [快速开始]() | 
| 144 | SwinTransformer_tiny</br>_patch4_window7_224 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://paperswithcode.com/paper/swin-transformer-hierarchical-vision) | <details><summary>Abstract</summary><div>This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{this https URL}.</div></details> | [快速开始]() | 
| 145 | ch_ppocr_mobile_v2.0</br>_det | [PP-OCR: A Practical Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocr-a-practical-ultra-lightweight-ocr) | <details><summary>Abstract</summary><div>The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., this https URL.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.2/doc/doc_ch/quickstart.md) | 
| 146 | ch_ppocr_mobile_v2.0</br>_det_FPGM | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 147 | ch_ppocr_mobile_v2.0</br>_det_PACT | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 148 | ch_ppocr_mobile_v2.0</br>_det_KL | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 149 | ch_ppocr_mobile_v2.0</br>_rec | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 150 | ch_ppocr_mobile_v2.0</br>_rec_FPGM | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 151 | ch_ppocr_mobile_v2.0</br>_rec_PACT | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 152 | ch_ppocr_mobile_v2.0</br>_rec_KL | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 153 | ch_ppocr_mobile_v2.0</br> | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 154 | ch_ppocr_server_v2.0</br>_det | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 155 | ch_ppocr_server_v2.0</br>_rec | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 156 | ch_ppocr_server_v2.0</br> | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 157 | ch_PP-OCRv2_det	 | [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight) | <details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md) | 
| 158 | ch_PP-OCRv2_det_FPGM</br> | [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight) | <details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md) | 
| 159 | ch_PP-OCRv2_det_PACT</br> | [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight) | <details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md) | 
| 160 | ch_PP-OCRv2_det_KL | [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight) | <details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md) | 
| 161 | ch_PP-OCRv2_rec | [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight) | <details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md) | 
| 162 | ch_PP-OCRv2_rec_FPGM</br> | [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight) | <details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md) | 
| 163 | ch_PP-OCRv2_rec_PACT</br> | [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight) | <details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md) | 
| 164 | ch_PP-OCRv2_rec_KL | [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight) | <details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md) | 
| 165 | ch_PP-OCRv2 | [PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System](https://paperswithcode.com/paper/pp-ocrv2-bag-of-tricks-for-ultra-lightweight) | <details><summary>Abstract</summary><div>Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/quickstart.md) | 
| 166 | det_mv3_db_v2.0 | [Real-time Scene Text Detection with Differentiable Binarization](https://paperswithcode.com/paper/real-time-scene-text-detection-with) | <details><summary>Abstract</summary><div>Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: this https URL</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 167 | det_r50_vd_db_v2.0 | [Real-time Scene Text Detection with Differentiable Binarization](https://paperswithcode.com/paper/real-time-scene-text-detection-with) | <details><summary>Abstract</summary><div>Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: this https URL</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 168 | det_mv3_east_v2.0 | [EAST: an efficient and accurate scene text detector](https://paperswithcode.com/paper/east-an-efficient-and-accurate-scene-text) | <details><summary>Abstract</summary><div>Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 169 | det_r50_vd_east_v2.0</br> | [EAST: an efficient and accurate scene text detector](https://paperswithcode.com/paper/east-an-efficient-and-accurate-scene-text) | <details><summary>Abstract</summary><div>Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 170 | det_r50_vd_sast_icda</br>r15_v2.0 | [A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning](https://paperswithcode.com/paper/a-single-shot-arbitrarily-shaped-text) | <details><summary>Abstract</summary><div>Detecting scene text of arbitrary shapes has been a challenging task over the past years. In this paper, we propose a novel segmentation-based text detector, namely SAST, which employs a context attended multi-task learning framework based on a Fully Convolutional Network (FCN) to learn various geometric properties for the reconstruction of polygonal representation of text regions. Taking sequential characteristics of text into consideration, a Context Attention Block is introduced to capture long-range dependencies of pixel information to obtain a more reliable segmentation. In post-processing, a Point-to-Quad assignment method is proposed to cluster pixels into text instances by integrating both high-level object knowledge and low-level pixel information in a single shot. Moreover, the polygonal representation of arbitrarily-shaped text can be extracted with the proposed geometric properties much more effectively. Experiments on several benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves better or comparable performance in terms of accuracy. Furthermore, the proposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0% on a single NVIDIA Titan Xp graphics card, surpassing most of the existing segmentation-based methods.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 171 | det_r50_vd_sast_tota</br>ltext_v2.0 | [A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning](https://paperswithcode.com/paper/a-single-shot-arbitrarily-shaped-text) | <details><summary>Abstract</summary><div>Detecting scene text of arbitrary shapes has been a challenging task over the past years. In this paper, we propose a novel segmentation-based text detector, namely SAST, which employs a context attended multi-task learning framework based on a Fully Convolutional Network (FCN) to learn various geometric properties for the reconstruction of polygonal representation of text regions. Taking sequential characteristics of text into consideration, a Context Attention Block is introduced to capture long-range dependencies of pixel information to obtain a more reliable segmentation. In post-processing, a Point-to-Quad assignment method is proposed to cluster pixels into text instances by integrating both high-level object knowledge and low-level pixel information in a single shot. Moreover, the polygonal representation of arbitrarily-shaped text can be extracted with the proposed geometric properties much more effectively. Experiments on several benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves better or comparable performance in terms of accuracy. Furthermore, the proposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0% on a single NVIDIA Titan Xp graphics card, surpassing most of the existing segmentation-based methods.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 172 | rec_mv3_none_bilstm_</br>ctc_v2.0 | [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition) | <details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 173 | rec_r34_vd_none_bils</br>tm_ctc_v2.0 | [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition) | <details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 174 | rec_mv3_none_none_ct</br>c_v2.0 | [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition) | <details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 175 | rec_r34_vd_none_none</br>_ctc_v2.0 | [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition) | <details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 176 | rec_mv3_tps_bilstm_a</br>tt_v2.0 | [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition) | <details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 177 | rec_r34_vd_tps_bilst</br>m_att_v2.0 | [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition) | <details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 178 | rec_mv3_tps_bilstm_c</br>tc_v2.0 | [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition) | <details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 179 | rec_r34_vd_tps_bilst</br>m_ctc_v2.0 | [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://paperswithcode.com/paper/what-is-wrong-with-scene-text-recognition) | <details><summary>Abstract</summary><div>Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 180 | rec_r50_vd_srn | [Towards Accurate Scene Text Recognition with Semantic Reasoning Networks](https://paperswithcode.com/paper/towards-accurate-scene-text-recognition-with) | <details><summary>Abstract</summary><div>Scene text image contains two levels of contents: visual texture and semantic information. Although the previous scene text recognition methods have made great progress over the past few years, the research on mining semantic information to assist text recognition attracts less attention, only RNN-like structures are explored to implicitly model semantic information. However, we observe that RNN based methods have some obvious shortcomings, such as time-dependent decoding manner and one-way serial transmission of semantic context, which greatly limit the help of semantic information and the computation efficiency. To mitigate these limitations, we propose a novel end-to-end trainable framework named semantic reasoning network (SRN) for accurate scene text recognition, where a global semantic reasoning module (GSRM) is introduced to capture global semantic context through multi-way parallel transmission. The state-of-the-art results on 7 public benchmarks, including regular text, irregular text and non-Latin long text, verify the effectiveness and robustness of the proposed method. In addition, the speed of SRN has significant advantages over the RNN based methods, demonstrating its value in practical use.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.3/doc/doc_ch/algorithm_overview.md) | 
| 181 | PP-Structure-layout | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 182 | PP-Structure-chartre</br>c | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 183 | PP-Structure | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 184 | ppyolo_mbv3_small_co</br>co | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 185 | ppyolo_r18vd_coco | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 186 | ppyolo_tiny_650e_coc</br>o | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 187 | ppyolov2_r101vd_dcn_</br>365e_coco | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 188 | picodet_s_320_coco | [PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 189 | picodet_m_416_coco | [PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 190 | picodet_l_640_coco | [PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 191 | picodet_lcnet_1_5x_4</br>16_coco | [PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 192 | picodet_mobilenetv3_</br>large_1x_416_coco | [PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 193 | picodet_r18_640_coco</br> | [PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 194 | picodet_shufflenetv2</br>_1x_416_coco | [PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 195 | tinypose_128x96 | [](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 196 | ssdlite_mobilenet_v1</br>_300_coco | [SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 197 | faster_rcnn_r50_fpn_</br>1x_coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 198 | faster_rcnn_swin_tin</br>y_fpn_1x_coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 199 | faster_rcnn_r34_fpn_</br>1x_coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 200 | faster_rcnn_r34_vd_f</br>pn_1x_coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 201 | faster_rcnn_r50_1x_c</br>oco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 202 | faster_rcnn_r50_vd_1</br>x_coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 203 | faster_rcnn_r50_vd_f</br>pn_1x_coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 204 | faster_rcnn_r101_1x_</br>coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 205 | faster_rcnn_r101_fpn</br>_1x_coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 206 | faster_rcnn_r101_vd_</br>fpn_1x_coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 207 | faster_rcnn_x101_vd_</br>64x4d_fpn_1x_coco | [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks]() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 208 | fcos_r50_fpn_1x_coco</br> | [FCOS: Fully Convolutional One-Stage Object Detection](https://github.com/tianzhi0549/FCOS/) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 209 | fcos_dcn_r50_fpn_1x_</br>coco | [FCOS: Fully Convolutional One-Stage Object Detection](https://github.com/tianzhi0549/FCOS/) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 210 | yolov3_mobilenet_v1_</br>270e_coco | [YOLOv3: An Incremental Improvement](https://pjreddie.com/darknet/yolo/) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 211 | yolov3_mobilenet_v3_</br>large_270e_coco | [YOLOv3: An Incremental Improvement](https://pjreddie.com/darknet/yolo/) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 212 | yolov3_r34_270e_coco</br> | [YOLOv3: An Incremental Improvement](https://pjreddie.com/darknet/yolo/) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 213 | yolov3_r50vd_dcn_270</br>e_coco | [YOLOv3: An Incremental Improvement](https://pjreddie.com/darknet/yolo/) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 214 | ttfnet_darknet53_1x_</br>coco | [Training-Time-Friendly Network for Real-Time Object Detection](https://github.com/ZJULearning/ttfnet) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 215 | cascade_rcnn_r50_fpn</br>_1x_coco | [Cascade R-CNN: Delving into High Quality Object Detection](https://github.com/zhaoweicai/cascade-rcnn) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 216 | cascade_rcnn_r50_vd_</br>fpn_ssld_1x_coco | [Cascade R-CNN: Delving into High Quality Object Detection](https://github.com/zhaoweicai/cascade-rcnn) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 217 | cascade_mask_rcnn_r5</br>0_fpn_1x_coco | [Cascade R-CNN: High Quality Object Detection and Instance Segmentation](https://github.com/zhaoweicai/Detectron-Cascade-RCNN) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 218 | cascade_mask_rcnn_r5</br>0_vd_fpn_ssld_1x_coco | [Cascade R-CNN: High Quality Object Detection and Instance Segmentation](https://github.com/zhaoweicai/Detectron-Cascade-RCNN) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 219 | blazeface_1000e | [BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 220 | blazeface_fpn_ssh_10</br>00e | [BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 221 | s2anet_conv_2x_dota | [Align Deep Features for Oriented Object Detection](https://github.com/csuhan/s2anet) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 222 | s2anet_alignconv_2x_</br>dota | [Align Deep Features for Oriented Object Detection](https://github.com/csuhan/s2anet) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 223 | s2anet_1x_spine | [Align Deep Features for Oriented Object Detection](https://github.com/csuhan/s2anet) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 224 | solov2_r50_fpn_1x_co</br>co | [SOLOv2: Dynamic, Faster and Stronger](https://git.io/AdelaiDet) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 225 | solov2_r50_enhance_c</br>oco | [SOLOv2: Dynamic, Faster and Stronger](https://git.io/AdelaiDet) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 226 | solov2_r101_vd_fpn_3</br>x_coco | [SOLOv2: Dynamic, Faster and Stronger](https://git.io/AdelaiDet) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 227 | mask_rcnn_r50_fpn_1x</br>_coco | [Mask R-CNN](https://github.com/facebookresearch/Detectron) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 228 | mask_rcnn_r50_1x_coc</br>o | [Mask R-CNN](https://github.com/facebookresearch/Detectron) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 229 | mask_rcnn_r50_vd_fpn</br>_1x_coco | [Mask R-CNN](https://github.com/facebookresearch/Detectron) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 230 | mask_rcnn_r101_fpn_1</br>x_coco | [Mask R-CNN](https://github.com/facebookresearch/Detectron) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 231 | mask_rcnn_r101_vd_fp</br>n_1x_coco | [Mask R-CNN](https://github.com/facebookresearch/Detectron) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 232 | mask_rcnn_x101_vd_64</br>x4d_fpn_1x_coco | [Mask R-CNN](https://github.com/facebookresearch/Detectron) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 233 | hrnet_w32_256x192 | [Deep High-Resolution Representation Learning for Human Pose Estimation]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 234 | dark_hrnet_w32_256x1</br>92 | [Deep High-Resolution Representation Learning for Human Pose Estimation]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 235 | dark_hrnet_w48_256x1</br>92 | [Deep High-Resolution Representation Learning for Human Pose Estimation]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 236 | higherhrnet_hrnet_w3</br>2_512 | [HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 237 | fairmot_dla34_30e_57</br>6x320 | [FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 238 | fairmot_hrnetv2_w18_</br>dlafpn_30e_576x320 | [FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 239 | jde_darknet53_30e_57</br>6x320 | [Towards Real-Time Multi-Object Tracking]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 240 | yolov3_darknet53_270</br>e_coco | [YOLOv3: An Incremental Improvement](https://pjreddie.com/darknet/yolo/) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 241 | yolov3_darknet53-FPG</br>M | [YOLOv3: An Incremental Improvement](https://pjreddie.com/darknet/yolo/) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 242 | yolov3_darknet53-PAC</br>T | [YOLOv3: An Incremental Improvement](https://pjreddie.com/darknet/yolo/) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 243 | yolov3_darknet53-KL | [YOLOv3: An Incremental Improvement](https://pjreddie.com/darknet/yolo/) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 244 | ppyolo_mbv3_large_co</br>co | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 245 | ppyolo_mbv3-FPGM | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 246 | ppyolo_mbv3-PACT | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 247 | ppyolo_mbv3-KL | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 248 | ppyolo_r50vd_dcn_1x_</br>coco | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 249 | ppyolo_r50vd_dcn-FPG</br>M | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 250 | ppyolo_r50vd_dcn-PAC</br>T | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 251 | ppyolo_r50vd_dcn-KL | [PP-YOLO: An Effective and Efficient Implementation of Object Detector](https://github.com/PaddlePaddle/PaddleDetection) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 252 | ppyolov2_r50vd_dcn_3</br>65e_coco | [PP-YOLOv2: A Practical Object Detector]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 253 | Deformable DETR | [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://github.com/fundamentalvision/Deformable-DETR) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 254 | DETR | [DETR: End-to-End Object Detection with Transformers](https://github.com/facebookresearch/detr) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 255 | Sparse R-CNN | [Sparse R-CNN: End-to-End Object Detection with Learnable Proposals](https://github.com/PeizeSun/SparseR-CNN) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 256 | RetinaNet | [Focal Loss for Dense Object Detection](https://github.com/facebookresearch/Detectron) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 257 | CornerNetLite | [CornerNet: Detecting Objects as Paired Keypoints]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 258 | EfficientDet | [EfficientDet: Scalable and Efficient Object Detection](https://github.com/google/automl/tree/master/efficientdet) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 259 | Faceboxes | [FaceBoxes: A CPU Real-time Face Detector with High Accuracy]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 260 | Libra R-CNN | [Libra R-CNN: Towards Balanced Learning for Object Detection]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 261 | PyramidBox | [PyramidBox: A Context-assisted Single Shot Face Detector]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/docs/tutorials/GETTING_STARTED_cn.md) | 
| 262 | PP-HumanSeg-Server(D</br>eepLabv3+) | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 263 | PP-HumanSeg-Lite | [无]() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleSeg/blob/develop/docs/whole_process.md) | 
| 264 | PP-HumanMatting | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 265 | U-Net | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 266 | ICNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 267 | PSPNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 268 | HRNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 269 | Fast-SCNN | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 270 | OCRNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 271 | ANN | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 272 | Deeplabv3 | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 273 | GCNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 274 | BiSeNetv2 | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 275 | DANet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 276 | HarDNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 277 | DecoupledSegNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 278 | ISANet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 279 | SFNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 280 | DNLNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 281 | GSCNN | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 282 | EMANet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 283 | Attention-UNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 284 | U^2-Net | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 285 | UNet++ | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 286 | UNet+++ | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 287 | SETR | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 288 | SwinTransformer | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 289 | SegFormer | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 290 | STDC | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 291 | PP-MSVSR | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 292 | Pix2Pix | [Image-to-Image Translation with Conditional Adversarial Networks](https://paperswithcode.com/paper/image-to-image-translation-with-conditional) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/pix2pix_cyclegan.md) | 
| 293 | CycleGAN | [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss](https://paperswithcode.com/paper/unpaired-image-to-image-translation-using) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/pix2pix_cyclegan.md) | 
| 294 | PSGAN | [PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer](https://paperswithcode.com/paper/psgan-pose-robust-spatial-aware-gan-for) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/psgan.md) | 
| 295 | Wav2Lip | [A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild](https://paperswithcode.com/paper/a-lip-sync-expert-is-all-you-need-for-speech) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/wav2lip.md) | 
| 296 | LESRCNN | [Lightweight image super-resolution with enhanced CNN](https://paperswithcode.com/paper/lightweight-image-super-resolution-with-2) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/single_image_super_resolution.md) | 
| 297 | ESRGAN | [Esrgan: Enhanced super-resolution generative adversarial networks](https://paperswithcode.com/paper/esrgan-enhanced-super-resolution-generative) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/single_image_super_resolution.md) | 
| 298 | RealSR | [Real-World Super-Resolution via Kernel Estimation and Noise Injection](https://paperswithcode.com/paper/real-world-super-resolution-via-kernel) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/single_image_super_resolution.md) | 
| 299 | StyleGAN2 | [Analyzing and Improving the Image Quality of StyleGAN](https://paperswithcode.com/paper/analyzing-and-improving-the-image-quality-of) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/styleganv2.md) | 
| 300 | U-GAT-IT | [U-GAT-IT: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation](https://paperswithcode.com/paper/u-gat-it-unsupervised-generative-attentional) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/ugatit.md) | 
| 301 | AnimeGAN2 | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 302 | Photo2Cartoon | [U-GAT-IT: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation](https://paperswithcode.com/paper/u-gat-it-unsupervised-generative-attentional) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/photo2cartoon.md) | 
| 303 | DRN | [Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution](https://paperswithcode.com/paper/closed-loop-matters-dual-regression-networks) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/single_image_super_resolution.md) | 
| 304 | starGAN2 | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 305 | FOM | [First Order Motion Model for Image Animation](https://paperswithcode.com/paper/first-order-motion-model-for-image-animation-1) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/motion_driving.md) | 
| 306 | EDVR | [EDVR: Video Restoration with Enhanced Deformable Convolutional Networks](https://paperswithcode.com/paper/edvr-video-restoration-with-enhanced) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/video_super_resolution.md) | 
| 307 | BasicVSR | [BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond](https://paperswithcode.com/paper/basicvsr-the-search-for-essential-components) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/video_super_resolution.md) | 
| 308 | LapStyle | [Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer](https://paperswithcode.com/paper/drafting-and-revision-laplacian-pyramid) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/zh_CN/tutorials/lap_style.md) | 
| 309 | DCGAN | [Deep Convolutional GAN](https://paperswithcode.com/method/dcgan) | <details><summary>Abstract</summary><div></div></details> | [快速开始](暂无) | 
| 310 | CGAN | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 311 | DSSM | [Learning Deep Structured Semantic Models for Web Search using Clickthrough Data](https://paperswithcode.com/paper/learning-deep-structured-semantic-models-for) | <details><summary>Abstract</summary><div>Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 312 | Match-Pyramid | [Text Matching as Image Recognition](https://paperswithcode.com/paper/text-matching-as-image-recognition) | <details><summary>Abstract</summary><div>Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 313 | MultiView-Simnet | [A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems](https://paperswithcode.com/paper/a-multi-view-deep-learning-approach-for-cross) | <details><summary>Abstract</summary><div>Recent online services rely heavily on automatic personalization to recommend relevant content to a large number of users. This requires systems to scale promptly to accommodate the stream of new users visiting the online services for the first time. In this work, we propose a content-based recommendation system to address both the recommendation quality and the system scalability. We propose to use a rich feature set to represent users, according to their web browsing history and search queries. We use a Deep Learning approach to map users and items to a latent space where the similarity between users and their preferred items is maximized. We extend the model to jointly learn from features of items from different domains and user features by introducing a multi-view Deep Learning model. We show how to make this rich-feature based user representation scalable by reducing the dimension of the inputs and the amount of training data. The rich user feature representation allows the model to learn relevant user behavior patterns and give useful recommendations for users who do not have any interaction with the service, given that they have adequate search and browsing history. The combination of different domains into a single model for learning helps improve the recommendation quality across all the domains, as well as having a more compact and a semantically richer user latent feature vector. We experiment with our approach on three real-world recommendation systems acquired from different sources of Microsoft products: Windows Apps recommendation, News recommendation, and Movie/TV recommendation. Results indicate that our approach is significantly better than the state-of-the-art algorithms (up to 49% enhancement on existing users and 115% enhancement on new users). In addition, experiments on a publicly open data set also indicate the superiority of our method in comparison with transitional generative topic models, for modeling cross-domain recommender systems. Scalability analysis show that our multi-view DNN model can easily scale to encompass millions of users and billions of item entries. Experimental results also confirm that combining features from all domains produces much better performance than building separate models for each domain.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 314 | DeepWalk | [DeepWalk: Online Learning of Social Representations](https://paperswithcode.com/paper/deepwalk-online-learning-of-social) | <details><summary>Abstract</summary><div>We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk’s latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk’s representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk’s representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 315 | Mind | [Multi-Interest Network with Dynamic Routing for Recommendation at Tmall](https://paperswithcode.com/paper/multi-interest-network-with-dynamic-routing) | <details><summary>Abstract</summary><div>Industrial recommender systems usually consist of the matching stage and the ranking stage, in order to handle the billion-scale of users and items. The matching stage retrieves candidate items relevant to user interests, while the ranking stage sorts candidate items by user interests. Thus, the most critical ability is to model and represent user interests for either stage. Most of the existing deep learning-based models represent one user as a single vector which is insufficient to capture the varying nature of user’s interests. In this paper, we approach this problem from a different view, to represent one user with multiple vectors encoding the different aspects of the user’s interests. We propose the Multi-Interest Network with Dynamic routing (MIND) for dealing with user’s diverse interests in the matching stage. Specifically, we design a multi-interest extractor layer based on capsule routing mechanism, which is applicable for clustering historical behaviors and extracting diverse interests. Furthermore, we develop a technique named label-aware attention to help learn a user representation with multiple vectors. Through extensive experiments on several public benchmarks and one largescale industrial dataset from Tmall, we demonstrate that MIND can achieve superior performance than state-of-the-art methods for recommendation. Currently, MIND has been deployed for handling major online traffic at the homepage on Mobile Tmall App.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 316 | NCF | [Neural Collaborative Filtering](https://paperswithcode.com/paper/neural-collaborative-filtering) | <details><summary>Abstract</summary><div>In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation — collaborative filtering — on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering — the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural networkbased Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user–item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 317 | Word2vec | [Distributed Representations of Words and Phrases and their Compositionality](https://paperswithcode.com/paper/distributed-representations-of-words-and-1) | <details><summary>Abstract</summary><div>The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 318 | Fasttext | [Bag of Tricks for Efficient Text Classification](https://paperswithcode.com/paper/bag-of-tricks-for-efficient-text) | <details><summary>Abstract</summary><div>This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 319 | GraphNeuralNetwork | [Session-based Recommendation with Graph Neural Networks](https://paperswithcode.com/paper/session-based-recommendation-with-graph) | <details><summary>Abstract</summary><div>The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 320 | GRU4Rec | [Session-based Recommendations with Recurrent Neural Networks](https://paperswithcode.com/paper/session-based-recommendations-with-recurrent) | <details><summary>Abstract</summary><div>We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 321 | RALM | [Real-time Attention Based Look-alike Model for Recommender System](https://paperswithcode.com/paper/real-time-attention-based-look-alike-model) | <details><summary>Abstract</summary><div>Recently, deep learning models play more and more important roles in contents recommender systems. However, although the performance of recommendations is greatly improved, the "Matthew effect" becomes increasingly evident. While the head contents get more and more popular, many competitive long-tail contents are difficult to achieve timely exposure because of lacking behavior features. This issue has badly impacted the quality and diversity of recommendations. To solve this problem, look-alike algorithm is a good choice to extend audience for high quality long-tail contents. But the traditional look-alike models which widely used in online advertising are not suitable for recommender systems because of the strict requirement of both real-time and effectiveness. This paper introduces a real-time attention based look-alike model (RALM) for recommender systems, which tackles the challenge of conflict between real-time and effectiveness. RALM realizes real-time lookalike audience extension benefiting from seeds-to-user similarity prediction and improves the effectiveness through optimizing user representation learning and look-alike learning modeling. For user representation learning, we propose a novel neural network structure named attention merge layer to replace the concatenation layer, which significantly improves the expressive ability of multifields feature learning. On the other hand, considering the various members of seeds, we design global attention unit and local attention unit to learn robust and adaptive seeds representation with respect to a certain target user. At last, we introduce seeds clustering mechanism which not only reduces the time complexity of attention units prediction but also minimizes the loss of seeds information at the same time. According to our experiments, RALM shows superior effectiveness and performance than popular lookalike models. RALM has been successfully deployed in "Top Stories" Recommender System of WeChat, leading to great improvement on diversity and quality of recommendations. As far as we know this is the first real-time look-alike model applied in recommender systems</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 322 | SSR | [Multtti-Rate Deep Learning for Temporal Recommendation]() | <details><summary>Abstract</summary><div>Modeling temporal behavior in recommendation systems is an important and challenging problem. Its challenges come from the fact that temporal modeling increases the cost of parameter estimation and inference, while requiring large amount of data to reliably learn the model with the additional time dimensions. Therefore, it is often difficult to model temporal behavior in large-scale real-world recommendation systems. In this work, we propose a novel deep neural network based architecture that models the combination of long-term static and short-term temporal user preferences to improve the recommendation performance. To train the model efficiently for large-scale applications, we propose a novel pre-train method to reduce the number of free parameters significantly. The resulted model is applied to a real-world data set from a commercial News recommendation system. We compare to a set of established baselines and the experimental results show that our method outperforms the state-of-the-art significantly.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 323 | Youtube_dnn | [Deep Neural Networks for YouTube Recommendations](https://paperswithcode.com/paper/deep-neural-networks-for-youtube) | <details><summary>Abstract</summary><div>YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 324 | BST | [Behavior Sequence Transformer for E-commerce Recommendation in Alibaba](https://paperswithcode.com/paper/behavior-sequence-transformer-for-e-commerce) | <details><summary>Abstract</summary><div>Deep learning based methods have been widely used in industrial recommendation systems (RSs). Previous works adopt an Embedding&MLP paradigm: raw features are embedded into lowdimensional vectors, which are then fed on to MLP for final recommendations. However, most of these works just concatenate different features, ignoring the sequential nature of users’ behaviors. In this paper, we propose to use the powerful Transformer model to capture the sequential signals underlying users’ behavior sequences for recommendation in Alibaba. Experimental results demonstrate the superiority of the proposed model, which is then deployed online at Taobao and obtain significant improvements in online Click-Through-Rate (CTR) comparing to two baselines.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 325 | DCN | [Deep & Cross Network for Ad Click Predictions](https://paperswithcode.com/paper/deep-cross-network-for-ad-click-predictions) | <details><summary>Abstract</summary><div>Feature engineering has been the key to the success of many prediction models. However, the process is nontrivial and oen requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily ecient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benets of a DNN model, and beyond that, it introduces a novel cross network that is more ecient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classication dataset, in terms of both model accuracy and memory usage.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 326 | DeepFM | [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://paperswithcode.com/paper/deepfm-a-factorization-machine-based-neural) | <details><summary>Abstract</summary><div>Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and highorder feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its “wide” and “deep” parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 327 | DMR | [Deep Match to Rank Model for Personalized Click-Through Rate Prediction](https://paperswithcode.com/paper/deep-match-to-rank-model-for-personalized) | <details><summary>Abstract</summary><div>Deep Match to Rank Model for Personalized Click-Through Rate Prediction</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 328 | DNN | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 329 | FFM | [Field-aware Factorization Machines for CTR Prediction](https://paperswithcode.com/paper/field-aware-factorization-machines-for-ctr) | <details><summary>Abstract</summary><div>Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, field-aware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for classifying large sparse data including those from CTR prediction. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 330 | FM | [Factorization machines]() | <details><summary>Abstract</summary><div>In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models. Index Terms—factorization machine; sparse data; tensor factorization; support vector machine</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 331 | GateDNN | [GateNet: Gating-Enhanced Deep Network for Click-Through Rate Prediction](https://paperswithcode.com/paper/gatenet-gating-enhanced-deep-network-for) | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 332 | Logistic_regression | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 333 | Naml | [Neural News Recommendation with Attentive Multi-View Learning](https://paperswithcode.com/paper/neural-news-recommendation-with-attentive) | <details><summary>Abstract</summary><div>Neural News Recommendation with Attentive Multi-View Learning</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 334 | Wide&Deep | [Wide & Deep Learning for Recommender Systems](https://paperswithcode.com/paper/wide-deep-learning-for-recommender-systems) | <details><summary>Abstract</summary><div>Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning—jointly trained wide linear models and deep neural networks—to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 335 | XDeepFM | [xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems](https://paperswithcode.com/paper/xdeepfm-combining-explicit-and-implicit) | <details><summary>Abstract</summary><div>Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at https://github.com/Leavingseason/xDeepFM.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 336 | AutoInt | [AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks](https://paperswithcode.com/paper/autoint-automatic-feature-interaction) | <details><summary>Abstract</summary><div>Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on highorder combinatorial features (a.k.a. cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding lowdimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the AutoInt to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multihead self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the lowdimensional space. With different layers of the multi-head selfattentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: https://github.com/DeepGraphLearning/RecommenderSystems.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 337 | AFM | [Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks](https://paperswithcode.com/paper/attentional-factorization-machines-learning) | <details><summary>Abstract</summary><div>Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep [Cheng et al., 2016] and DeepCross[Shan et al., 2016] with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: https://github. com/hexiangnan/attentional factorization machine</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 338 | DeepCross | [Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features]() | <details><summary>Abstract</summary><div>Manually crafted combinatorial features have been the “secret sauce” behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units. Deep Crossing is implemented with a modeling tool called the Computational Network Tool Kit (CNTK), powered by a multi-GPU platform. It was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models. This demonstrates the potential of using Deep Crossing as a general modeling paradigm to improve existing products, as well as to speed up the development of new models with a fraction of the investment in feature engineering and acquisition of deep domain knowledge.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 339 | DIEN | [Deep Interest Evolution Network for Click-Through Rate Prediction](https://paperswithcode.com/paper/deep-interest-evolution-network-for-click) | <details><summary>Abstract</summary><div>Click-through rate (CTR) prediction, whose goal is to estimate the probability of a user clicking on the item, has become one of the core tasks in the advertising system. For CTR prediction model, it is necessary to capture the latent user interest behind the user behavior data. Besides, considering the changing of the external environment and the internal cognition, user interest evolves over time dynamically. There are several CTR prediction methods for interest modeling, while most of them regard the representation of behavior as the interest directly, and lack specially modeling for latent interest behind the concrete behavior. Moreover, little work considers the changing trend of the interest. In this paper, we propose a novel model, named Deep Interest Evolution Network (DIEN), for CTR prediction. Specifically, we design interest extractor layer to capture temporal interests from history behavior sequence. At this layer, we introduce an auxiliary loss to supervise interest extracting at each step. As user interests are diverse, especially in the e-commerce system, we propose interest evolving layer to capture interest evolving process that is relative to the target item. At interest evolving layer, attention mechanism is embedded into the sequential structure novelly, and the effects of relative interests are strengthened during interest evolution. In the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. Notably, DIEN has been deployed in the display advertisement system of Taobao, and obtained 20.7% improvement on CTR.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 340 | DIN | [Deep Interest Network for Click-Through Rate Prediction](https://paperswithcode.com/paper/deep-interest-network-for-click-through-rate) | <details><summary>Abstract</summary><div>Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 341 | FGCNN | [Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction](https://paperswithcode.com/paper/feature-generation-by-convolutional-neural) | <details><summary>Abstract</summary><div>Click-Through Rate prediction is an important task in recommender systems, which aims to estimate the probability of a user to click on a given item. Recently, many deep models have been proposed to learn low-order and high-order feature interactions from original features. However, since useful interactions are always sparse, it is difficult for DNN to learn them effectively under a large number of parameters. In real scenarios, artificial features are able to improve the performance of deep models (such as Wide & Deep Learning), but feature engineering is expensive and requires domain knowledge, making it impractical in different scenarios. Therefore, it is necessary to augment feature space automatically.In this paper, We propose a novel Feature Generation by Convolutional Neural Network (FGCNN) model with two components: Feature Generation and Deep Classifier. Feature Generation leverages the strength of CNN to generate local patterns and recombine them to generate new features. Deep Classifier adopts the structure of IPNN to learn interactions from the augmented feature space. Experimental results on three large-scale datasets show that FGCNN significantly outperforms nine state-of-the-art models. Moreover, when applying some state-of-the-art models as Deep Classifier, better performance is always achieved, showing the great compatibility of our FGCNN model. This work explores a novel direction for CTR predictions: it is quite useful to reduce the learning difficulties of DNN by automatically identifying important features.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 342 | Fibinet | [FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction](https://paperswithcode.com/paper/fibinet-combining-feature-importance-and) | <details><summary>Abstract</summary><div>Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two realworld datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM)</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 343 | FLEN | [FLEN: Leveraging Field for Scalable CTR Prediction](https://paperswithcode.com/paper/flen-leveraging-field-for-scalable-ctr) | <details><summary>Abstract</summary><div>Click-Through Rate (CTR) prediction systems are usually based on multi-field categorical features, i.e., every feature is categorical and belongs to one and only one field. Modeling feature conjunctions is crucial for CTR prediction accuracy. However, it usually requires a massive number of parameters to explicitly model all feature conjunctions, which is not scalable for real-world production systems. In this paper, we describe a novel Field-Leveraged Embedding Network (FLEN) which has been deployed in the commercial recommender systems in Meitu and serves the main traffic. FLEN devises a field-wise bi-interaction pooling technique. By suitably exploiting field information, the field-wise bi-interaction pooling layer captures both inter-field and intra-field feature conjunctions with a small number of model parameters and an acceptable time complexity for industrial applications. We show that some classic shallow CTR models can be regarded as special cases of this technique, i.e., MF, FM and FwFM. We identify a unique challenge in this technique, i.e., the FM module in our model may suffer from the coupled gradient issue, which will damage the performance of the model. To solve this challenge, we develop Dicefactor: a novel dropout method to prevent independent latent features from co-adapting. Extensive experiments, including offline evaluations and online A/B testing on real production systems, demonstrate the effectiveness and efficiency of FLEN against the state-of-the-art models. In particular, compared to the previous version deployed on the system (i.e. NFM), FLEN has obtained 5.19% improvement on CTR with 1/6 of memory usage and computation time.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 344 | FNN | [Deep Learning over Multi-field Categorical Data](https://paperswithcode.com/paper/deep-learning-over-multi-field-categorical) | <details><summary>Abstract</summary><div>Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users’ ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 345 | NFM | [Neural Factorization Machines for Sparse Predictive Analytics](https://paperswithcode.com/paper/neural-factorization-machines-for-sparse) | <details><summary>Abstract</summary><div>Many predictive tasks of web applications need to model categorical variables, such as user IDs and demographics like genders and occupations. To apply standard machine learning techniques, these categorical predictors are always converted to a set of binary features via one-hot encoding, making the resultant feature vector highly sparse. To learn from such sparse data effectively, it is crucial to account for the interactions between features.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 346 | PNN | [Product-based Neural Networks for User Response Prediction](https://paperswithcode.com/paper/product-based-neural-networks-for-user) | <details><summary>Abstract</summary><div>Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfield categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 347 | ESMM | [Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate](https://paperswithcode.com/paper/entire-space-multi-task-model-an-effective) | <details><summary>Abstract</summary><div>Estimating post-click conversion rate (CVR) accurately is crucial for ranking systems in industrial applications such as recommendation and advertising. Conventional CVR modeling applies popular deep learning methods and achieves state-of-the-art performance. However it encounters several task-specific problems in practice, making CVR modeling challenging. For example, conventional CVR models are trained with samples of clicked impressions while utilized to make inference on the entire space with samples of all impressions. This causes a sample selection bias problem. Besides, there exists an extreme data sparsity problem, making the model fitting rather difficult. In this paper, we model CVR in a brand-new perspective by making good use of sequential pattern of user actions, i.e., impression -> click -> conversion. The proposed Entire Space Multi-task Model (ESMM) can eliminate the two problems simultaneously by i) modeling CVR directly over the entire space, ii) employing a feature representation transfer learning strategy. Experiments on dataset gathered from Taobao's recommender system demonstrate that ESMM significantly outperforms competitive methods. We also release a sampling version of this dataset to enable future research. To the best of our knowledge, this is the first public dataset which contains samples with sequential dependence of click and conversion labels for CVR modeling.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 348 | MMOE | [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://paperswithcode.com/paper/modeling-task-relationships-in-multi-task) | <details><summary>Abstract</summary><div>Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 349 | PLE | [Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations](https://paperswithcode.com/paper/progressive-layered-extraction-ple-a-novel) | <details><summary>Abstract</summary><div>Multi-task learning (MTL) has been successfully applied to many recommendation applications. However, MTL models often suffer from performance degeneration with negative transfer due to the complex and competing task correlation in real-world recommender systems. Moreover, through extensive experiments across SOTA MTL models, we have observed an interesting seesaw phenomenon that performance of one task is often improved by hurting the performance of some other tasks. To address these issues, we propose a Progressive Layered Extraction (PLE) model with a novel sharing structure design. PLE separates shared components and task-specific components explicitly and adopts a progressive routing mechanism to extract and separate deeper semantic knowledge gradually, improving efficiency of joint representation learning and information routing across tasks in a general setup. We apply PLE to both complicatedly correlated and normally correlated tasks, ranging from two-task cases to multi-task cases on a real-world Tencent video recommendation dataset with 1 billion samples, and results show that PLE outperforms state-of-the-art MTL models significantly under different task correlations and task-group size. Furthermore, online evaluation of PLE on a large-scale content recommendation platform at Tencent manifests 2.23% increase in view-count and 1.84% increase in watch time compared to SOTA MTL models, which is a significant improvement and demonstrates the effectiveness of PLE. Finally, extensive offline experiments on public benchmark datasets demonstrate that PLE can be applied to a variety of scenarios besides recommendations to eliminate the seesaw phenomenon. PLE now has been deployed to the online video recommender system in Tencent successfully.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 350 | ShareBottom | [Multitask learning](https://paperswithcode.com/paper/multitask-learning-a-knowledge-based-source) | <details><summary>Abstract</summary><div>Multitask Learning is an approach to inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. In this thesis we demonstrate multitask learning for a dozen problems. We explain how multitask learning works and show that there are many opportunities for multitask learning in real domains. We show that in some cases features that would normally be used as inputs work better if used as multitask outputs instead. We present suggestions for how to get the most out of multitask learning in articial neural nets, present an algorithm for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Multitask learning improves generalization performance, can be applied in many dierent kinds of domains, and can be used with dierent learning algorithms. We conjecture there will be many opportunities for its use on real-world problems.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 351 | Maml | [Model-agnostic meta-learning for fast adaptation of deep networks](https://paperswithcode.com/paper/model-agnostic-meta-learning-for-fast) | <details><summary>Abstract</summary><div>We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 352 | Listwise | [Sequential Evaluation and Generation Framework for Combinatorial Recommender System]() | <details><summary>Abstract</summary><div>to the user at one time in the result page, where the correlations among the items have impact on the user behavior. In this work, we model the combinatorial recommendation as the problem of generating a sequence(ordered list) of items from a candidate set, with the target of maximizing the expected overall utility(e.g. total clicks) of the sequence. Toward solving this problem, we propose the Evaluation-Generation framework. On the one hand of this framework, an evaluation model is trained to evaluate the expected overall utility, by fully considering the user, item information and the correlations among the co-exposed items. On the other hand, generation policies based on heuristic searching or reinforcement learning are devised to generate potential high-quality sequences, from which the evaluation model select one to expose. We propose effective model architectures and learning metrics under this framework. We also offer series of offline tests to thoroughly investigate the performance of the proposed framework, as supplements to the online experiments. Our results show obvious increase in performance compared with the previous solutions.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 353 | TDM | [Learning Tree-based Deep Model for Recommender Systems]() | <details><summary>Abstract</summary><div>Model-based methods for recommender systems have been studied extensively in recent years. In systems with large corpus, however, the calculation cost for the learnt model to predict all useritem preferences is tremendous, which makes full corpus retrieval extremely difficult. To overcome the calculation barriers, models such as matrix factorization resort to inner product form (i.e., model user-item preference as the inner product of user, item latent factors) and indexes to facilitate efficient approximate k-nearest neighbor searches. However, it still remains challenging to incorporate more expressive interaction forms between user and item features, e.g., interactions through deep neural networks, because of the calculation cost. In this paper, we focus on the problem of introducing arbitrary advanced models to recommender systems with large corpus. We propose a novel tree-based method which can provide logarithmic complexity w.r.t. corpus size even with more expressive models such as deep neural networks. Our main idea is to predict user interests from coarse to fine by traversing tree nodes in a top-down fashion and making decisions for each user-node pair. We also show that the tree structure can be jointly learnt towards better compatibility with users’ interest distribution and hence facilitate both training and prediction. Experimental evaluations with two large-scale real-world datasets show that the proposed method significantly outperforms traditional methods. Online A/B test results in Taobao display advertising platform also demonstrate the effectiveness of the proposed method in production environments.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 354 | Tagspace | [TagSpace: Semantic Embeddings from Hashtags](https://paperswithcode.com/paper/tagspace-semantic-embeddings-from-hashtags) | <details><summary>Abstract</summary><div>We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal. The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags. As well as strong performance on the hashtag prediction task itself, we show that its learned representation of text (ignoring the hashtag labels) is useful for other tasks as well. To that end, we present results on a document recommendation task, where it also outperforms a number of baselines.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 355 | Textcnn | [Convolutional neural networks for sentence classication]() | <details><summary>Abstract</summary><div>We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleRec/blob/master/README.md) | 
| 356 | conformer offline/on</br>line | [Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition](https://paperswithcode.com/paper/unified-streaming-and-non-streaming-two-pass) | <details><summary>Abstract</summary><div>In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42% CER with 640ms latency in a streaming ASR system</div></details> | [快速开始]() | 
| 357 | transformer offline/</br>online | [Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition](https://paperswithcode.com/paper/unified-streaming-and-non-streaming-two-pass) | <details><summary>Abstract</summary><div>In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42% CER with 640ms latency in a streaming ASR system</div></details> | [快速开始]() | 
| 358 | deepspeech2 offline/</br>online | [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://paperswithcode.com/paper/deep-speech-2-end-to-end-speech-recognition) | <details><summary>Abstract</summary><div>We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.</div></details> | [快速开始]() | 
| 359 | fastspeech2/fastpitc</br>h | [FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://paperswithcode.com/paper/fastspeech-2-fast-and-high-quality-end-to-end) | <details><summary>Abstract</summary><div>Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al.,2019) can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more informationas input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speechvariations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated andtime-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit thevoice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTSby 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech(e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directlytake them as conditional inputs in training and use predicted values in inference.We further design FastSpeech 2s, which is the first attempt to directly generatespeech waveform from text in parallel, enjoying the benefit of fully end-to-endinference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inferencespeed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available athttps://speechresearch.github.io/fastspeech2/.</div></details> | [快速开始]() | 
| 360 | speedyspeech | [SpeedySpeech: Efficient Neural Speech Synthesis](https://paperswithcode.com/paper/speedyspeech-efficient-neural-speech) | <details><summary>Abstract</summary><div>While recent neural sequence-to-sequence models have greatly improved the quality of speech synthesis, there has not been a system capable of fast training, fast inference and high-quality audio synthesis at the same time. We propose a student-teacher network capable of high-quality faster-than-real-time spectrogram synthesis, with low requirements on computational resources and fast training time. We show that self-attention layers are not necessary for generation of high quality audio. We utilize simple convolutional blocks with residual connections in both student and teacher networks and use only a single attention layer in the teacher model. Coupled with a MelGAN vocoder, our model's voice quality was rated significantly higher than Tacotron 2. Our model can be efficiently trained on a single GPU and can run in real time even on a CPU. We provide both our source code and audio samples in our GitHub repository. </div></details> | [快速开始]() | 
| 361 | transformer_tts | [Neural Speech Synthesis with Transformer Network](https://paperswithcode.com/paper/neural-speech-synthesis-with-transformer) | <details><summary>Abstract</summary><div>Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-the- art performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we intro- duce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mecha- nism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves training efficiency. Meanwhile, any two inputs at different times are connected directly by a self-attention mechanism, which solves the long range de- pendency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spec- trograms, followed by a WaveNet vocoder to output the fi- nal audio results. Experiments are conducted to test the ef- ficiency and performance of our new network. For the effi- ciency, our Transformer TTS network can speed up the train- ing about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our pro- posed model achieves state-of-the-art performance (outper- forms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).</div></details> | [快速开始]() | 
| 362 | PP-Waveflow | [WaveFlow: A Compact Flow-based Model for Raw Audio](https://paperswithcode.com/paper/waveflow-a-compact-flow-based-model-for-raw-1) | <details><summary>Abstract</summary><div>In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood. It handles the long-range structure of 1-D waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions. WaveFlow provides a unified view of likelihood-based models for 1-D data, including WaveNet and WaveGlow as special cases. It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms with hundreds of thousands of time-steps. Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis. Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15× smaller than WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6× faster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered inference kernels. </div></details> | [快速开始]() | 
| 363 | Parallel WaveGAN | [PARALLEL WAVEGAN: A FAST WAVEFORM GENERATION MODEL BASED ON GENERATIVE ADVERSARIAL NETWORKS WITH MULTI-RESOLUTION SPECTROGRAM](https://paperswithcode.com/paper/parallel-wavegan-a-fast-waveform-generation) | <details><summary>Abstract</summary><div>We propose Parallel WaveGAN, a distillation-free, fast, and small- footprint waveform generation method using a generative adver- sarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectro- gram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained. Furthermore, our model is able to generate high- fidelity speech even with its compact architecture. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real- time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet sys- tem.</div></details> | [快速开始]() | 
| 364 | GE2E | [Generalized End-to-End Loss for Speaker Verification](https://paperswithcode.com/paper/generalized-end-to-end-loss-for-speaker) | <details><summary>Abstract</summary><div>In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e. "OK Google" and "Hey Google") as well as multiple dialects. </div></details> | [快速开始]() | 
| 365 | VoiceCloning | [Transfer Learning from Speaker Verification toMultispeaker Text-To-Speech Synthesis](https://paperswithcode.com/paper/transfer-learning-from-speaker-verification) | <details><summary>Abstract</summary><div>We describe a neural network-based system for text-to-speech (TTS) synthesis thatis able to generate speech audio in the voice of different speakers, including thoseunseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using anindependent dataset of noisy speech without transcripts from thousands of speakers,to generate a fixed-dimensional embedding vector from only seconds of referencespeech from a target speaker; (2) a sequence-to-sequence synthesis network basedon Tacotron 2 that generates a mel spectrogram from text, conditioned on thespeaker embedding; (3) an auto-regressive WaveNet-based vocoder network thatconverts the mel spectrogram into time domain waveform samples. We demonstratethat the proposed model is able to transfer the knowledge of speaker variabilitylearned by the discriminatively-trained speaker encoder to the multispeaker TTStask, and is able to synthesize natural speech from speakers unseen during training.We quantify the importance of training the speaker encoder on a large and diversespeaker set in order to obtain the best generalization performance. Finally, we showthat randomly sampled speaker embeddings can be used to synthesize speech inthe voice of novel speakers dissimilar from those used in training, indicating thatthe model has learned a high quality speaker representation.</div></details> | [快速开始]() | 
| 366 | tacotron2 | [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://paperswithcode.com/paper/neural-speech-synthesis-with-transformer) | <details><summary>Abstract</summary><div>This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.</div></details> | [快速开始]() | 
| 367 | Attention LSTM | [Beyond Short Snippets: Deep Networks for Video Classification](https://paperswithcode.com/paper/beyond-short-snippets-deep-networks-for-video) | <details><summary>Abstract</summary><div>Convolutional neural networks (CNNs) have been exten- sively applied for image recognition problems giving state- of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image infor- mation across a video over longer time periods than previ- ously attempted. We propose two methods capable of han- dling full length videos. The first method explores various convolutional temporal feature pooling architectures, ex- amining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improve- ments over previously published results on the Sports 1 mil- lion dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 73.0%). </div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/attention_lstm.md) | 
| 368 | TSM | [TSM: Temporal Shift Module for Efficient Video Understanding](https://paperswithcode.com/paper/temporal-shift-module-for-efficient-video) | <details><summary>Abstract</summary><div>Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/tsm.md) | 
| 369 | PP-TSM | [TSM: Temporal Shift Module for Efficient Video Understanding](https://paperswithcode.com/paper/temporal-shift-module-for-efficient-video) | <details><summary>Abstract</summary><div>Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/pp-tsm.md) | 
| 370 | TSN | [Temporal Segment Networks for Action Recognition in Video](https://paperswithcode.com/paper/temporal-segment-networks-for-action) | <details><summary>Abstract</summary><div>Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/tsn.md) | 
| 371 | PP-TSN | [Temporal Segment Networks for Action Recognition in Video](https://paperswithcode.com/paper/temporal-segment-networks-for-action) | <details><summary>Abstract</summary><div>Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/pp-tsn.md) | 
| 372 | SlowFast | [SlowFast Networks for Video Recognition](https://paperswithcode.com/paper/slowfast-networks-for-video-recognition) | <details><summary>Abstract</summary><div>We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/slowfast.md) | 
| 373 | TimeSformer | [Is Space-Time Attention All You Need for Video Understanding?](https://paperswithcode.com/paper/is-space-time-attention-all-you-need-for) | <details><summary>Abstract</summary><div>We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/timesformer.md) | 
| 374 | ST-GCN | [Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition](https://paperswithcode.com/paper/spatial-temporal-graph-convolutional-networks-1) | <details><summary>Abstract</summary><div>Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/stgcn.md) | 
| 375 | AGCN | [Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks](https://paperswithcode.com/paper/skeleton-based-action-recognition-with-multi) | <details><summary>Abstract</summary><div>Graph convolutional networks (GCNs), which generalize CNNs to more generic non-Euclidean structures, have achieved remarkable performance for skeleton-based action recognition. However, there still exist several issues in the previous GCN-based models. First, the topology of the graph is set heuristically and fixed over all the model layers and input data. This may not be suitable for the hierarchy of the GCN model and the diversity of the data in action recognition tasks. Second, the second-order information of the skeleton data, i.e., the length and orientation of the bones, is rarely investigated, which is naturally more informative and discriminative for the human action recognition. In this work, we propose a novel multi-stream attention-enhanced adaptive graph convolutional neural network (MS-AAGCN) for skeleton-based action recognition. The graph topology in our model can be either uniformly or individually learned based on the input data in an end-to-end manner. This data-driven approach increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Besides, the proposed adaptive graph convolutional layer is further enhanced by a spatial-temporal-channel attention module, which helps the model pay more attention to important joints, frames and features. Moreover, the information of both the joints and bones, together with their motion information, are simultaneously modeled in a multi-stream framework, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/agcn.md) | 
| 376 | BMN | [BMN: Boundary-Matching Network for Temporal Action Proposal Generation](https://paperswithcode.com/paper/bmn-boundary-matching-network-for-temporal) | <details><summary>Abstract</summary><div>Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/localization/bmn.md) | 
| 377 | IGSQL | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 378 | RAT-SQL | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 379 | BiGRU-CRF | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 380 | Deep Biaffine Attent</br>ion | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 381 | ERNIE-CSC | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 382 | PLATO-2 | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 383 | Seq2Seq | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 384 | Transformer | [attention is all you need](https://paperswithcode.com/paper/attention-is-all-you-need) | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 385 | STACL | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 386 | SKEP | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 387 | SimNet | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 388 | Sentence-Transformer</br> | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 389 | EFL | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 390 | PET | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 391 | P-Tuning | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 392 | Pointer Generator Ne</br>twork | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 393 | ERNIE | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 394 | ERNIE-DOC | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 395 | ERNIE-GEN | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 396 | ERNIE-GRAM | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 397 | RoFormer | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 398 | BART | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 399 | ALBERT | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 400 | BERT | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 401 | BigBird | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 402 | DistilBert | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 403 | ELECTRA | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 404 | GPT | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 405 | NeZha | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 406 | RoBERTa | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 407 | MiniLMv2 | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 408 | TinyBert | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 409 | XLNet | [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://paperswithcode.com/paper/xlnet-generalized-autoregressive-pretraining) | <details><summary>Abstract</summary><div>With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining ap- proaches based on autoregressive language modeling. However, relying on corrupt- ing the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.</div></details> | [快速开始](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/language_model/xlnet/README.md) | 
| 410 | GaAN | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 411 | stgcn | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 412 | graphsage | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 413 | metapath2vec | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 414 | SAGPool | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 415 | line | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 416 | pgl-ke | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 417 | xformer | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 418 | erniesage | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 419 | dgi | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 420 | sgc | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 421 | gcn | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 422 | gin | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 423 | strucvec | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 424 | node2vec | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 425 | GATNE | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 426 | deeper_gcn | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 427 | ges | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 428 | gat | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 429 | deepwalk | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 430 | MAG240M | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 431 | PCQM4M | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 432 | WikiKG90M | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 433 | lightgcn | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 434 | ngcf | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 435 | rgcn | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 436 | ssgc | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 437 | Prioritized_DQN | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 438 | PPO | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 439 | GA3C | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 440 | SAC | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 441 | IMPALA | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 442 | DDPG | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 443 | PolicyGradient | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 444 | NeurIPS2019-Learn-to</br>-Move-Challenge | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 445 | TD3 | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 446 | DQN | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 447 | ES | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 448 | DQN_variant | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 449 | A2C | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 450 | NeurIPS2018-AI-for-P</br>rosthetics-Challenge | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 451 | MADDPG | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 452 | AlphaZero | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 453 | CARLA_SAC | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 454 | NeurIPS2020 L2RPN Ch</br>allenge | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 455 | OAC | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 
| 456 | QMIX | []() | <details><summary>Abstract</summary><div></div></details> | [快速开始]() | 

