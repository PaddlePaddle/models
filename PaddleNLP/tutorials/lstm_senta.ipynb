{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言情感分析\n",
    "\n",
    "人类自然语言中包含了丰富的情感色彩，情感倾向分析（Sentiment Classification）即针对带有主观描述的中文文本，自动判断该文本的情感极性类别并给出相应的置信度。情感倾向分析能够帮助企业理解用户消费习惯、分析热点话题和危机舆情监控，为企业提供有利的决策支持。，利用机器自动分析这些情感倾向，不但有助于帮助企业了解消费者对其产品的感受，为产品改进提供依据；同时还有助于企业分析商业伙伴们的态度，以便更好地进行商业决策。\n",
    "\n",
    "简单的说，我们可以将情感分析（sentiment classification）任务定义为一个分类问题，即指定一个文本输入，机器通过对文本进行分析、处理、归纳和推理后自动输出结论，如**图1**所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/0a5a60199b0d49daaf6e1dfe616accce7a0c97f8c5ad4ad2b9bf0013d2c3ecab\" width=\"800\" ></center>\n",
    "<br><center>图1：情感分析任务</center></br>\n",
    "\n",
    "通常情况下，人们把情感分析任务看成一个三分类问题，如 **图2** 所示：\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b630901b397e4e7a8e78ab1d306dfa1fc070d91015a64ef0b8d590aaa8cfde14\" width=\"600\" ></center>\n",
    "<br><center>图2：情感分析任务</center></br>\n",
    "\n",
    "- **正向：** 表示正面积极的情感，如高兴，幸福，惊喜，期待等。\n",
    "- **负向：** 表示负面消极的情感，如难过，伤心，愤怒，惊恐等。\n",
    "- **其他：** 其他类型的情感。\n",
    "\n",
    "在情感分析任务中，研究人员除了分析句子的情感类型外，还细化到以句子中具体的“方面”为分析主体进行情感分析（aspect-level），如下：\n",
    "\n",
    "> 这个薯片口味有点咸，太辣了，不过口感很脆。\n",
    "\n",
    "关于薯片的口味方面是一个负向评价（咸，太辣），然而对于口感方面却是一个正向评价（很脆）。\n",
    "\n",
    "> 我很喜欢夏威夷，就是这边的海鲜太贵了。\n",
    "\n",
    "关于夏威夷是一个正向评价（喜欢），然而对于夏威夷的海鲜却是一个负向评价（价格太贵）。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用深度神经网络完成情感分析任务\n",
    "\n",
    "一般通过深度神经网络将句子表示成向量，接入分类器，从而实现句子分类。\n",
    "\n",
    "如何把自然语言句子转换向量呢？\n",
    "\n",
    "有一个非常简单粗暴的解决方式：就是先把一个句子中所有词的embedding进行加和平均，再用得到的平均embedding作为整个句子的向量表示。然而由于自然语言变幻莫测，我们在使用神经网络处理句子的时候，往往会遇到如下两类问题：\n",
    "\n",
    "- **变长的句子：** 自然语言句子往往是变长的，不同的句子长度可能差别很大。然而大部分神经网络接受的输入都是张量，长度是固定的，那么如何让神经网络处理变长数据成为了一大挑战。\n",
    "\n",
    "- **组合的语义：** 自然语言句子往往对结构非常敏感，有时稍微颠倒单词的顺序都可能改变这句话的意思，比如：\n",
    "\n",
    "  >你等一下我做完作业就走。\n",
    "  >\n",
    "  >我等一下你做完工作就走。\n",
    "\n",
    "  >我不爱吃你做的饭。\n",
    "  >\n",
    "  >你不爱吃我做的饭。\n",
    "  \n",
    "  >我瞅你咋地。\n",
    "  >\n",
    "  >你瞅我咋地。\n",
    "  \n",
    "因此，我们需要找到一个可以考虑词和词之间顺序（关系）的神经网络，用于更好地实现自然语言句子建模。\n",
    "\n",
    "循环神经网络(Recurrent Neural Network, RNN)广泛适用于自然语言处理领域(Natural Language Processing, NLP)，RNN有什么显著的特点呢？普通的神经网络，每一层的输出是下一层的输入，每一层之间是相互独立的，没有关系。但是对于语言来说，一句话中的单词顺序不同，整个语义就完全变了。因此自然语言处理往往需要能够更好地处理序列信息的神经网络，RNN 能够满足这个需求。\n",
    "\n",
    "RNN 中，隐藏层的状态，不仅取决于当前输入层的输出，还和上一步隐藏层的状态有关。\n",
    "\n",
    "PaddleNLP将常见[RNN类模型](http://gitlab.baidu.com/PaddleSL/PaddleNLP/tree/master/examples/text_classification/rnn)和经典[预训练模型](http://gitlab.baidu.com/PaddleSL/PaddleNLP/tree/master/examples/text_classification/pretrained_models)封装到了[seq2vec](http://gitlab.baidu.com/PaddleSL/PaddleNLP/tree/master/paddlenlp/seq2vec)，便于大家通过配置参数灵活调用。这里采用了LSTM网络。\n",
    "\n",
    "长短期记忆模型(Long short-term memory, LSTM)是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。\n",
    "\n",
    "\n",
    "关于数据集，这里采用的是PaddleNLP内置数据集之一：情感倾向分类数据集ChnSentiCorp。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 长短时记忆网络LSTM\n",
    "\n",
    "长短时记忆网络（Long Short-Term Memory，LSTM）是一个非常经典的循环神经网络（Recurrent Neural Network，RNN）可以对自然语言句子或是其它时序信号进行建模，RNN网络结构如 **图5** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3f4d413393824d208177a020dcfa68205fa192ebecf04b42a5a17cbe7a146abb\" width=\"600\" ></center>\n",
    "<br><center>图5：循环神经网络结构</center></br>\n",
    "\n",
    "普通RNN网络存在一个明显的缺陷，就是当阅读很长的序列时，网络内部的信息会变得越来越复杂，甚至会超过网络的记忆能力，使得最终的输出信息变得混乱无用。长短时记忆网络（Long Short-Term Memory，LSTM）内部的复杂结构正是为处理这类问题而设计的，其网络结构如 **图6** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d7d112d458ff401ba7fb089cf0521b284c795798dcfb4b88bd2dd6b65854193a\" width=\"600\" ></center>\n",
    "<br><center>图6：LSTM网络结构</center></br>\n",
    "\n",
    "长短时记忆网络的结构和循环神经网络非常类似，都是通过不断调用同一个cell来逐次处理时序信息。每阅读一个新单词$x_t$，就会输出一个新的输出信号$h_t$，用来表示当前阅读到所有内容的整体向量表示。不过二者又有一个明显区别，长短时记忆网络在不同cell之间传递的是两个记忆信息，而不像循环神经网络一样只有一个记忆信息，此外长短时记忆网络的内部结构也更加复杂，如 **图7** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/024b611db4ad4b709a4e1bd403ac41897c73792b895d4557a2dd561d625fcbf1\" width=\"400\" ></center>\n",
    "<br><center>图7：LSTM网络内部结构示意图</center></br>\n",
    "\n",
    "区别于循环神经网络RNN，长短时记忆网络最大的特点是在更新内部记忆时，引入了遗忘机制。即容许网络忘记过去阅读过程中看到的一些无关紧要的信息，只保留有用的历史信息。通过这种方式延长了记忆长度。举个例子：\n",
    "\n",
    "> 我觉得这家餐馆的菜品很不错，烤鸭非常正宗，包子也不错，酱牛肉很有嚼劲。但是服务员态度太恶劣了，我们在门口等了50分钟都没有能成功进去，好不容易进去了，桌子也半天没人打扫。整个环境非常吵闹，我的孩子都被吓哭了，我下次不会带朋友来。\n",
    "\n",
    "当我们阅读上面这段话的时候，可能会记住一些关键词，如烤鸭好吃、牛肉有嚼劲、环境吵等，但也会忽略一些不重要的内容，如“我觉得”、“好不容易”等，长短时记忆网络正是受这个启发而设计的。\n",
    "\n",
    "长短时记忆网络的Cell有三个输入：\n",
    "\n",
    "- 这个网络新看到的输入信号，如下一个单词，记为$x_{t}$， 其中$x_{t}$是一个向量，$t$代表了当前时刻。\n",
    "- 这个网络在上一步的输出信号，记为$h_{t-1}$，这是一个向量，维度同$x_{t}$相同。\n",
    "- 这个网络在上一步的记忆信号，记为$c_{t-1}$，这是一个向量，维度同$x_{t}$相同。\n",
    "\n",
    "得到这两个信号之后，长短时记忆网络没有立即去融合这两个向量，而是计算了权重。\n",
    "\n",
    "- 输入门：$i_{t}=sigmoid(W_{i}X_{t}+V_{i}H_{t-1}+b_i)$，控制有多少输入信号会被融合。\n",
    "- 遗忘门：$f_{t}=sigmoid(W_{f}X_{t}+V_{f}H_{t-1}+b_f)$，控制有多少过去的记忆会被遗忘。\n",
    "- 输出门：$o_{t}=sigmoid(W_{o}X_{t}+V_{o}H_{t-1}+b_o)$，控制最终输出多少记忆。\n",
    "- 单元状态：$g_{t}=tanh(W_{g}X_{t}+V_{g}H_{t-1}+b_g)$，输入信号和过去的输入信号做一个信息融合。\n",
    "\n",
    "通过学习这些门的权重设置，长短时记忆网络可以根据当前的输入信号和记忆信息，有选择性地忽略或者强化当前的记忆或是输入信号，帮助网络更好地学习长句子的语义信息：\n",
    "\n",
    "- 记忆信号：$c_{t} = f_{t} \\cdot c_{t-1} + i_{t} \\cdot g_{t}$\n",
    "\n",
    "- 输出信号：$h_{t} = o_{t} \\cdot tanh(c_{t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用PaddlNLP实现LSTM句子情感分类\n",
    "## 一、安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "paddle.__version__\n",
    "# !pip install paddlenlp\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据加载和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chnsenticorp import ChnSentiCorp\n",
    "from data_process import load_vocab, convert_tokens_to_ids\n",
    "from functools import partial\n",
    "import jieba\n",
    "import numpy as np\n",
    "from dataset import TSVDataset, SimpleDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据集\n",
    "- 使用内置数据集：[PaddleNLP内置数据集](http://gitlab.baidu.com/PaddleSL/PaddleNLP/tree/master/paddlenlp/datasets)\n",
    "- 自定义数据集 <font color=red size=4>待补充自定义数据集的demo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChnSentiCorp数据集在`paddlenlp.datasets`中是可以直接获取的。   \n",
    "\n",
    "通过`paddlenlp.datasets`获取到的数据是否已经经过了文本预处理，即 Tokenizer，向量化文本(将文本转为数字序列)。<font color=red size=4>这里待统一</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1713/1713 [00:00<00:00, 11732.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'chnsenticorp.ChnSentiCorp'>\n",
      "['0', '1']\n",
      "这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般 1\n"
     ]
    }
   ],
   "source": [
    "# 加载paddlenlp.datasets的内置数据集\n",
    "train_dataset = ChnSentiCorp('train')\n",
    "dev_dataset = ChnSentiCorp('dev')\n",
    "test_dataset = ChnSentiCorp('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据格式**    \n",
    "每个例子包含一句评论和对应的标签，0或1。0代表负向评论，1代表正向评论。   \n",
    "看一下前5条数据和对应标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'chnsenticorp.ChnSentiCorp'>\n",
      "['0', '1']\n",
      "这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般 1\n",
      "怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片！开始还怀疑是不是赠送的个别现象，可是后来发现每张DVD后面都有！真不知道生产商怎么想的，我想看的是猫和老鼠，不是米老鼠！如果厂家是想赠送的话，那就全套米老鼠和唐老鸭都赠送，只在每张DVD后面添加一集算什么？？简直是画蛇添足！！ 0\n",
      "还稍微重了点，可能是硬盘大的原故，还要再轻半斤就好了。其他要进一步验证。贴的几种膜气泡较多，用不了多久就要更换了，屏幕膜稍好点，但比没有要强多了。建议配赠几张膜让用用户自己贴。 0\n",
      "交通方便；环境很好；服务态度很好 房间较小 1\n",
      "不错，作者的观点很颠覆目前中国父母的教育方式，其实古人们对于教育已经有了很系统的体系了，可是现在的父母以及祖父母们更多的娇惯纵容孩子，放眼看去自私的孩子是大多数，父母觉得自己的孩子在外面只要不吃亏就是好事，完全把古人几千年总结的教育古训抛在的九霄云外。所以推荐准妈妈们可以在等待宝宝降临的时候，好好学习一下，怎么把孩子教育成一个有爱心、有责任心、宽容、大度的人。 1\n"
     ]
    }
   ],
   "source": [
    "print (type(train_dataset))\n",
    "print (train_dataset.get_labels())\n",
    "\n",
    "for sent, label in test_dataset[:5]:\n",
    "    print (sent, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集\n",
    "# to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理\n",
    "- **对齐句子长度，用于一个batch内批运算**       \n",
    "句子长度统一设为多少？可指定固定长度，或选取数据集中最大句子长度。这里采用每个batch中的最大长度。\n",
    "- **数据多进程异步加载（可选）**      \n",
    "使用`paddle.io.DataLoader`接口\n",
    "\n",
    "<font color=red size=4>？collate_fn代码怎么理解；\n",
    "generate_batch()里遍历batch为啥有长度，各个dataset返回的数据项是否一致；构造batch的逻辑是否能通用？；pad_texts_to_max_seq_len和convert_example与dataloder耦合，可接耦，并作为通用数据处理函数</font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256608 words <class 'dict'>\n",
      "[PAD] 0\n",
      "[UNK] 1\n",
      "一斤三 2\n",
      "意面屋 3\n",
      "11点25分 4\n"
     ]
    }
   ],
   "source": [
    "# 构造词-id词典\n",
    "vocab = load_vocab(\"./word_dict.txt\")\n",
    "print ('%d words' % len(vocab), type(vocab))\n",
    "for term in list(vocab)[:5]:\n",
    "    print (term, vocab[term])\n",
    "\n",
    "# 为特殊符PAD指定id\n",
    "if '[PAD]' not in vocab:\n",
    "    pad_token_id = len(vocab)\n",
    "    vocab['[PAD]'] = pad_token_id\n",
    "else:\n",
    "    pad_token_id = vocab['[PAD]']\n",
    "\n",
    "# 对齐句子长度\n",
    "def pad_texts_to_max_seq_len(texts, max_seq_len, pad_token_id=0):\n",
    "    for index, text in enumerate(texts):\n",
    "        seq_len = len(text)\n",
    "        if seq_len < max_seq_len:\n",
    "            padded_tokens = [pad_token_id for _ in range(max_seq_len - seq_len)]\n",
    "            new_text = text + padded_tokens\n",
    "            texts[index] = new_text\n",
    "        elif seq_len > max_seq_len:\n",
    "            new_text = text[:max_seq_len]\n",
    "            texts[index] = new_text\n",
    "\n",
    "# 构造batch数据\n",
    "def generate_batch(batch, pad_token_id=0, return_label=True):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        batch(:obj:`Tuple[list]`): The batch data which contains texts, seq_lens and labels.\n",
    "    \"\"\"\n",
    "    seq_lens = [entry[1] for entry in batch]\n",
    "\n",
    "    batch_max_seq_len = max(seq_lens)\n",
    "    texts = [entry[0] for entry in batch]\n",
    "    pad_texts_to_max_seq_len(texts, batch_max_seq_len, pad_token_id)\n",
    "\n",
    "    if return_label:\n",
    "        labels = [[entry[-1]] for entry in batch]\n",
    "        return texts, seq_lens, labels\n",
    "    else:\n",
    "        return texts, seq_lens\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      trans_fn=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      use_gpu=False,\n",
    "                      pad_token_id=0):\n",
    "    if trans_fn:\n",
    "        dataset = SimpleDataset(dataset).apply(\n",
    "            trans_fn, lazy=True)\n",
    "\n",
    "    if mode == 'train' and use_gpu:\n",
    "        sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "        dataloader = paddle.io.DataLoader(\n",
    "            dataset,\n",
    "            batch_sampler=sampler,\n",
    "            return_list=True,\n",
    "            collate_fn=lambda batch: generate_batch(batch,\n",
    "                                                    pad_token_id=pad_token_id))\n",
    "    else:\n",
    "        shuffle = True if mode == 'train' else False\n",
    "        sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        dataloader = paddle.io.DataLoader(\n",
    "            dataset,\n",
    "            batch_sampler=sampler,\n",
    "            return_list=True,\n",
    "            collate_fn=lambda batch: generate_batch(batch,\n",
    "                                                    pad_token_id=pad_token_id))\n",
    "    return dataloader\n",
    "\n",
    "def convert_example(example, vocab, unk_token_id=1, is_test=False):\n",
    "    input_ids = []\n",
    "    for token in jieba.cut(example[0]):\n",
    "        token_id = vocab.get(token, unk_token_id)\n",
    "        input_ids.append(token_id)\n",
    "    valid_length = len(input_ids)\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array(example[-1], dtype=\"int64\")\n",
    "        return input_ids, valid_length, label\n",
    "    else:\n",
    "        return input_ids, valid_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造batch数据集、dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.792 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 1\n",
    "lr = 5e-4\n",
    "\n",
    "trans_fn = partial(\n",
    "        convert_example,\n",
    "        vocab=vocab,\n",
    "        unk_token_id=vocab['[UNK]'],\n",
    "        is_test=False)\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "        train_dataset,\n",
    "        trans_fn=trans_fn,\n",
    "        batch_size=batch_size,\n",
    "        mode='train',\n",
    "        pad_token_id=pad_token_id)\n",
    "dev_loader = create_dataloader(\n",
    "        dev_dataset,\n",
    "        trans_fn=trans_fn,\n",
    "        batch_size=batch_size,\n",
    "        mode='validation',\n",
    "        pad_token_id=pad_token_id)\n",
    "test_loader = create_dataloader(\n",
    "        test_dataset,\n",
    "        trans_fn=trans_fn,\n",
    "        batch_size=batch_size,\n",
    "        mode='test',\n",
    "        pad_token_id=pad_token_id)\n",
    "for k in train_loader:\n",
    "        print (type(k))\n",
    "        # print (k)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、组网、训练、保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组网\n",
    "- 调用`paddle.nn.Embedding`获得词向量   \n",
    "- 调用`paddlenlp.seq2vec的encoder`模型，对句子建模，得到句子向量\n",
    "- 句子向量接入分类器，这里使用`paddle.nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import LSTMEncoder\n",
    "\n",
    "class LSTMModel(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_labels,\n",
    "                 emb_dim=128,\n",
    "                 padding_idx=0,\n",
    "                 lstm_hidden_size=198,\n",
    "                 direction='forward',\n",
    "                 lstm_layers=1,\n",
    "                 dropout_rate=0.0,\n",
    "                 pooling_type=None,\n",
    "                 fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "        self.embedder = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=padding_idx)\n",
    "        self.lstm_encoder = LSTMEncoder(\n",
    "            emb_dim,\n",
    "            lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            direction=direction,\n",
    "            dropout=dropout_rate,\n",
    "            pooling_type=pooling_type)\n",
    "        self.fc = nn.Linear(self.lstm_encoder.get_output_dim(), fc_hidden_size)\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, text, seq_len):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        # print (text.shape)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)\n",
    "        # num_directions = 2 if direction is 'bidirectional'\n",
    "        # if not, num_directions = 1\n",
    "        text_repr = self.lstm_encoder(embedded_text, sequence_length=seq_len)\n",
    "        # Shape: (batch_size, fc_hidden_size)\n",
    "        fc_out = paddle.tanh(self.fc(text_repr))\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        logits = self.output_layer(fc_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 调用`paddle.Model`封装网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "num_labels = len(train_dataset.get_labels())\n",
    "network = LSTMModel(\n",
    "            vocab_size=vocab_size,\n",
    "            num_labels=num_labels,\n",
    "            direction='bidirectional',\n",
    "            padding_idx=pad_token_id)\n",
    "model = paddle.Model(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可调用`model.summary`查看网络结构\n",
    "<font color=red size=4>求助，这里没跑通</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary([(64, 200), (200)], dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层是Embedding层，将每个句子中的词转换为词向量，输出的维度是：句子个数 * 嵌入维度(20)。\n",
    "\n",
    "接下来是LSTM层，将句子表示成向量，输出的纬度是。 \n",
    "\n",
    "然后是全连接层(Full-connected, FC)，即Dense层，x个节点，输出的纬度是。 \n",
    " \n",
    "最后一层是全连接层，只有一个节点。使用sigmoid激活函数，输出值是float，范围0-1，代表可能性/置信度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络配置、训练和保存\n",
    "- 调用`model.prepare`配置模型，如损失函数、优化器。\n",
    "- 调用`model.fit`训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "step  10/150 - loss: 0.0489 - acc_accumulation: 0.9828 - 90ms/step\n",
      "step  20/150 - loss: 0.0210 - acc_accumulation: 0.9844 - 82ms/step\n",
      "step  30/150 - loss: 0.0107 - acc_accumulation: 0.9828 - 77ms/step\n",
      "step  40/150 - loss: 0.1994 - acc_accumulation: 0.9812 - 76ms/step\n",
      "step  50/150 - loss: 0.0354 - acc_accumulation: 0.9822 - 75ms/step\n",
      "step  60/150 - loss: 0.0130 - acc_accumulation: 0.9820 - 74ms/step\n",
      "step  70/150 - loss: 0.2209 - acc_accumulation: 0.9806 - 73ms/step\n",
      "step  80/150 - loss: 0.0503 - acc_accumulation: 0.9795 - 73ms/step\n",
      "step  90/150 - loss: 0.0608 - acc_accumulation: 0.9793 - 72ms/step\n",
      "step 100/150 - loss: 0.1298 - acc_accumulation: 0.9794 - 72ms/step\n",
      "step 110/150 - loss: 0.1620 - acc_accumulation: 0.9786 - 72ms/step\n",
      "step 120/150 - loss: 0.1417 - acc_accumulation: 0.9777 - 71ms/step\n",
      "step 130/150 - loss: 0.0552 - acc_accumulation: 0.9770 - 70ms/step\n",
      "step 140/150 - loss: 0.1250 - acc_accumulation: 0.9770 - 70ms/step\n",
      "step 150/150 - loss: 0.2011 - acc_accumulation: 0.9758 - 69ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/0\n",
      "Eval begin...\n",
      "step 10/19 - loss: 0.3983 - acc_accumulation: 0.9234 - 65ms/step\n",
      "step 19/19 - loss: 0.3928 - acc_accumulation: 0.9100 - 47ms/step\n",
      "Eval samples: 1200\n",
      "save checkpoint at /home/aistudio/checkpoints/final\n"
     ]
    }
   ],
   "source": [
    "optimizer = paddle.optimizer.Adam(\n",
    "        parameters=model.parameters(), learning_rate=lr)\n",
    "\n",
    "loss = paddle.nn.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy(name=\"acc_accumulation\")\n",
    "\n",
    "model.prepare(optimizer, loss, metric)\n",
    "\n",
    "model.fit(train_loader,\n",
    "              dev_loader,\n",
    "              epochs=epochs,\n",
    "              eval_freq=1,\n",
    "              log_freq=10,\n",
    "              save_dir='./checkpoints',\n",
    "              save_freq=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 调用`model.evaluate`评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval begin...\n",
      "step 10/19 - loss: 0.2477 - acc_accumulation: 0.8812 - 68ms/step\n",
      "step 19/19 - loss: 0.1612 - acc_accumulation: 0.8992 - 50ms/step\n",
      "Eval samples: 1200\n",
      "Finally test acc: 0.89917\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_loader)\n",
    "print(\"Finally test acc: %.5f\" % results['acc_accumulation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个非常基础的模型达到了90%的正确率，复杂一点的模型可以达到x%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、网络加载和预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 调用`model.predict`进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 19/19 [==============================] - 49ms/step        \n",
      "Predict samples: 1200\n",
      "<class 'list'> 1 64\n"
     ]
    }
   ],
   "source": [
    "r = model.predict(test_loader)\n",
    "print (type(r), len(r), len(r[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 调用`model.load`加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from checkpoints/final.pdparams\n"
     ]
    }
   ],
   "source": [
    "model_new = paddle.Model(LSTMModel(\n",
    "            vocab_size=vocab_size,\n",
    "            num_labels=num_labels,\n",
    "            direction='bidirectional',\n",
    "            padding_idx=pad_token_id))\n",
    "# Loads model parameters.\n",
    "params_path = 'checkpoints/final.pdparams'\n",
    "model_new.load(params_path)\n",
    "print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过`model.network.eval`设置预测模式，\n",
    "<font color=red size=4>预测时不要构造batch、dataloader，使用`model.predict`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Example(text=[169379, 955944, 1108158, 276253, 823066, 1106339, 939557, 173188, 672225, 224268, 261577, 1250380, 4783, 75937, 142790, 1250380], seq_len=16)\n",
      "Data: 这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般 \t Lable: negative\n",
      "Data: 怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片 \t Lable: negative\n",
      "Data: 作为老的四星酒店，房间依然很整洁，相当不错。机场接机服务很好，可以在车上办理入住手续，节省时间。 \t Lable: positive\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def preprocess_prediction_data(data):\n",
    "    Example = namedtuple('Example', ['text', 'seq_len'])\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        tokens = \" \".join(jieba.cut(text)).split(' ')\n",
    "        ids = convert_tokens_to_ids(tokens, vocab)\n",
    "        example = Example(text=ids, seq_len=len(ids))\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "def predict(model, data, label_map, collate_fn, batch_size=1):\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in data:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.network.eval()\n",
    "    for batch in batches:\n",
    "        texts, seq_lens = collate_fn(\n",
    "            batch, pad_token_id=pad_token_id, return_label=False)\n",
    "        texts = paddle.to_tensor(texts)\n",
    "        seq_lens = paddle.to_tensor(seq_lens)\n",
    "        logits = model.network(texts, seq_lens)\n",
    "        probs = F.softmax(logits, axis=-1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results\n",
    "\n",
    "data = [\n",
    "    '这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般',\n",
    "    '怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片',\n",
    "    '作为老的四星酒店，房间依然很整洁，相当不错。机场接机服务很好，可以在车上办理入住手续，节省时间。',\n",
    "]\n",
    "examples = preprocess_prediction_data(data)\n",
    "print (len(examples), examples[0])\n",
    "label_map = {0: 'negative', 1: 'positive'}\n",
    "results = predict(\n",
    "    model_new,\n",
    "    examples,\n",
    "    label_map=label_map,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=generate_batch)\n",
    "\n",
    "for idx, text in enumerate(data):\n",
    "    print('Data: {} \\t Lable: {}'.format(text, results[idx]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
