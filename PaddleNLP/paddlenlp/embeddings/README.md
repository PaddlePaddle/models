# paddlenlp.embeddings

## TokenEmbedding参数
* embedding_name：string，预训练embedding名称，可通过paddlenlp.embeddings.list_embedding_name查询。
* unknown_token：string，unknown token。
* unknown_token_vector：list 或者 np.array, 用来初始化unknown token对应的vector。默认为None（正态分布初始化vector）
* extended_vocab_path：string，扩展词表的文件名路径。词表格式为一行一个词。
* trainable：bool，是否可训练。True表示Embedding可以更新参数，False为不可更新。

## 初始化
```python
import paddle
from paddlenlp.embeddings import TokenEmbedding, list_embedding_name
paddle.set_device("cpu")

# 查看预训练embedding名称：
print(list_embedding_name()) # ['w2v.baidu_encyclopedia.target.word-word.dim300']

# 初始化TokenEmbedding， 预训练embedding没下载时会自动下载并加载数据
token_embedding = TokenEmbedding(embedding_name="w2v.baidu_encyclopedia.target.word-word.dim300")

# 查看token_embedding详情
print(token_embedding)

Object   type: <paddlenlp.embeddings.token_embedding.TokenEmbedding object at 0x7f67fd192e30>
Unknown index: 1
Unknown token: [UNK]
Padding index: 0
Padding token: [PAD]
Parameter containing:
Tensor(shape=[636015, 300], dtype=float32, place=CPUPlace, stop_gradient=False,
       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ],
        [ 0.00372404,  0.01534354,  0.01341010, ..., -0.00605236, -0.02150303,  0.02372430],
        [-0.24200200,  0.13931701,  0.07378800, ...,  0.14103900,  0.05592300, -0.08004800],
        ...,
        [ 0.01615800, -0.00266300, -0.00628300, ...,  0.01484100,  0.00196600, -0.01032000],
        [ 0.01705700,  0.00040400, -0.01222000, ...,  0.02837200,  0.02402500, -0.00814800],
        [ 0.02628800, -0.00008300, -0.00393500, ...,  0.00654000,  0.00024600, -0.00662600]])
```

## 查询embedding结果

```python
test_token_embedding = token_embedding.search("中国")
print(test_token_embedding)
[[ 0.260801  0.1047    0.129453 -0.257317 -0.16152   0.19567  -0.074868
   0.361168  0.245882 -0.219141 -0.388083  0.235189  0.029316  0.154215
  -0.354343  0.017746  0.009028  0.01197  -0.121429  0.096542  0.009255
   0.039721  0.363704 -0.239497 -0.41168   0.16958   0.261758  0.022383
  -0.053248 -0.000994 -0.209913 -0.208296  0.197332 -0.3426   -0.162112
   0.134557 -0.250201  0.431298  0.303116  0.517221  0.243843  0.022219
  -0.136554 -0.189223  0.148563 -0.042963 -0.456198  0.14546  -0.041207
   0.049685  0.20294   0.147355 -0.206953 -0.302796 -0.111834  0.128183
   0.289539 -0.298934 -0.096412  0.063079  0.324821 -0.144471  0.052456
   0.088761 -0.040925 -0.103281 -0.216065 -0.200878 -0.100664  0.170614
  -0.355546 -0.062115 -0.52595  -0.235442  0.300866 -0.521523 -0.070713
  -0.331768  0.023021  0.309111 -0.125696  0.016723 -0.0321   -0.200611
   0.057294 -0.128891 -0.392886  0.423002  0.282569 -0.212836  0.450132
   0.067604 -0.124928 -0.294086  0.136479  0.091505 -0.061723 -0.577495
   0.293856 -0.401198  0.302559 -0.467656  0.021708 -0.088507  0.088322
  -0.015567  0.136594  0.112152  0.005394  0.133818  0.071278 -0.198807
   0.043538  0.116647 -0.210486 -0.217972 -0.320675  0.293977  0.277564
   0.09591  -0.359836  0.473573  0.083847  0.240604  0.441624  0.087959
   0.064355 -0.108271  0.055709  0.380487 -0.045262  0.04014  -0.259215
  -0.398335  0.52712  -0.181298  0.448978 -0.114245 -0.028225 -0.146037
   0.347414 -0.076505  0.461865 -0.105099  0.131892  0.079946  0.32422
  -0.258629  0.05225   0.566337  0.348371  0.124111  0.229154  0.075039
  -0.139532 -0.08839  -0.026703 -0.222828 -0.106018  0.324477  0.128269
  -0.045624  0.071815 -0.135702  0.261474  0.297334 -0.031481  0.18959
   0.128716  0.090022  0.037609 -0.049669  0.092909  0.0564   -0.347994
  -0.367187 -0.292187  0.021649 -0.102004 -0.398568 -0.278248 -0.082361
  -0.161823  0.044846  0.212597 -0.013164  0.005527 -0.004024  0.176243
   0.237274 -0.174856 -0.197214  0.150825 -0.164427 -0.244255 -0.14897
   0.098907 -0.295891 -0.013408 -0.146875 -0.126049  0.033235 -0.133444
  -0.003258  0.082053 -0.162569  0.283657  0.315608 -0.171281 -0.276051
   0.258458  0.214045 -0.129798 -0.511728  0.198481 -0.35632  -0.186253
  -0.203719  0.22004  -0.016474  0.080321 -0.463004  0.290794 -0.003445
   0.061247 -0.069157 -0.022525  0.13514   0.001354  0.011079  0.014223
  -0.079145 -0.41402  -0.404242 -0.301509  0.036712  0.037076 -0.061683
  -0.202429  0.130216  0.054355  0.140883 -0.030627 -0.281293 -0.28059
  -0.214048 -0.467033  0.203632 -0.541544  0.183898 -0.129535 -0.286422
  -0.162222  0.262487  0.450505  0.11551  -0.247965 -0.15837   0.060613
  -0.285358  0.498203  0.025008 -0.256397  0.207582  0.166383  0.669677
  -0.067961 -0.049835 -0.444369  0.369306  0.134493 -0.080478 -0.304565
  -0.091756  0.053657  0.114497 -0.076645 -0.123933  0.168645  0.018987
  -0.260592 -0.019668 -0.063312 -0.094939  0.657352  0.247547 -0.161621
   0.289043 -0.284084  0.205076  0.059885  0.055871  0.159309  0.062181
   0.123634  0.282932  0.140399 -0.076253 -0.087103  0.07262 ]]
```

## 计算词向量cosine相似度

```python
score = token_embedding.cosine_sim("中国", "美国")
print(score) # 0.49586025
```

## 计算词向量内积

```python
score = token_embedding.dot("中国", "美国")
print(score) # 8.611071
```


## 训练

```python
in_words = paddle.to_tensor([0, 2, 3])
input_embeddings = token_embedding(in_words)
linear = paddle.nn.Linear(token_embedding.embedding_dim, 20)
input_fc = linear(input_embeddings)
print(input_fc)
Tensor(shape=[3, 20], dtype=float32, place=CPUPlace, stop_gradient=False,
       [[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],
        [-0.23473957,  0.17878169,  0.07215232, -0.14522184, -0.03620089,  0.41896617,  0.01590110, -0.29603502,  0.13492256,  0.15482053,  0.22773527,  0.04608973, -0.05948054,  0.12360383,  0.19023067,  0.18236779,  0.24359611,  0.03698236,  0.14291850,  0.05136518],
        [-0.42466098,  0.15017235, -0.04780108, -0.26157820, -0.07374368,  0.23643826, -0.12256938, -0.26510242, -0.20593795, -0.01788964,  0.53416538, -0.10751566,  0.03907865,  0.10803267,  0.35362548,  0.14286348,  0.19552971, -0.04995505,  0.15847842,  0.00025209]])
```

## 切词

```python
from paddlenlp.data import JiebaTokenizer
tokenizer = JiebaTokenizer(vocab=token_embedding.vocab)
words = tokenizer.cut("中国人民")
print(words) # ['中国人', '民']

tokens = tokenizer.encode("中国人民")
print(tokens) # [12532, 1336]
```
