{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Mengzi-T5 model (Chinese)\n", "Pretrained model on 300G Chinese corpus.\n", "\n", "[Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese](https://arxiv.org/abs/2110.06696)\n", "\n", "## Usage\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import T5Tokenizer, T5ForConditionalGeneration\n", "\n", "tokenizer = T5Tokenizer.from_pretrained(\"Langboat/mengzi-t5-base\")\n", "model = T5ForConditionalGeneration.from_pretrained(\"Langboat/mengzi-t5-base\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Citation\n", "If you find the technical report or resource is useful, please cite the following technical report in your paper.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["@misc{zhang2021mengzi,\n", "title={Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese},\n", "author={Zhuosheng Zhang and Hanqing Zhang and Keming Chen and Yuhang Guo and Jingyun Hua and Yulong Wang and Ming Zhou},\n", "year={2021},\n", "eprint={2110.06696},\n", "archivePrefix={arXiv},\n", "primaryClass={cs.CL}\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> 此模型来源于：[https://huggingface.co/Langboat/mengzi-t5-base](https://huggingface.co/https://huggingface.co/Langboat/mengzi-t5-base)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [Langboat/mengzi-t5-base](https://huggingface.co/Langboat/mengzi-t5-base)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}