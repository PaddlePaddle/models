{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Mengzi-BERT base fin model (Chinese)\n", "Continue trained mengzi-bert-base with 20G financial news and research reports. Masked language modeling(MLM), part-of-speech(POS) tagging and sentence order prediction(SOP) are used as training task.\n", "\n", "[Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese](https://arxiv.org/abs/2110.06696)\n", "\n", "## Usage\n"]}, {"cell_type": "code", "metadata": {}, "source": "import paddle\nfrom paddlenlp.transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"Langboat/mengzi-bert-base-fin\")\ninput_ids = paddle.randint(100, 200, shape=[1, 20])\nprint(model(input_ids))"}, {"cell_type": "code", "metadata": {}, "source": ["@misc{zhang2021mengzi,\n", "title={Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese},\n", "author={Zhuosheng Zhang and Hanqing Zhang and Keming Chen and Yuhang Guo and Jingyun Hua and Yulong Wang and Ming Zhou},\n", "year={2021},\n", "eprint={2110.06696},\n", "archivePrefix={arXiv},\n", "primaryClass={cs.CL}\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> 此模型来源于：[https://huggingface.co/Langboat/mengzi-bert-base-fin](https://huggingface.co/https://huggingface.co/Langboat/mengzi-bert-base-fin)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> 此模型来源于：[Langboat/mengzi-bert-base-fin](https://huggingface.co/Langboat/mengzi-bert-base-fin)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}