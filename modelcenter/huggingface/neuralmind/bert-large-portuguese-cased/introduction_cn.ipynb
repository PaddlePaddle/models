{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# BERTimbau Large (aka \"bert-large-portuguese-cased\")\n", "\n", "![Bert holding a berimbau](https://imgur.com/JZ7Hynh.jpg)\n", "\n", "## Introduction\n", "\n", "BERTimbau Large is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.\n", "\n", "For further information or requests, please go to [BERTimbau repository](https://github.com/neuralmind-ai/portuguese-bert/).\n", "\n", "## Available models\n", "\n", "| Model                                    | Arch.      | #Layers | #Params |\n", "| ---------------------------------------- | ---------- | ------- | ------- |\n", "| `neuralmind/bert-base-portuguese-cased`  | BERT-Base  | 12      | 110M    |\n", "| `neuralmind/bert-large-portuguese-cased` | BERT-Large | 24      | 335M    |\n", "\n", "## Usage\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import AutoTokenizer  # Or BertTokenizer\n", "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n", "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n", "\n", "model = AutoModelForPreTraining.from_pretrained('neuralmind/bert-large-portuguese-cased')\n", "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', do_lower_case=False)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Masked language modeling prediction example\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import pipeline\n", "\n", "pipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n", "\n", "pipe('Tinha uma [MASK] no meio do caminho.')\n", "# [{'score': 0.5054386258125305,\n", "#   'sequence': '[CLS] Tinha uma pedra no meio do caminho. [SEP]',\n", "#   'token': 5028,\n", "#   'token_str': 'pedra'},\n", "#  {'score': 0.05616172030568123,\n", "#   'sequence': '[CLS] Tinha uma curva no meio do caminho. [SEP]',\n", "#   'token': 9562,\n", "#   'token_str': 'curva'},\n", "#  {'score': 0.02348282001912594,\n", "#   'sequence': '[CLS] Tinha uma parada no meio do caminho. [SEP]',\n", "#   'token': 6655,\n", "#   'token_str': 'parada'},\n", "#  {'score': 0.01795753836631775,\n", "#   'sequence': '[CLS] Tinha uma mulher no meio do caminho. [SEP]',\n", "#   'token': 2606,\n", "#   'token_str': 'mulher'},\n", "#  {'score': 0.015246033668518066,\n", "#   'sequence': '[CLS] Tinha uma luz no meio do caminho. [SEP]',\n", "#   'token': 3377,\n", "#   'token_str': 'luz'}]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### For BERT embeddings\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import torch\n", "\n", "model = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased')\n", "input_ids = tokenizer.encode('Tinha uma pedra no meio do caminho.', return_tensors='pt')\n", "\n", "with torch.no_grad():\n", "outs = model(input_ids)\n", "encoded = outs[0][0, 1:-1]  # Ignore [CLS] and [SEP] special tokens\n", "\n", "# encoded.shape: (8, 1024)\n", "# tensor([[ 1.1872,  0.5606, -0.2264,  ...,  0.0117, -0.1618, -0.2286],\n", "#         [ 1.3562,  0.1026,  0.1732,  ..., -0.3855, -0.0832, -0.1052],\n", "#         [ 0.2988,  0.2528,  0.4431,  ...,  0.2684, -0.5584,  0.6524],\n", "#         ...,\n", "#         [ 0.3405, -0.0140, -0.0748,  ...,  0.6649, -0.8983,  0.5802],\n", "#         [ 0.1011,  0.8782,  0.1545,  ..., -0.1768, -0.8880, -0.1095],\n", "#         [ 0.7912,  0.9637, -0.3859,  ...,  0.2050, -0.1350,  0.0432]])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Citation\n", "\n", "If you use our work, please cite:\n"]}, {"cell_type": "code", "metadata": {}, "source": ["@inproceedings{souza2020bertimbau,\n", "author    = {F{\\'a}bio Souza and\n", "Rodrigo Nogueira and\n", "Roberto Lotufo},\n", "title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n", "booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n", "year      = {2020}\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> 此模型来源于：[https://huggingface.co/neuralmind/bert-large-portuguese-cased](https://huggingface.co/https://huggingface.co/neuralmind/bert-large-portuguese-cased)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [neuralmind/bert-large-portuguese-cased](https://huggingface.co/neuralmind/bert-large-portuguese-cased)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}