{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## RoBERTa-base fine-tuned on SQuAD v1\n", "\n", "This model was fine-tuned from the HuggingFace [RoBERTa](https://arxiv.org/abs/1907.11692) base checkpoint on [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer).\n", "This model is case-sensitive: it makes a difference between english and English.\n", "\n", "## Details\n", "\n", "| Dataset  | Split | # samples |\n", "| -------- | ----- | --------- |\n", "| SQuAD1.1 | train | 96.8K      |\n", "| SQuAD1.1 | eval  | 11.8k     |\n", "\n", "\n", "### Fine-tuning\n", "- Python: `3.7.5`\n", "\n", "- Machine specs:\n", "\n", "`CPU: Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz`\n", "\n", "`Memory: 32 GiB`\n", "\n", "`GPUs: 2 GeForce GTX 1070, each with 8GiB memory`\n", "\n", "`GPU driver: 418.87.01, CUDA: 10.1`\n", "\n", "- script:\n"]}, {"cell_type": "code", "metadata": {}, "source": ["# after install https://github.com/huggingface/transformers\n", "\n", "cd examples/question-answering\n", "mkdir -p data\n", "\n", "wget -O data/train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n", "\n", "wget -O data/dev-v1.1.json  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n", "\n", "python run_energy_squad.py \\\n", "--model_type roberta \\\n", "--model_name_or_path roberta-base \\\n", "--do_train \\\n", "--do_eval \\\n", "--train_file train-v1.1.json \\\n", "--predict_file dev-v1.1.json \\\n", "--per_gpu_train_batch_size 12 \\\n", "--per_gpu_eval_batch_size 16 \\\n", "--learning_rate 3e-5 \\\n", "--num_train_epochs 2.0 \\\n", "--max_seq_length 320 \\\n", "--doc_stride 128 \\\n", "--data_dir data \\\n", "--output_dir data/roberta-base-squad-v1 2>&1 | tee train-roberta-base-squad-v1.log\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It took about 2 hours to finish.\n", "\n", "### Results\n", "\n", "**Model size**: `477M`\n", "\n", "| Metric | # Value   |\n", "| ------ | --------- |\n", "| **EM** | **83.0** |\n", "| **F1** | **90.4** |\n", "\n", "Note that the above results didn't involve any hyperparameter search.\n", "\n", "## Example Usage\n", "\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import pipeline\n", "\n", "qa_pipeline = pipeline(\n", "\"question-answering\",\n", "model=\"csarron/roberta-base-squad-v1\",\n", "tokenizer=\"csarron/roberta-base-squad-v1\"\n", ")\n", "\n", "predictions = qa_pipeline({\n", "'context': \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n", "'question': \"What day was the game played on?\"\n", "})\n", "\n", "print(predictions)\n", "# output:\n", "# {'score': 0.8625259399414062, 'start': 23, 'end': 39, 'answer': 'February 7, 2016'}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> Created by [Qingqing Cao](https://awk.ai/) | [GitHub](https://github.com/csarron) | [Twitter](https://twitter.com/sysnlp)\n", "\n", "> Made with ❤️ in New York.\n", "> 此模型来源于：[https://huggingface.co/csarron/roberta-base-squad-v1](https://huggingface.co/https://huggingface.co/csarron/roberta-base-squad-v1)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [csarron/roberta-base-squad-v1](https://huggingface.co/csarron/roberta-base-squad-v1)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}