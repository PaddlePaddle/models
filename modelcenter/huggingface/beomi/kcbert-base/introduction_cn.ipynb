{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# KcBERT: Korean comments BERT\n", "\n", "** Updates on 2021.04.07 **\n", "\n", "- KcELECTRAê°€ ë¦´ë¦¬ì¦ˆ ë˜ì—ˆìŠµë‹ˆë‹¤!ğŸ¤—\n", "- KcELECTRAëŠ” ë³´ë‹¤ ë” ë§ì€ ë°ì´í„°ì…‹, ê·¸ë¦¬ê³  ë” í° General vocabì„ í†µí•´ KcBERT ëŒ€ë¹„ **ëª¨ë“  íƒœìŠ¤í¬ì—ì„œ ë” ë†’ì€ ì„±ëŠ¥**ì„ ë³´ì…ë‹ˆë‹¤.\n", "- ì•„ë˜ ê¹ƒí—™ ë§í¬ì—ì„œ ì§ì ‘ ì‚¬ìš©í•´ë³´ì„¸ìš”!\n", "- https://github.com/Beomi/KcELECTRA\n", "\n", "** Updates on 2021.03.14 **\n", "\n", "- KcBERT Paper ì¸ìš© í‘œê¸°ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.(bibtex)\n", "- KcBERT-finetune Performance scoreë¥¼ ë³¸ë¬¸ì— ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.\n", "\n", "** Updates on 2020.12.04 **\n", "\n", "Huggingface Transformersê°€ v4.0.0ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨ì— ë”°ë¼ Tutorialì˜ ì½”ë“œê°€ ì¼ë¶€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.\n", "\n", "ì—…ë°ì´íŠ¸ëœ KcBERT-Large NSMC Finetuning Colab: <a href=\"https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing\">\n", "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n", "</a>\n", "\n", "** Updates on 2020.09.11 **\n", "\n", "KcBERTë¥¼ Google Colabì—ì„œ TPUë¥¼ í†µí•´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” íŠœí† ë¦¬ì–¼ì„ ì œê³µí•©ë‹ˆë‹¤! ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ë³´ì„¸ìš”.\n", "\n", "Colabì—ì„œ TPUë¡œ KcBERT Pretrain í•´ë³´ê¸°: <a href=\"https://colab.research.google.com/drive/1lYBYtaXqt9S733OXdXvrvC09ysKFN30W\">\n", "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n", "</a>\n", "\n", "í…ìŠ¤íŠ¸ ë¶„ëŸ‰ë§Œ ì „ì²´ 12G í…ìŠ¤íŠ¸ ì¤‘ ì¼ë¶€(144MB)ë¡œ ì¤„ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.\n", "\n", "í•œêµ­ì–´ ë°ì´í„°ì…‹/ì½”í¼ìŠ¤ë¥¼ ì¢€ë” ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” [Korpora](https://github.com/ko-nlp/Korpora) íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n", "\n", "** Updates on 2020.09.08 **\n", "\n", "Github Releaseë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì˜€ìŠµë‹ˆë‹¤.\n", "\n", "ë‹¤ë§Œ í•œ íŒŒì¼ë‹¹ 2GB ì´ë‚´ì˜ ì œì•½ìœ¼ë¡œ ì¸í•´ ë¶„í• ì••ì¶•ë˜ì–´ìˆìŠµë‹ˆë‹¤.\n", "\n", "ì•„ë˜ ë§í¬ë¥¼ í†µí•´ ë°›ì•„ì£¼ì„¸ìš”. (ê°€ì… ì—†ì´ ë°›ì„ ìˆ˜ ìˆì–´ìš”. ë¶„í• ì••ì¶•)\n", "\n", "ë§Œì•½ í•œ íŒŒì¼ë¡œ ë°›ê³ ì‹¶ìœ¼ì‹œê±°ë‚˜/Kaggleì—ì„œ ë°ì´í„°ë¥¼ ì‚´í´ë³´ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ì•„ë˜ì˜ ìºê¸€ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ì£¼ì„¸ìš”.\n", "\n", "- Githubë¦´ë¦¬ì¦ˆ: https://github.com/Beomi/KcBERT/releases/tag/TrainData_v1\n", "\n", "** Updates on 2020.08.22 **\n", "\n", "Pretrain Dataset ê³µê°œ\n", "\n", "- ìºê¸€: https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments (í•œ íŒŒì¼ë¡œ ë°›ì„ ìˆ˜ ìˆì–´ìš”. ë‹¨ì¼íŒŒì¼)\n", "\n", "Kaggleì— í•™ìŠµì„ ìœ„í•´ ì •ì œí•œ(ì•„ë˜ `clean`ì²˜ë¦¬ë¥¼ ê±°ì¹œ) Datasetì„ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤!\n", "\n", "ì§ì ‘ ë‹¤ìš´ë°›ìœ¼ì…”ì„œ ë‹¤ì–‘í•œ Taskì— í•™ìŠµì„ ì§„í–‰í•´ë³´ì„¸ìš” :)\n", "\n", "\n", "ê³µê°œëœ í•œêµ­ì–´ BERTëŠ” ëŒ€ë¶€ë¶„ í•œêµ­ì–´ ìœ„í‚¤, ë‰´ìŠ¤ ê¸°ì‚¬, ì±… ë“± ì˜ ì •ì œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì…ë‹ˆë‹¤. í•œí¸, ì‹¤ì œë¡œ NSMCì™€ ê°™ì€ ëŒ“ê¸€í˜• ë°ì´í„°ì…‹ì€ ì •ì œë˜ì§€ ì•Šì•˜ê³  êµ¬ì–´ì²´ íŠ¹ì§•ì— ì‹ ì¡°ì–´ê°€ ë§ìœ¼ë©°, ì˜¤íƒˆì ë“± ê³µì‹ì ì¸ ê¸€ì“°ê¸°ì—ì„œ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” í‘œí˜„ë“¤ì´ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•©ë‹ˆë‹¤.\n", "\n", "KcBERTëŠ” ìœ„ì™€ ê°™ì€ íŠ¹ì„±ì˜ ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸° ìœ„í•´, ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ì„ ìˆ˜ì§‘í•´, í† í¬ë‚˜ì´ì €ì™€ BERTëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•œ Pretrained BERT ëª¨ë¸ì…ë‹ˆë‹¤.\n", "\n", "KcBERTëŠ” Huggingfaceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ê°„í¸íˆ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë³„ë„ì˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)\n", "\n", "## KcBERT Performance\n", "\n", "- Finetune ì½”ë“œëŠ” https://github.com/Beomi/KcBERT-finetune ì—ì„œ ì°¾ì•„ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n", "\n", "|                       | Size<br/>(ìš©ëŸ‰)  | **NSMC**<br/>(acc) | **Naver NER**<br/>(F1) | **PAWS**<br/>(acc) | **KorNLI**<br/>(acc) | **KorSTS**<br/>(spearman) | **Question Pair**<br/>(acc) | **KorQuaD (Dev)**<br/>(EM/F1) |\n", "| :-------------------- | :---: | :----------------: | :--------------------: | :----------------: | :------------------: | :-----------------------: | :-------------------------: | :---------------------------: |\n", "| KcBERT-Base                | 417M  |       89.62        |         84.34          |       66.95        |        74.85         |           75.57           |            93.93            |         60.25 / 84.39         |\n", "| KcBERT-Large                | 1.2G  |       **90.68**        |         85.53          |       70.15        |        76.99         |           77.49           |            94.06            |         62.16 / 86.64          |\n", "| KoBERT                | 351M  |       89.63        |         86.11          |       80.65        |        79.00         |           79.64           |            93.93            |         52.81 / 80.27         |\n", "| XLM-Roberta-Base      | 1.03G |       89.49        |         86.26          |       82.95        |        79.92         |           79.09           |            93.53            |         64.70 / 88.94         |\n", "| HanBERT               | 614M  |       90.16        |       **87.31**        |       82.40        |      **80.89**       |           83.33           |            94.19            |         78.74 / 92.02         |\n", "| KoELECTRA-Base    | 423M  |     **90.21**      |         86.87          |       81.90        |        80.85         |           83.21           |            94.20            |         61.10 / 89.59         |\n", "| KoELECTRA-Base-v2 | 423M  |       89.70        |         87.02          |     **83.90**      |        80.61         |         **84.30**         |          **94.72**          |       **84.34 / 92.58**       |\n", "| DistilKoBERT           | 108M |       88.41        |         84.13          |       62.55        |        70.55         |           73.21           |            92.48            |         54.12 / 77.80         |\n", "\n", "\n", "\\*HanBERTì˜ SizeëŠ” Bert Modelê³¼ Tokenizer DBë¥¼ í•©ì¹œ ê²ƒì…ë‹ˆë‹¤.\n", "\n", "\\***configì˜ ì„¸íŒ…ì„ ê·¸ëŒ€ë¡œ í•˜ì—¬ ëŒë¦° ê²°ê³¼ì´ë©°, hyperparameter tuningì„ ì¶”ê°€ì ìœ¼ë¡œ í•  ì‹œ ë” ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**\n", "\n", "## How to use\n", "\n", "### Requirements\n", "\n", "- `pytorch <= 1.8.0`\n", "- `transformers ~= 3.0.1`\n", "- `transformers ~= 4.0.0` ë„ í˜¸í™˜ë©ë‹ˆë‹¤.\n", "- `emoji ~= 0.6.0`\n", "- `soynlp ~= 0.0.493`\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import AutoTokenizer, AutoModelWithLMHead\n", "\n", "# Base Model (108M)\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n", "\n", "model = AutoModelWithLMHead.from_pretrained(\"beomi/kcbert-base\")\n", "\n", "# Large Model (334M)\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-large\")\n", "\n", "model = AutoModelWithLMHead.from_pretrained(\"beomi/kcbert-large\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Pretrain & Finetune Colab ë§í¬ ëª¨ìŒ\n", "\n", "#### Pretrain Data\n", "\n", "- [ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ(Kaggle, ë‹¨ì¼íŒŒì¼, ë¡œê·¸ì¸ í•„ìš”)](https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments)\n", "- [ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ(Github, ì••ì¶• ì—¬ëŸ¬íŒŒì¼, ë¡œê·¸ì¸ ë¶ˆí•„ìš”)](https://github.com/Beomi/KcBERT/releases/tag/TrainData_v1)\n", "\n", "#### Pretrain Code\n", "\n", "Colabì—ì„œ TPUë¡œ KcBERT Pretrain í•´ë³´ê¸°: <a href=\"https://colab.research.google.com/drive/1lYBYtaXqt9S733OXdXvrvC09ysKFN30W\">\n", "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n", "</a>\n", "\n", "#### Finetune Samples\n", "\n", "**KcBERT-Base** NSMC Finetuning with PyTorch-Lightning (Colab) <a href=\"https://colab.research.google.com/drive/1fn4sVJ82BrrInjq6y5655CYPP-1UKCLb?usp=sharing\">\n", "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n", "</a>\n", "\n", "**KcBERT-Large** NSMC Finetuning with PyTorch-Lightning (Colab) <a href=\"https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing\">\n", "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n", "</a>\n", "\n", "> ìœ„ ë‘ ì½”ë“œëŠ” Pretrain ëª¨ë¸(base, large)ì™€ batch sizeë§Œ ë‹¤ë¥¼ ë¿, ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ì™„ì „íˆ ë™ì¼í•©ë‹ˆë‹¤.\n", "\n", "## Train Data & Preprocessing\n", "\n", "### Raw Data\n", "\n", "í•™ìŠµ ë°ì´í„°ëŠ” 2019.01.01 ~ 2020.06.15 ì‚¬ì´ì— ì‘ì„±ëœ **ëŒ“ê¸€ ë§ì€ ë‰´ìŠ¤** ê¸°ì‚¬ë“¤ì˜ **ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€**ì„ ëª¨ë‘ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤.\n", "\n", "ë°ì´í„° ì‚¬ì´ì¦ˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œì‹œ **ì•½ 15.4GBì´ë©°, 1ì–µ1ì²œë§Œê°œ ì´ìƒì˜ ë¬¸ì¥**ìœ¼ë¡œ ì´ë¤„ì ¸ ìˆìŠµë‹ˆë‹¤.\n", "\n", "### Preprocessing\n", "\n", "PLM í•™ìŠµì„ ìœ„í•´ì„œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n", "\n", "1. í•œê¸€ ë° ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, ê·¸ë¦¬ê³  ì´ëª¨ì§€(ğŸ¥³)ê¹Œì§€!\n", "\n", "ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ í•œê¸€, ì˜ì–´, íŠ¹ìˆ˜ë¬¸ìë¥¼ í¬í•¨í•´ Emojiê¹Œì§€ í•™ìŠµ ëŒ€ìƒì— í¬í•¨í–ˆìŠµë‹ˆë‹¤.\n", "\n", "í•œí¸, í•œê¸€ ë²”ìœ„ë¥¼ `ã„±-ã…ê°€-í£` ìœ¼ë¡œ ì§€ì •í•´ `ã„±-í£` ë‚´ì˜ í•œìë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤.\n", "\n", "2. ëŒ“ê¸€ ë‚´ ì¤‘ë³µ ë¬¸ìì—´ ì¶•ì•½\n", "\n", "`ã…‹ã…‹ã…‹ã…‹ã…‹`ì™€ ê°™ì´ ì¤‘ë³µëœ ê¸€ìë¥¼ `ã…‹ã…‹`ì™€ ê°™ì€ ê²ƒìœ¼ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.\n", "\n", "3. Cased Model\n", "\n", "KcBERTëŠ” ì˜ë¬¸ì— ëŒ€í•´ì„œëŠ” ëŒ€ì†Œë¬¸ìë¥¼ ìœ ì§€í•˜ëŠ” Cased modelì…ë‹ˆë‹¤.\n", "\n", "4. ê¸€ì ë‹¨ìœ„ 10ê¸€ì ì´í•˜ ì œê±°\n", "\n", "10ê¸€ì ë¯¸ë§Œì˜ í…ìŠ¤íŠ¸ëŠ” ë‹¨ì¼ ë‹¨ì–´ë¡œ ì´ë¤„ì§„ ê²½ìš°ê°€ ë§ì•„ í•´ë‹¹ ë¶€ë¶„ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤.\n", "\n", "5. ì¤‘ë³µ ì œê±°\n", "\n", "ì¤‘ë³µì ìœ¼ë¡œ ì“°ì¸ ëŒ“ê¸€ì„ ì œê±°í•˜ê¸° ìœ„í•´ ì¤‘ë³µ ëŒ“ê¸€ì„ í•˜ë‚˜ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.\n", "\n", "ì´ë¥¼ í†µí•´ ë§Œë“  ìµœì¢… í•™ìŠµ ë°ì´í„°ëŠ” **12.5GB, 8.9ì²œë§Œê°œ ë¬¸ì¥**ì…ë‹ˆë‹¤.\n", "\n", "ì•„ë˜ ëª…ë ¹ì–´ë¡œ pipë¡œ ì„¤ì¹˜í•œ ë’¤, ì•„ë˜ cleaní•¨ìˆ˜ë¡œ í´ë¦¬ë‹ì„ í•˜ë©´ Downstream taskì—ì„œ ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤. (`[UNK]` ê°ì†Œ)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["pip install soynlp emoji\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ì•„ë˜ `clean` í•¨ìˆ˜ë¥¼ Text dataì— ì‚¬ìš©í•´ì£¼ì„¸ìš”.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import re\n", "import emoji\n", "from soynlp.normalizer import repeat_normalize\n", "\n", "emojis = list({y for x in emoji.UNICODE_EMOJI.values() for y in x.keys()})\n", "emojis = ''.join(emojis)\n", "pattern = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\\x00-\\x7Fã„±-ã…£ê°€-í£{emojis}]+')\n", "url_pattern = re.compile(\n", "r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n", "\n", "def clean(x):\n", "x = pattern.sub(' ', x)\n", "x = url_pattern.sub('', x)\n", "x = x.strip()\n", "x = repeat_normalize(x, num_repeats=2)\n", "return x\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Cleaned Data (Released on Kaggle)\n", "\n", "ì›ë³¸ ë°ì´í„°ë¥¼ ìœ„ `clean`í•¨ìˆ˜ë¡œ ì •ì œí•œ 12GBë¶„ëŸ‰ì˜ txt íŒŒì¼ì„ ì•„ë˜ Kaggle Datasetì—ì„œ ë‹¤ìš´ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ :)\n", "\n", "https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments\n", "\n", "\n", "## Tokenizer Train\n", "\n", "TokenizerëŠ” Huggingfaceì˜ [Tokenizers](https://github.com/huggingface/tokenizers) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n", "\n", "ê·¸ ì¤‘ `BertWordPieceTokenizer` ë¥¼ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , Vocab SizeëŠ” `30000`ìœ¼ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n", "\n", "Tokenizerë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì—ëŠ” `1/10`ë¡œ ìƒ˜í”Œë§í•œ ë°ì´í„°ë¡œ í•™ìŠµì„ ì§„í–‰í–ˆê³ , ë³´ë‹¤ ê³¨ê³ ë£¨ ìƒ˜í”Œë§í•˜ê¸° ìœ„í•´ ì¼ìë³„ë¡œ stratifyë¥¼ ì§€ì •í•œ ë’¤ í–‘ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n", "\n", "## BERT Model Pretrain\n", "\n", "- KcBERT Base config\n"]}, {"cell_type": "code", "metadata": {}, "source": ["{\n", "\"max_position_embeddings\": 300,\n", "\"hidden_dropout_prob\": 0.1,\n", "\"hidden_act\": \"gelu\",\n", "\"initializer_range\": 0.02,\n", "\"num_hidden_layers\": 12,\n", "\"type_vocab_size\": 2,\n", "\"vocab_size\": 30000,\n", "\"hidden_size\": 768,\n", "\"attention_probs_dropout_prob\": 0.1,\n", "\"directionality\": \"bidi\",\n", "\"num_attention_heads\": 12,\n", "\"intermediate_size\": 3072,\n", "\"architectures\": [\n", "\"BertForMaskedLM\"\n", "],\n", "\"model_type\": \"bert\"\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- KcBERT Large config\n"]}, {"cell_type": "code", "metadata": {}, "source": ["{\n", "\"type_vocab_size\": 2,\n", "\"initializer_range\": 0.02,\n", "\"max_position_embeddings\": 300,\n", "\"vocab_size\": 30000,\n", "\"hidden_size\": 1024,\n", "\"hidden_dropout_prob\": 0.1,\n", "\"model_type\": \"bert\",\n", "\"directionality\": \"bidi\",\n", "\"pad_token_id\": 0,\n", "\"layer_norm_eps\": 1e-12,\n", "\"hidden_act\": \"gelu\",\n", "\"num_hidden_layers\": 24,\n", "\"num_attention_heads\": 16,\n", "\"attention_probs_dropout_prob\": 0.1,\n", "\"intermediate_size\": 4096,\n", "\"architectures\": [\n", "\"BertForMaskedLM\"\n", "]\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["BERT Model ConfigëŠ” Base, Large ê¸°ë³¸ ì„¸íŒ…ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (MLM 15% ë“±)\n", "\n", "TPU `v3-8` ì„ ì´ìš©í•´ ê°ê° 3ì¼, Nì¼(LargeëŠ” í•™ìŠµ ì§„í–‰ ì¤‘)ì„ ì§„í–‰í–ˆê³ , í˜„ì¬ Huggingfaceì— ê³µê°œëœ ëª¨ë¸ì€ 1m(100ë§Œ) stepì„ í•™ìŠµí•œ ckptê°€ ì—…ë¡œë“œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.\n", "\n", "ëª¨ë¸ í•™ìŠµ LossëŠ” Stepì— ë”°ë¼ ì´ˆê¸° 200kì— ê°€ì¥ ë¹ ë¥´ê²Œ Lossê°€ ì¤„ì–´ë“¤ë‹¤ 400kì´í›„ë¡œëŠ” ì¡°ê¸ˆì”© ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n", "\n", "- Base Model Loss\n", "\n", "![KcBERT-Base Pretraining Loss](https://raw.githubusercontent.com/Beomi/KcBERT/master/img/image-20200719183852243.38b124.png)\n", "\n", "- Large Model Loss\n", "\n", "![KcBERT-Large Pretraining Loss](https://raw.githubusercontent.com/Beomi/KcBERT/master/img/image-20200806160746694.d56fa1.png)\n", "\n", "í•™ìŠµì€ GCPì˜ TPU v3-8ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , í•™ìŠµ ì‹œê°„ì€ Base Model ê¸°ì¤€ 2.5ì¼ì •ë„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. Large Modelì€ ì•½ 5ì¼ì •ë„ ì§„í–‰í•œ ë’¤ ê°€ì¥ ë‚®ì€ lossë¥¼ ê°€ì§„ ì²´í¬í¬ì¸íŠ¸ë¡œ ì •í–ˆìŠµë‹ˆë‹¤.\n", "\n", "## Example\n", "\n", "### HuggingFace MASK LM\n", "\n", "[HuggingFace kcbert-base ëª¨ë¸](https://huggingface.co/beomi/kcbert-base?text=ì˜¤ëŠ˜ì€+ë‚ ì”¨ê°€+[MASK]) ì—ì„œ ì•„ë˜ì™€ ê°™ì´ í…ŒìŠ¤íŠ¸ í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n", "\n", "![ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ \"ì¢‹ë„¤ìš”\", KcBERT-Base](https://raw.githubusercontent.com/Beomi/KcBERT/master/img/image-20200719205919389.5670d6.png)\n", "\n", "ë¬¼ë¡  [kcbert-large ëª¨ë¸](https://huggingface.co/beomi/kcbert-large?text=ì˜¤ëŠ˜ì€+ë‚ ì”¨ê°€+[MASK]) ì—ì„œë„ í…ŒìŠ¤íŠ¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n", "\n", "![image-20200806160624340](https://raw.githubusercontent.com/Beomi/KcBERT/master/img/image-20200806160624340.58f9be.png)\n", "\n", "\n", "\n", "### NSMC Binary Classification\n", "\n", "[ë„¤ì´ë²„ ì˜í™”í‰ ì½”í¼ìŠ¤](https://github.com/e9t/nsmc) ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ Fine Tuningì„ ì§„í–‰í•´ ì„±ëŠ¥ì„ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸í•´ë³´ì•˜ìŠµë‹ˆë‹¤.\n", "\n", "Base Modelì„ Fine Tuneí•˜ëŠ” ì½”ë“œëŠ” <a href=\"https://colab.research.google.com/drive/1fn4sVJ82BrrInjq6y5655CYPP-1UKCLb?usp=sharing\">\n", "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n", "</a> ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n", "\n", "Large Modelì„ Fine Tuneí•˜ëŠ” ì½”ë“œëŠ” <a href=\"https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing\">\n", "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n", "</a> ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n", "\n", "- GPUëŠ” P100 x1ëŒ€ ê¸°ì¤€ 1epochì— 2-3ì‹œê°„, TPUëŠ” 1epochì— 1ì‹œê°„ ë‚´ë¡œ ì†Œìš”ë©ë‹ˆë‹¤.\n", "- GPU RTX Titan x4ëŒ€ ê¸°ì¤€ 30ë¶„/epoch ì†Œìš”ë©ë‹ˆë‹¤.\n", "- ì˜ˆì‹œ ì½”ë“œëŠ” [pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning)ìœ¼ë¡œ ê°œë°œí–ˆìŠµë‹ˆë‹¤.\n", "\n", "#### ì‹¤í—˜ê²°ê³¼\n", "\n", "- KcBERT-Base Model ì‹¤í—˜ê²°ê³¼: Val acc `.8905`\n", "\n", "![KcBERT Base finetune on NSMC](https://raw.githubusercontent.com/Beomi/KcBERT/master/img/image-20200719201102895.ddbdfc.png)\n", "\n", "- KcBERT-Large Model ì‹¤í—˜ ê²°ê³¼: Val acc `.9089`\n", "\n", "![image-20200806190242834](https://raw.githubusercontent.com/Beomi/KcBERT/master/img/image-20200806190242834.56d6ee.png)\n", "\n", "> ë” ë‹¤ì–‘í•œ Downstream Taskì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ê³  ê³µê°œí•  ì˜ˆì •ì…ë‹ˆë‹¤.\n", "\n", "## ì¸ìš©í‘œê¸°/Citation\n", "\n", "KcBERTë¥¼ ì¸ìš©í•˜ì‹¤ ë•ŒëŠ” ì•„ë˜ ì–‘ì‹ì„ í†µí•´ ì¸ìš©í•´ì£¼ì„¸ìš”.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["@inproceedings{lee2020kcbert,\n", "title={KcBERT: Korean Comments BERT},\n", "author={Lee, Junbum},\n", "booktitle={Proceedings of the 32nd Annual Conference on Human and Cognitive Language Technology},\n", "pages={437--440},\n", "year={2020}\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- ë…¼ë¬¸ì§‘ ë‹¤ìš´ë¡œë“œ ë§í¬: http://hclt.kr/dwn/?v=bG5iOmNvbmZlcmVuY2U7aWR4OjMy (*í˜¹ì€ http://hclt.kr/symp/?lnb=conference )\n", "\n", "## Acknowledgement\n", "\n", "KcBERT Modelì„ í•™ìŠµí•˜ëŠ” GCP/TPU í™˜ê²½ì€ [TFRC](https://www.tensorflow.org/tfrc?hl=ko) í”„ë¡œê·¸ë¨ì˜ ì§€ì›ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.\n", "\n", "ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ë§ì€ ì¡°ì–¸ì„ ì£¼ì‹  [Monologg](https://github.com/monologg/) ë‹˜ ê°ì‚¬í•©ë‹ˆë‹¤ :)\n", "\n", "## Reference\n", "\n", "### Github Repos\n", "\n", "- [BERT by Google](https://github.com/google-research/bert)\n", "- [KoBERT by SKT](https://github.com/SKTBrain/KoBERT)\n", "- [KoELECTRA by Monologg](https://github.com/monologg/KoELECTRA/)\n", "\n", "- [Transformers by Huggingface](https://github.com/huggingface/transformers)\n", "- [Tokenizers by Hugginface](https://github.com/huggingface/tokenizers)\n", "\n", "### Papers\n", "\n", "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n", "\n", "### Blogs\n", "\n", "- [Monologgë‹˜ì˜ KoELECTRA í•™ìŠµê¸°](https://monologg.kr/categories/NLP/ELECTRA/)\n", "- [Colabì—ì„œ TPUë¡œ BERT ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ê¸° - Tensorflow/Google ver.](https://beomi.github.io/2020/02/26/Train-BERT-from-scratch-on-colab-TPU-Tensorflow-ver/)\n", "\n", "> æ­¤æ¨¡å‹æ¥æºäºï¼š[https://huggingface.co/beomi/kcbert-base](https://huggingface.co/https://huggingface.co/beomi/kcbert-base)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [beomi/kcbert-base](https://huggingface.co/beomi/kcbert-base)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}