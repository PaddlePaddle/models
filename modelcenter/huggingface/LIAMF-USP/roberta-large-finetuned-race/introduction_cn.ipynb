{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Roberta Large Fine Tuned on RACE\n", "\n", "## Model description\n", "\n", "This model is a fine-tuned model of Roberta-large applied on RACE\n", "\n", "#### How to use\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import datasets\n", "from transformers import RobertaTokenizer\n", "from transformers import  RobertaForMultipleChoice\n", "\n", "tokenizer = RobertaTokenizer.from_pretrained(\n", "\"LIAMF-USP/roberta-large-finetuned-race\")\n", "model = RobertaForMultipleChoice.from_pretrained(\n", "\"LIAMF-USP/roberta-large-finetuned-race\")\n", "dataset = datasets.load_dataset(\n", "\"race\",\n", "\"all\",\n", "split=[\"train\", \"validation\", \"test\"],\n", ")training_examples = dataset[0]\n", "evaluation_examples = dataset[1]\n", "test_examples = dataset[2]\n", "\n", "example=training_examples[0]\n", "example_id = example[\"example_id\"]\n", "question = example[\"question\"]\n", "context = example[\"article\"]\n", "options = example[\"options\"]\n", "label_example = example[\"answer\"]\n", "label_map = {label: i\n", "for i, label in enumerate([\"A\", \"B\", \"C\", \"D\"])}\n", "choices_inputs = []\n", "for ending_idx, (_, ending) in enumerate(\n", "zip(context, options)):\n", "if question.find(\"_\") != -1:\n", "# fill in the banks questions\n", "question_option = question.replace(\"_\", ending)\n", "else:\n", "question_option = question + \" \" + ending\n", "inputs = tokenizer(\n", "context,\n", "question_option,\n", "add_special_tokens=True,\n", "max_length=MAX_SEQ_LENGTH,\n", "padding=\"max_length\",\n", "truncation=True,\n", "return_overflowing_tokens=False,\n", ")\n", "label = label_map[label_example]\n", "input_ids = [x[\"input_ids\"] for x in choices_inputs]\n", "attention_mask = (\n", "[x[\"attention_mask\"] for x in choices_inputs]\n", "# as the senteces follow the same structure,\n", "#just one of them is necessary to check\n", "if \"attention_mask\" in choices_inputs[0]\n", "else None\n", ")\n", "example_encoded = {\n", "\"example_id\": example_id,\n", "\"input_ids\": input_ids,\n", "\"attention_mask\": attention_mask,\n", "\"label\": label,\n", "}\n", "output = model(**example_encoded)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## Training data\n", "\n", "The initial model was [roberta large model](https://huggingface.co/roberta-large) which was then fine-tuned on [RACE dataset](https://www.cs.cmu.edu/~glai1/data/race/)\n", "\n", "## Training procedure\n", "\n", "It was necessary to preprocess the data with a method that is exemplified for a single instance in the _How to use_ section. The used hyperparameters were the following:\n", "\n", "| Hyperparameter | Value |\n", "|:----:|:----:|\n", "| adam_beta1                  | 0.9      |\n", "| adam_beta2                  | 0.98     |\n", "| adam_epsilon                | 1.000e-8 |\n", "| eval_batch_size             | 32       |\n", "| train_batch_size            | 1        |\n", "| fp16                        | True     |\n", "| gradient_accumulation_steps | 16       |\n", "| learning_rate               | 0.00001  |\n", "| warmup_steps                | 1000     |\n", "| max_length                  | 512      |\n", "| epochs                      | 4        |\n", "\n", "## Eval results:\n", "| Dataset Acc | Eval | All Test |High School Test |Middle School Test |\n", "|:----:|:----:|:----:|:----:|:----:|\n", "|      | 85.2 | 84.9|83.5|88.0|\n", "\n", "**The model was trained with a Tesla V100-PCIE-16GB**\n", "> 此模型来源于：[https://huggingface.co/LIAMF-USP/roberta-large-finetuned-race](https://huggingface.co/https://huggingface.co/LIAMF-USP/roberta-large-finetuned-race)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [LIAMF-USP/roberta-large-finetuned-race](https://huggingface.co/LIAMF-USP/roberta-large-finetuned-race)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}