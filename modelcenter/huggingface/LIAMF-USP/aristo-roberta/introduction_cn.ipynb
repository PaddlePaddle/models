{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Roberta Large Fine Tuned on RACE\n", "\n", "## Model description\n", "\n", "This model follows the implementation by Allen AI team about [Aristo Roberta V7 Model](https://leaderboard.allenai.org/arc/submission/blcotvl7rrltlue6bsv0) given in [ARC Challenge](https://leaderboard.allenai.org/arc/submissions/public)\n", "\n", "#### How to use\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import datasets\n", "from transformers import RobertaTokenizer\n", "from transformers import  RobertaForMultipleChoice\n", "\n", "tokenizer = RobertaTokenizer.from_pretrained(\n", "\"LIAMF-USP/aristo-roberta\")\n", "model = RobertaForMultipleChoice.from_pretrained(\n", "\"LIAMF-USP/aristo-roberta\")\n", "dataset = datasets.load_dataset(\n", "\"arc\",,\n", "split=[\"train\", \"validation\", \"test\"],\n", ")\n", "training_examples = dataset[0]\n", "evaluation_examples = dataset[1]\n", "test_examples = dataset[2]\n", "\n", "example=training_examples[0]\n", "example_id = example[\"example_id\"]\n", "question = example[\"question\"]\n", "label_example = example[\"answer\"]\n", "options = example[\"options\"]\n", "if label_example in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n", "label_map = {label: i for i, label in enumerate(\n", "[\"A\", \"B\", \"C\", \"D\", \"E\"])}\n", "elif label_example in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n", "label_map = {label: i for i, label in enumerate(\n", "[\"1\", \"2\", \"3\", \"4\", \"5\"])}\n", "else:\n", "print(f\"{label_example} not found\")\n", "while len(options) < 5:\n", "empty_option = {}\n", "empty_option['option_context'] = ''\n", "empty_option['option_text'] = ''\n", "options.append(empty_option)\n", "choices_inputs = []\n", "for ending_idx, option in enumerate(options):\n", "ending = option[\"option_text\"]\n", "context = option[\"option_context\"]\n", "if question.find(\"_\") != -1:\n", "# fill in the banks questions\n", "question_option = question.replace(\"_\", ending)\n", "else:\n", "question_option = question + \" \" + ending\n", "\n", "inputs = tokenizer(\n", "context,\n", "question_option,\n", "add_special_tokens=True,\n", "max_length=MAX_SEQ_LENGTH,\n", "padding=\"max_length\",\n", "truncation=True,\n", "return_overflowing_tokens=False,\n", ")\n", "\n", "if \"num_truncated_tokens\" in inputs and inputs[\"num_truncated_tokens\"] > 0:\n", "logging.warning(f\"Question: {example_id} with option {ending_idx} was truncated\")\n", "choices_inputs.append(inputs)\n", "label = label_map[label_example]\n", "input_ids = [x[\"input_ids\"] for x in choices_inputs]\n", "attention_mask = (\n", "[x[\"attention_mask\"] for x in choices_inputs]\n", "# as the senteces follow the same structure, just one of them is\n", "# necessary to check\n", "if \"attention_mask\" in choices_inputs[0]\n", "else None\n", ")\n", "example_encoded = {\n", "\"example_id\": example_id,\n", "\"input_ids\": input_ids,\n", "\"attention_mask\": attention_mask,\n", "\"token_type_ids\": token_type_ids,\n", "\"label\": label\n", "\n", "}\n", "output = model(**example_encoded)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## Training data\n", "\n", "the Training data was the same as proposed [here](https://leaderboard.allenai.org/arc/submission/blcotvl7rrltlue6bsv0)\n", "\n", "The only diferrence was the hypeparameters of RACE fine tuned model, which were reported [here](https://huggingface.co/LIAMF-USP/roberta-large-finetuned-race#eval-results)\n", "\n", "## Training procedure\n", "\n", "It was necessary to preprocess the data with a method that is exemplified for a single instance in the _How to use_ section. The used hyperparameters were the following:\n", "\n", "| Hyperparameter | Value |\n", "|:----:|:----:|\n", "| adam_beta1                  | 0.9      |\n", "| adam_beta2                  | 0.98     |\n", "| adam_epsilon                | 1.000e-8 |\n", "| eval_batch_size             | 16       |\n", "| train_batch_size            | 4        |\n", "| fp16                        | True     |\n", "| gradient_accumulation_steps | 4       |\n", "| learning_rate               | 0.00001  |\n", "| warmup_steps                | 0.06     |\n", "| max_length                  | 256      |\n", "| epochs                  |   4      |\n", "\n", "The other parameters were the default ones from [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) and [Trainer Arguments](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments)\n", "\n", "## Eval results:\n", "| Dataset Acc | Challenge Test |\n", "|:----:|:----:|\n", "|      | 65.358 |\n", "\n", "**The model was trained with a TITAN RTX**\n", "> 此模型来源于：[https://huggingface.co/LIAMF-USP/aristo-roberta](https://huggingface.co/https://huggingface.co/LIAMF-USP/aristo-roberta)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [LIAMF-USP/aristo-roberta](https://huggingface.co/LIAMF-USP/aristo-roberta)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}