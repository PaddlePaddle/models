{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# DeCLUTR-base\n", "\n", "## Model description\n", "\n", "The \"DeCLUTR-base\" model from our paper: [DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations](https://arxiv.org/abs/2006.03659).\n", "\n", "## Intended uses & limitations\n", "\n", "The model is intended to be used as a universal sentence encoder, similar to [Google's Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4) or [Sentence Transformers](https://github.com/UKPLab/sentence-transformers).\n", "\n", "#### How to use\n", "\n", "Please see [our repo](https://github.com/JohnGiorgi/DeCLUTR) for full details. A simple example is shown below.\n", "\n", "##### With [SentenceTransformers](https://www.sbert.net/)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from scipy.spatial.distance import cosine\n", "from sentence_transformers import SentenceTransformer\n", "\n", "# Load the model\n", "model = SentenceTransformer(\"johngiorgi/declutr-base\")\n", "\n", "# Prepare some text to embed\n", "texts = [\n", "\"A smiling costumed woman is holding an umbrella.\",\n", "\"A happy woman in a fairy costume holds an umbrella.\",\n", "]\n", "\n", "# Embed the text\n", "embeddings = model.encode(texts)\n", "\n", "# Compute a semantic similarity via the cosine distance\n", "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### With ðŸ¤— Transformers\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import torch\n", "from scipy.spatial.distance import cosine\n", "from transformers import AutoModel, AutoTokenizer\n", "\n", "# Load the model\n", "tokenizer = AutoTokenizer.from_pretrained(\"johngiorgi/declutr-base\")\n", "model = AutoModel.from_pretrained(\"johngiorgi/declutr-base\")\n", "\n", "# Prepare some text to embed\n", "text = [\n", "\"A smiling costumed woman is holding an umbrella.\",\n", "\"A happy woman in a fairy costume holds an umbrella.\",\n", "]\n", "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n", "\n", "# Embed the text\n", "with torch.no_grad():\n", "sequence_output = model(**inputs)[0]\n", "\n", "# Mean pool the token-level embeddings to get sentence-level embeddings\n", "embeddings = torch.sum(\n", "sequence_output * inputs[\"attention_mask\"].unsqueeze(-1), dim=1\n", ") / torch.clamp(torch.sum(inputs[\"attention_mask\"], dim=1, keepdims=True), min=1e-9)\n", "\n", "# Compute a semantic similarity via the cosine distance\n", "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### BibTeX entry and citation info\n"]}, {"cell_type": "code", "metadata": {}, "source": ["@inproceedings{giorgi-etal-2021-declutr,\n", "title        = {{D}e{CLUTR}: Deep Contrastive Learning for Unsupervised Textual Representations},\n", "author       = {Giorgi, John  and Nitski, Osvald  and Wang, Bo  and Bader, Gary},\n", "year         = 2021,\n", "month        = aug,\n", "booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n", "publisher    = {Association for Computational Linguistics},\n", "address      = {Online},\n", "pages        = {879--895},\n", "doi          = {10.18653/v1/2021.acl-long.72},\n", "url          = {https://aclanthology.org/2021.acl-long.72}\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> æ­¤æ¨¡åž‹æ¥æºäºŽï¼š[https://huggingface.co/johngiorgi/declutr-base](https://huggingface.co/https://huggingface.co/johngiorgi/declutr-base)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [johngiorgi/declutr-base](https://huggingface.co/johngiorgi/declutr-base)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}