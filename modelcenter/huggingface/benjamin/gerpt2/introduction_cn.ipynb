{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# GerPT2\n", "\n", "German large and small versions of GPT2:\n", "\n", "- https://huggingface.co/benjamin/gerpt2\n", "- https://huggingface.co/benjamin/gerpt2-large\n", "\n", "See the [GPT2 model card](https://huggingface.co/gpt2) for considerations on limitations and bias. See the [GPT2 documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for details on GPT2.\n", "\n", "## Comparison to [dbmdz/german-gpt2](https://huggingface.co/dbmdz/german-gpt2)\n", "\n", "I evaluated both GerPT2-large and the other German GPT2, [dbmdz/german-gpt2](https://huggingface.co/dbmdz/german-gpt2) on the [CC-100](http://data.statmt.org/cc-100/) dataset and on the German Wikipedia:\n", "\n", "|                   | CC-100 (PPL) | Wikipedia (PPL) |\n", "|-------------------|--------------|-----------------|\n", "| dbmdz/german-gpt2 | 49.47        | 62.92           |\n", "| GerPT2            | 24.78        | 35.33           |\n", "| GerPT2-large      | __16.08__    | __23.26__       |\n", "|                   |              |                 |\n", "\n", "See the script `evaluate.py` in the [GerPT2 Github repository](https://github.com/bminixhofer/gerpt2) for the code.\n", "\n", "## Usage\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/gerpt2-large\")\n", "model = AutoModelForCausalLM.from_pretrained(\"benjamin/gerpt2-large\")\n", "\n", "prompt = \"<your prompt>\"\n", "\n", "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n", "print(pipe(prompt)[0][\"generated_text\"])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Also, two tricks might improve the generated text:\n"]}, {"cell_type": "code", "metadata": {}, "source": ["output = model.generate(\n", "# during training an EOS token was used to mark the beginning of each text\n", "# so it can help to insert it at the start\n", "torch.tensor(\n", "[tokenizer.eos_token_id] + tokenizer.encode(prompt)\n", ").unsqueeze(0),\n", "do_sample=True,\n", "# try setting bad_words_ids=[[0]] to disallow generating an EOS token, without this the model is\n", "# prone to ending generation early because a significant number of texts from the training corpus\n", "# is quite short\n", "bad_words_ids=[[0]],\n", "max_length=max_length,\n", ")[0]\n", "print(tokenizer.decode(output))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training details\n", "\n", "GerPT2-large is trained on the entire German data from the [CC-100 Corpus](http://data.statmt.org/cc-100/) and weights were initialized from the [English GPT2 model](https://huggingface.co/gpt2-large).\n", "GerPT2-large was trained with:\n", "\n", "- a batch size of 256\n", "- using OneCycle learning rate with a maximum of 5e-3\n", "- with AdamW with a weight decay of 0.01\n", "- for 2 epochs\n", "\n", "Training took roughly 12 days on 8 TPUv3 cores.\n", "\n", "To train GerPT2-large, follow these steps. Scripts are located in the [Github repository](https://github.com/bminixhofer/gerpt2):\n", "\n", "0. Download and unzip training data from http://data.statmt.org/cc-100/.\n", "1. Train a tokenizer using `prepare/train_tokenizer.py`. As training data for the tokenizer I used a random subset of 5% of the CC-100 data.\n", "2. (optionally) generate a German input embedding matrix with `prepare/generate_aligned_wte.py`. This uses a neat trick to semantically map tokens from the English tokenizer to tokens from the German tokenizer using aligned word embeddings. E. g.:\n"]}, {"cell_type": "code", "metadata": {}, "source": ["ĠMinde -> Ġleast\n", "Ġjed -> Ġwhatsoever\n", "flughafen -> Air\n", "vermittlung -> employment\n", "teilung -> ignment\n", "ĠInterpretation -> Ġinterpretation\n", "Ġimport -> Ġimported\n", "hansa -> irl\n", "genehmigungen -> exempt\n", "ĠAuflist -> Ġlists\n", "Ġverschwunden -> Ġdisappeared\n", "ĠFlyers -> ĠFlyers\n", "Kanal -> Channel\n", "Ġlehr -> Ġteachers\n", "Ġnahelie -> Ġconvenient\n", "gener -> Generally\n", "mitarbeiter -> staff\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This helps a lot on a trial run I did, although I wasn't able to do a full comparison due to budget and time constraints. To use this WTE matrix it can be passed via the `wte_path` to the training script. Credit to [this blogpost](https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787) for the idea of initializing GPT2 from English weights.\n", "\n", "3. Tokenize the corpus using `prepare/tokenize_text.py`. This generates files for train and validation tokens in JSON Lines format.\n", "4. Run the training script `train.py`! `run.sh` shows how this was executed for the full run with config `configs/tpu_large.json`.\n", "\n", "## License\n", "\n", "GerPT2 is licensed under the MIT License.\n", "\n", "## Citing\n", "\n", "Please cite GerPT2 as follows:\n"]}, {"cell_type": "code", "metadata": {}, "source": ["@misc{Minixhofer_GerPT2_German_large_2020,\n", "author = {Minixhofer, Benjamin},\n", "doi = {10.5281/zenodo.5509984},\n", "month = {12},\n", "title = {{GerPT2: German large and small versions of GPT2}},\n", "url = {https://github.com/bminixhofer/gerpt2},\n", "year = {2020}\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Acknowledgements\n", "\n", "Thanks to [Hugging Face](https://huggingface.co) for awesome tools and infrastructure.\n", "Huge thanks to [Artus Krohn-Grimberghe](https://twitter.com/artuskg) at [LYTiQ](https://www.lytiq.de/) for making this possible by sponsoring the resources used for training.\n", "> 此模型来源于：[https://huggingface.co/benjamin/gerpt2](https://huggingface.co/https://huggingface.co/benjamin/gerpt2)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [benjamin/gerpt2](https://huggingface.co/benjamin/gerpt2)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}