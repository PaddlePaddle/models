{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# bert-base-romanian-cased-v1\n", "\n", "The BERT **base**, **cased** model for Romanian, trained on a 15GB corpus, version ![v1.0](https://img.shields.io/badge/v1.0-21%20Apr%202020-ff6666)\n", "\n", "### How to use\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import AutoTokenizer, AutoModel\n", "import torch\n", "# load tokenizer and model\n", "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n", "model = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n", "# tokenize a sentence and run through the model\n", "input_ids = torch.tensor(tokenizer.encode(\"Acesta este un test.\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n", "outputs = model(input_ids)\n", "# get encoding\n", "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remember to always sanitize your text! Replace ``s`` and ``t`` cedilla-letters to comma-letters with :\n"]}, {"cell_type": "code", "metadata": {}, "source": ["text = text.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\").replace(\"Ţ\", \"Ț\").replace(\"Ş\", \"Ș\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["because the model was **NOT** trained on cedilla ``s`` and ``t``s. If you don't, you will have decreased performance due to ``<UNK>``s and increased number of tokens per word.\n", "\n", "### Evaluation\n", "\n", "Evaluation is performed on Universal Dependencies [Romanian RRT](https://universaldependencies.org/treebanks/ro_rrt/index.html) UPOS, XPOS and LAS, and on a NER task based on [RONEC](https://github.com/dumitrescustefan/ronec). Details, as well as more in-depth tests not shown here, are given in the dedicated [evaluation page](https://github.com/dumitrescustefan/Romanian-Transformers/tree/master/evaluation/README.md).\n", "\n", "The baseline is the [Multilingual BERT](https://github.com/google-research/bert/blob/master/multilingual.md) model ``bert-base-multilingual-(un)cased``, as at the time of writing it was the only available BERT model that works on Romanian.\n", "\n", "| Model                          |  UPOS |  XPOS  |  NER  |  LAS  |\n", "|--------------------------------|:-----:|:------:|:-----:|:-----:|\n", "| bert-base-multilingual-cased   | 97.87 |  96.16 | 84.13 | 88.04 |\n", "| bert-base-romanian-cased-v1    | **98.00** |  **96.46** | **85.88** | **89.69** |\n", "\n", "### Corpus\n", "\n", "The model is trained on the following corpora (stats in the table below are after cleaning):\n", "\n", "| Corpus    \t| Lines(M) \t| Words(M) \t| Chars(B) \t| Size(GB) \t|\n", "|-----------|:--------:|:--------:|:--------:|:--------:|\n", "| OPUS      \t|   55.05  \t|  635.04  \t|   4.045  \t|    3.8   \t|\n", "| OSCAR     \t|   33.56  \t|  1725.82 \t|  11.411  \t|    11    \t|\n", "| Wikipedia \t|   1.54   \t|   60.47  \t|   0.411  \t|    0.4   \t|\n", "| **Total**     \t|   **90.15**  \t|  **2421.33** \t|  **15.867**  \t|   **15.2**   \t|\n", "\n", "### Citation\n", "\n", "If you use this model in a research paper, I'd kindly ask you to cite the following paper:\n"]}, {"cell_type": "code", "metadata": {}, "source": ["Stefan Dumitrescu, Andrei-Marius Avram, and Sampo Pyysalo. 2020. The birth of Romanian BERT. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4324–4328, Online. Association for Computational Linguistics.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["or, in bibtex:\n"]}, {"cell_type": "code", "metadata": {}, "source": ["@inproceedings{dumitrescu-etal-2020-birth,\n", "title = \"The birth of {R}omanian {BERT}\",\n", "author = \"Dumitrescu, Stefan  and\n", "Avram, Andrei-Marius  and\n", "Pyysalo, Sampo\",\n", "booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n", "month = nov,\n", "year = \"2020\",\n", "address = \"Online\",\n", "publisher = \"Association for Computational Linguistics\",\n", "url = \"https://aclanthology.org/2020.findings-emnlp.387\",\n", "doi = \"10.18653/v1/2020.findings-emnlp.387\",\n", "pages = \"4324--4328\",\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Acknowledgements\n", "\n", "- We'd like to thank [Sampo Pyysalo](https://github.com/spyysalo) from TurkuNLP for helping us out with the compute needed to pretrain the v1.0 BERT models. He's awesome!\n", "> 此模型来源于：[https://huggingface.co/dumitrescustefan/bert-base-romanian-cased-v1](https://huggingface.co/https://huggingface.co/dumitrescustefan/bert-base-romanian-cased-v1)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [dumitrescustefan/bert-base-romanian-cased-v1](https://huggingface.co/dumitrescustefan/bert-base-romanian-cased-v1)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}