{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# LASSL roberta-ko-small\n", "## How to use\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import AutoModel, AutoTokenizer\n", "model = AutoModel.from_pretrained(\"lassl/roberta-ko-small\")\n", "tokenizer = AutoTokenizer.from_pretrained(\"lassl/roberta-ko-small\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Evaluation\n", "Pretrained `roberta-ko-small` on korean language was trained by [LASSL](https://github.com/lassl/lassl) framework. Below performance was evaluated at 2021/12/15.\n", "\n", "| nsmc | klue_nli | klue_sts | korquadv1 | klue_mrc | avg |\n", "| ---- | -------- | -------- | --------- | ---- | -------- |\n", "| 87.8846 | 66.3086 | 83.8353 | 83.1780 | 42.4585 | 72.7330 |\n", "\n", "## Corpora\n", "This model was trained from 6,860,062 examples (whose have 3,512,351,744 tokens). 6,860,062 examples are extracted from below corpora. If you want to get information for training, you should see `config.json`.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["corpora/\n", "├── [707M]  kowiki_latest.txt\n", "├── [ 26M]  modu_dialogue_v1.2.txt\n", "├── [1.3G]  modu_news_v1.1.txt\n", "├── [9.7G]  modu_news_v2.0.txt\n", "├── [ 15M]  modu_np_v1.1.txt\n", "├── [1008M]  modu_spoken_v1.2.txt\n", "├── [6.5G]  modu_written_v1.0.txt\n", "└── [413M]  petition.txt\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "> 此模型来源于：[https://huggingface.co/lassl/roberta-ko-small](https://huggingface.co/https://huggingface.co/lassl/roberta-ko-small)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> this model is from [lassl/roberta-ko-small](https://huggingface.co/lassl/roberta-ko-small)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}