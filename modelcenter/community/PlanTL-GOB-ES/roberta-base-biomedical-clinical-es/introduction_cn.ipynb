{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f16b2b",
   "metadata": {},
   "source": [
    "# Biomedical-clinical language model for Spanish\n",
    "Biomedical pretrained language model for Spanish. For more details about the corpus, the pretraining and the evaluation, check the official [repository](https://github.com/PlanTL-SANIDAD/lm-biomedical-clinical-es) and read our [preprint](https://arxiv.org/abs/2109.03570) \"_Carrino, C. P., Armengol-Estapé, J., Gutiérrez-Fandiño, A., Llop-Palao, J., Pàmies, M., Gonzalez-Agirre, A., & Villegas, M. (2021). Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario._\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d83755",
   "metadata": {},
   "source": [
    "## Tokenization and model pretraining\n",
    "This model is a [RoBERTa-based](https://github.com/pytorch/fairseq/tree/master/examples/roberta) model trained on a\n",
    "**biomedical-clinical** corpus in Spanish collected from several sources (see next section).\n",
    "The training corpus has been tokenized using a byte version of [Byte-Pair Encoding (BPE)](https://github.com/openai/gpt-2)\n",
    "used in the original [RoBERTA](https://github.com/pytorch/fairseq/tree/master/examples/roberta) model with a vocabulary size of 52,000 tokens. The pretraining consists of a masked language model training at the subword level following the approach employed for the RoBERTa base model with the same hyperparameters as in the original work. The training lasted a total of 48 hours with 16 NVIDIA V100 GPUs of 16GB DDRAM, using Adam optimizer with a peak learning rate of 0.0005 and an effective batch size of 2,048 sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63bf3ef",
   "metadata": {},
   "source": [
    "## Training corpora and preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2374a9b",
   "metadata": {},
   "source": [
    "The training corpus is composed of several biomedical corpora in Spanish, collected from publicly available corpora and crawlers, and a real-world clinical corpus collected from more than 278K clinical documents and notes. To obtain a high-quality training corpus while retaining the idiosyncrasies of the clinical language, a cleaning pipeline has been applied only to the biomedical corpora, keeping the clinical corpus uncleaned. Essentially, the cleaning operations used are:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a512a",
   "metadata": {},
   "source": [
    "- data parsing in different formats\n",
    "- sentence splitting\n",
    "- language detection\n",
    "- filtering of ill-formed sentences\n",
    "- deduplication of repetitive contents\n",
    "- keep the original document boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a90cf",
   "metadata": {},
   "source": [
    "Then, the biomedical corpora are concatenated and further global deduplication among the biomedical corpora have been applied.\n",
    "Eventually, the clinical corpus is concatenated to the cleaned biomedical corpus resulting in a medium-size biomedical-clinical corpus for Spanish composed of more than 1B tokens. The table below shows some basic statistics of the individual cleaned corpora:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f74ae",
   "metadata": {},
   "source": [
    "| Name                                                                                    | No. tokens  | Description                                                                                                                                                                                                                                          |\n",
    "|-----------------------------------------------------------------------------------------|-------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [Medical crawler](https://zenodo.org/record/4561970)                                    | 745,705,946 | Crawler of more than 3,000 URLs belonging to Spanish biomedical and health domains.                                                                                                                                                                                 |\n",
    "| Clinical cases misc.                                                                    | 102,855,267 | A miscellany of medical content, essentially clinical cases. Note that a clinical case report is a scientific publication where medical practitioners share patient cases and it is different from a clinical note or document.                                                                                                                                                                                 |\n",
    "| Clinical notes/documents                                                                | 91,250,080 | Collection of more than 278K clinical documents, including discharge reports, clinical course notes and X-ray reports, for a total of 91M tokens.                                                                                                                                                                                 |\n",
    "| [Scielo](https://github.com/PlanTL-SANIDAD/SciELO-Spain-Crawler)                        | 60,007,289  | Publications written in Spanish crawled from the Spanish SciELO server in 2017.                                                                                                                                       |\n",
    "| [BARR2_background](https://temu.bsc.es/BARR2/downloads/background_set.raw_text.tar.bz2) | 24,516,442  | Biomedical Abbreviation Recognition and Resolution (BARR2) containing Spanish clinical case study sections from a variety of clinical disciplines.                                                                                       |\n",
    "| Wikipedia_life_sciences                                                                 | 13,890,501  | Wikipedia articles crawled 04/01/2021 with the [Wikipedia API python library](https://pypi.org/project/Wikipedia-API/) starting from the \"Ciencias\\_de\\_la\\_vida\" category up to a maximum of 5 subcategories. Multiple links to the same articles are then discarded to avoid repeating content.                                                                                                                                                                    |\n",
    "| Patents                                                                                 | 13,463,387  | Google Patent in Medical Domain for Spain (Spanish). The accepted codes (Medical Domain) for Json files of patents are: \"A61B\", \"A61C\",\"A61F\", \"A61H\", \"A61K\", \"A61L\",\"A61M\", \"A61B\", \"A61P\".                                                        |\n",
    "| [EMEA](http://opus.nlpl.eu/download.php?f=EMEA/v3/moses/en-es.txt.zip)                  | 5,377,448   | Spanish-side documents extracted from parallel corpora made out of PDF documents from the European Medicines Agency.                                                                                                                            |\n",
    "| [mespen_Medline](https://zenodo.org/record/3562536#.YTt1fH2xXbR)                        | 4,166,077   | Spanish-side articles extracted from a collection of Spanish-English parallel corpus consisting of biomedical scientific literature.  The collection of parallel resources are aggregated from the MedlinePlus source. |\n",
    "| PubMed                                                                                  | 1,858,966   | Open-access articles from the PubMed repository crawled in 2017.                                                                                                                                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc8f5d",
   "metadata": {},
   "source": [
    "## Evaluation and results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ede8a8",
   "metadata": {},
   "source": [
    "The model has been evaluated on the Named Entity Recognition (NER) using the following datasets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66423a0d",
   "metadata": {},
   "source": [
    "- [PharmaCoNER](https://zenodo.org/record/4270158): is a track on chemical and drug mention recognition from Spanish medical texts (for more info see: https://temu.bsc.es/pharmaconer/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3338a6",
   "metadata": {},
   "source": [
    "- [CANTEMIST](https://zenodo.org/record/3978041#.YTt5qH2xXbQ): is a shared task specifically focusing on named entity recognition of tumor morphology, in Spanish (for more info see: https://zenodo.org/record/3978041#.YTt5qH2xXbQ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fe07f",
   "metadata": {},
   "source": [
    "- ICTUSnet: consists of 1,006 hospital discharge reports of patients admitted for stroke from 18 different Spanish hospitals. It contains more than 79,000 annotations for 51 different kinds of variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2df73e",
   "metadata": {},
   "source": [
    "The evaluation results are compared against the [mBERT](https://huggingface.co/bert-base-multilingual-cased) and [BETO](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3355a",
   "metadata": {},
   "source": [
    "| F1 - Precision - Recall | roberta-base-biomedical-clinical-es | mBERT                   | BETO                    |\n",
    "|---------------------------|----------------------------|-------------------------------|-------------------------|\n",
    "| PharmaCoNER               | **90.04** - **88.92** - **91.18**    | 87.46 - 86.50 - 88.46 | 88.18 - 87.12 - 89.28 |\n",
    "| CANTEMIST                 | **83.34** - **81.48** - **85.30**    | 82.61 - 81.12 - 84.15 | 82.42 - 80.91 - 84.00 |\n",
    "| ICTUSnet                  | **88.08** - **84.92** - **91.50**    | 86.75 - 83.53 - 90.23 | 85.95 - 83.10 - 89.02 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c53204",
   "metadata": {},
   "source": [
    "## Intended uses & limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a57e5b",
   "metadata": {},
   "source": [
    "The model is ready-to-use only for masked language modelling to perform the Fill Mask task (try the inference API or read the next section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a381f8",
   "metadata": {},
   "source": [
    "However, the is intended to be fine-tuned on downstream tasks such as Named Entity Recognition or Text Classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407889a8",
   "metadata": {},
   "source": [
    "## Cite\n",
    "If you use our models, please cite our latest preprint:\n",
    "\n",
    "```\n",
    "@misc{carrino2021biomedical,\n",
    "      title={Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario}, \n",
    "      author={Casimiro Pio Carrino and Jordi Armengol-Estapé and Asier Gutiérrez-Fandiño and Joan Llop-Palao and Marc Pàmies and Aitor Gonzalez-Agirre and Marta Villegas},\n",
    "      year={2021},\n",
    "      eprint={2109.03570},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL}\n",
    "}\n",
    "```\n",
    "\n",
    "If you use our Medical Crawler corpus, please cite the preprint:\n",
    "\n",
    "\n",
    "```\n",
    "@misc{carrino2021spanish,\n",
    "      title={Spanish Biomedical Crawled Corpus: A Large, Diverse Dataset for Spanish Biomedical Language Models}, \n",
    "      author={Casimiro Pio Carrino and Jordi Armengol-Estapé and Ona de Gibert Bonet and Asier Gutiérrez-Fandiño and Aitor Gonzalez-Agirre and Martin Krallinger and Marta Villegas},\n",
    "      year={2021},\n",
    "      eprint={2109.07765},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ff968",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b558ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"PlanTL-GOB-ES/roberta-base-biomedical-clinical-es\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895509e8",
   "metadata": {},
   "source": [
    "## Copyright\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f55446",
   "metadata": {},
   "source": [
    "Copyright by the Spanish State Secretariat for Digitalization and Artificial Intelligence (SEDIA) (2022)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec85abe",
   "metadata": {},
   "source": [
    "## Licensing information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52359709",
   "metadata": {},
   "source": [
    "[Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233b4e8",
   "metadata": {},
   "source": [
    "## Funding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15679c",
   "metadata": {},
   "source": [
    "This work was funded by the Spanish State Secretariat for Digitalization and Artificial Intelligence (SEDIA) within the framework of the Plan-TL.\n",
    "> 此模型来源于：[https://huggingface.co/PlanTL-GOB-ES/roberta-base-biomedical-clinical-es](https://huggingface.co/PlanTL-GOB-ES/roberta-base-biomedical-clinical-es)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
