{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e875e0cc",
   "metadata": {},
   "source": [
    "# ðŸ¤— + ðŸ“š dbmdz German BERT models\n",
    "\n",
    "In this repository the MDZ Digital Library team (dbmdz) at the Bavarian State\n",
    "Library open sources another German BERT models ðŸŽ‰\n",
    "\n",
    "# German BERT\n",
    "\n",
    "## Stats\n",
    "\n",
    "In addition to the recently released [German BERT](https://deepset.ai/german-bert)\n",
    "model by [deepset](https://deepset.ai/) we provide another German-language model.\n",
    "\n",
    "The source data for the model consists of a recent Wikipedia dump, EU Bookshop corpus,\n",
    "Open Subtitles, CommonCrawl, ParaCrawl and News Crawl. This results in a dataset with\n",
    "a size of 16GB and 2,350,234,427 tokens.\n",
    "\n",
    "For sentence splitting, we use [spacy](https://spacy.io/). Our preprocessing steps\n",
    "(sentence piece model for vocab generation) follow those used for training\n",
    "[SciBERT](https://github.com/allenai/scibert). The model is trained with an initial\n",
    "sequence length of 512 subwords and was performed for 1.5M steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcad967",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c65281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a7a3d",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a0566",
   "metadata": {},
   "source": [
    "For results on downstream tasks like NER or PoS tagging, please refer to\n",
    "[this repository](https://github.com/stefan-it/fine-tuned-berts-seq).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de6719",
   "metadata": {},
   "source": [
    "# Contact (Bugs, Feedback, Contribution and more)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb08b74",
   "metadata": {},
   "source": [
    "For questions about our BERT models just open an issue\n",
    "[here](https://github.com/dbmdz/berts/issues/new) ðŸ¤—\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52feb8",
   "metadata": {},
   "source": [
    "# Acknowledgments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4745160",
   "metadata": {},
   "source": [
    "Research supported with Cloud TPUs from Google's TensorFlow Research Cloud (TFRC).\n",
    "Thanks for providing access to the TFRC â¤ï¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00304a",
   "metadata": {},
   "source": [
    "Thanks to the generous support from the [Hugging Face](https://huggingface.co/) team,\n",
    "it is possible to download both cased and uncased models from their S3 storage ðŸ¤—\n",
    "> The introduciton and weight file of this model are from [https://huggingface.co/dbmdz/bert-base-german-cased](https://huggingface.co/dbmdz/bert-base-german-cased), and we convert them to paddle related files.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
