{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9ff499",
   "metadata": {},
   "source": [
    "# ğŸ¤— + ğŸ“š dbmdz BERT and ELECTRA models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e531a",
   "metadata": {},
   "source": [
    "In this repository the MDZ Digital Library team (dbmdz) at the Bavarian State\n",
    "Library open sources Italian BERT and ELECTRA models ğŸ‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78068",
   "metadata": {},
   "source": [
    "# Italian BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a646c7b",
   "metadata": {},
   "source": [
    "The source data for the Italian BERT model consists of a recent Wikipedia dump and\n",
    "various texts from the [OPUS corpora](http://opus.nlpl.eu/) collection. The final\n",
    "training corpus has a size of 13GB and 2,050,057,573 tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18fda2",
   "metadata": {},
   "source": [
    "For sentence splitting, we use NLTK (faster compared to spacy).\n",
    "Our cased and uncased models are training with an initial sequence length of 512\n",
    "subwords for ~2-3M steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18bb01",
   "metadata": {},
   "source": [
    "For the XXL Italian models, we use the same training data from OPUS and extend\n",
    "it with data from the Italian part of the [OSCAR corpus](https://traces1.inria.fr/oscar/).\n",
    "Thus, the final training corpus has a size of 81GB and 13,138,379,147 tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9be84",
   "metadata": {},
   "source": [
    "Note: Unfortunately, a wrong vocab size was used when training the XXL models.\n",
    "This explains the mismatch of the \"real\" vocab size of 31102, compared to the\n",
    "vocab size specified in `config.json`. However, the model is working and all\n",
    "evaluations were done under those circumstances.\n",
    "See [this issue](https://github.com/dbmdz/berts/issues/7) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38f0cd",
   "metadata": {},
   "source": [
    "The Italian ELECTRA model was trained on the \"XXL\" corpus for 1M steps in total using a batch\n",
    "size of 128. We pretty much following the ELECTRA training procedure as used for\n",
    "[BERTurk](https://github.com/stefan-it/turkish-bert/tree/master/electra).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f1360",
   "metadata": {},
   "source": [
    "## Model weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d5e01e",
   "metadata": {},
   "source": [
    "Currently only PyTorch-[Transformers](https://github.com/huggingface/transformers)\n",
    "compatible weights are available. If you need access to TensorFlow checkpoints,\n",
    "please raise an issue!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be2f8e",
   "metadata": {},
   "source": [
    "| Model                                                | Downloads\n",
    "| ---------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------\n",
    "| `dbmdz/bert-base-italian-cased`                      | [`config.json`](https://cdn.huggingface.co/dbmdz/bert-base-italian-cased/config.json)                                               â€¢ [`pytorch_model.bin`](https://cdn.huggingface.co/dbmdz/bert-base-italian-cased/pytorch_model.bin)                      â€¢ [`vocab.txt`](https://cdn.huggingface.co/dbmdz/bert-base-italian-cased/vocab.txt)\n",
    "| `dbmdz/bert-base-italian-uncased`                    | [`config.json`](https://cdn.huggingface.co/dbmdz/bert-base-italian-uncased/config.json)                                             â€¢ [`pytorch_model.bin`](https://cdn.huggingface.co/dbmdz/bert-base-italian-uncased/pytorch_model.bin)                    â€¢ [`vocab.txt`](https://cdn.huggingface.co/dbmdz/bert-base-italian-uncased/vocab.txt)\n",
    "| `dbmdz/bert-base-italian-xxl-cased`                  | [`config.json`](https://cdn.huggingface.co/dbmdz/bert-base-italian-xxl-cased/config.json)                                           â€¢ [`pytorch_model.bin`](https://cdn.huggingface.co/dbmdz/bert-base-italian-xxl-cased/pytorch_model.bin)                  â€¢ [`vocab.txt`](https://cdn.huggingface.co/dbmdz/bert-base-italian-xxl-cased/vocab.txt)\n",
    "| `dbmdz/bert-base-italian-xxl-uncased`                | [`config.json`](https://cdn.huggingface.co/dbmdz/bert-base-italian-xxl-uncased/config.json)                                         â€¢ [`pytorch_model.bin`](https://cdn.huggingface.co/dbmdz/bert-base-italian-xxl-uncased/pytorch_model.bin)                â€¢ [`vocab.txt`](https://cdn.huggingface.co/dbmdz/bert-base-italian-xxl-uncased/vocab.txt)\n",
    "| `dbmdz/electra-base-italian-xxl-cased-discriminator` | [`config.json`](https://s3.amazonaws.com/models.huggingface.co/bert/dbmdz/electra-base-italian-xxl-cased-discriminator/config.json) â€¢ [`pytorch_model.bin`](https://cdn.huggingface.co/dbmdz/electra-base-italian-xxl-cased-discriminator/pytorch_model.bin) â€¢ [`vocab.txt`](https://cdn.huggingface.co/dbmdz/electra-base-italian-xxl-cased-discriminator/vocab.txt)\n",
    "| `dbmdz/electra-base-italian-xxl-cased-generator`     | [`config.json`](https://s3.amazonaws.com/models.huggingface.co/bert/dbmdz/electra-base-italian-xxl-cased-generator/config.json)     â€¢ [`pytorch_model.bin`](https://cdn.huggingface.co/dbmdz/electra-base-italian-xxl-cased-generator/pytorch_model.bin)     â€¢ [`vocab.txt`](https://cdn.huggingface.co/dbmdz/electra-base-italian-xxl-cased-generator/vocab.txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09ea88",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df105670",
   "metadata": {},
   "source": [
    "For results on downstream tasks like NER or PoS tagging, please refer to\n",
    "[this repository](https://github.com/stefan-it/italian-bertelectra).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032b915",
   "metadata": {},
   "source": [
    "## Usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4fd6a",
   "metadata": {},
   "source": [
    "With Transformers >= 2.3 our Italian BERT models can be loaded like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44796cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"dbmdz/bert-base-italian-uncased\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f79276",
   "metadata": {},
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceeff95",
   "metadata": {},
   "source": [
    "model_name = \"dbmdz/electra-base-italian-xxl-cased-discriminator\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea24c9",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d0d7c",
   "metadata": {},
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bad1f6",
   "metadata": {},
   "source": [
    "# Huggingface model hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e3517",
   "metadata": {},
   "source": [
    "All models are available on the [Huggingface model hub](https://huggingface.co/dbmdz).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7cd3d",
   "metadata": {},
   "source": [
    "# Contact (Bugs, Feedback, Contribution and more)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad6e0d",
   "metadata": {},
   "source": [
    "For questions about our BERT/ELECTRA models just open an issue\n",
    "[here](https://github.com/dbmdz/berts/issues/new) ğŸ¤—\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd57357",
   "metadata": {},
   "source": [
    "# Acknowledgments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958b015",
   "metadata": {},
   "source": [
    "Research supported with Cloud TPUs from Google's TensorFlow Research Cloud (TFRC).\n",
    "Thanks for providing access to the TFRC â¤ï¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf765a",
   "metadata": {},
   "source": [
    "Thanks to the generous support from the [Hugging Face](https://huggingface.co/) team,\n",
    "it is possible to download both cased and uncased models from their S3 storage ğŸ¤—\n",
    "> æ­¤æ¨¡å‹æ¥æºäºï¼š[https://huggingface.co/dbmdz/bert-base-italian-uncased](https://huggingface.co/https://huggingface.co/dbmdz/bert-base-italian-uncased)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
