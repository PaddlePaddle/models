{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0d446c",
   "metadata": {},
   "source": [
    "# ðŸ¤— + ðŸ“š dbmdz German BERT models\n",
    "\n",
    "In this repository the MDZ Digital Library team (dbmdz) at the Bavarian State\n",
    "Library open sources another German BERT models ðŸŽ‰\n",
    "\n",
    "# German BERT\n",
    "\n",
    "## Stats\n",
    "\n",
    "In addition to the recently released [German BERT](https://deepset.ai/german-bert)\n",
    "model by [deepset](https://deepset.ai/) we provide another German-language model.\n",
    "\n",
    "The source data for the model consists of a recent Wikipedia dump, EU Bookshop corpus,\n",
    "Open Subtitles, CommonCrawl, ParaCrawl and News Crawl. This results in a dataset with\n",
    "a size of 16GB and 2,350,234,427 tokens.\n",
    "\n",
    "For sentence splitting, we use [spacy](https://spacy.io/). Our preprocessing steps\n",
    "(sentence piece model for vocab generation) follow those used for training\n",
    "[SciBERT](https://github.com/allenai/scibert). The model is trained with an initial\n",
    "sequence length of 512 subwords and was performed for 1.5M steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524680d5",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39332440",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"dbmdz/bert-base-german-uncased\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a0ad13",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf030ad3",
   "metadata": {},
   "source": [
    "For results on downstream tasks like NER or PoS tagging, please refer to\n",
    "[this repository](https://github.com/stefan-it/fine-tuned-berts-seq).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d159fa5",
   "metadata": {},
   "source": [
    "# Contact (Bugs, Feedback, Contribution and more)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eff4c5",
   "metadata": {},
   "source": [
    "For questions about our BERT models just open an issue\n",
    "[here](https://github.com/dbmdz/berts/issues/new) ðŸ¤—\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb81d709",
   "metadata": {},
   "source": [
    "# Acknowledgments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b671ae6",
   "metadata": {},
   "source": [
    "Research supported with Cloud TPUs from Google's TensorFlow Research Cloud (TFRC).\n",
    "Thanks for providing access to the TFRC â¤ï¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747fd5d3",
   "metadata": {},
   "source": [
    "Thanks to the generous support from the [Hugging Face](https://huggingface.co/) team,\n",
    "it is possible to download both cased and uncased models from their S3 storage ðŸ¤—\n",
    "> The introduciton and weight file of this model are from [https://huggingface.co/dbmdz/bert-base-german-uncased](https://huggingface.co/dbmdz/bert-base-german-uncased), and we convert them to paddle related files.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
