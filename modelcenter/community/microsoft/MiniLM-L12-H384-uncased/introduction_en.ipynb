{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51ca3d5",
   "metadata": {},
   "source": [
    "## MiniLM: Small and Fast Pre-trained Models for Language Understanding and Generation\n",
    "\n",
    "MiniLM is a distilled model from the paper \"[MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957)\".\n",
    "\n",
    "Please find the information about preprocessing, training and full details of the MiniLM in the [original MiniLM repository](https://github.com/microsoft/unilm/blob/master/minilm/).\n",
    "\n",
    "Please note: This checkpoint can be an inplace substitution for BERT and it needs to be fine-tuned before use!\n",
    "\n",
    "### English Pre-trained Models\n",
    "We release the **uncased** **12**-layer model with **384** hidden size distilled from an in-house pre-trained [UniLM v2](/unilm) model in BERT-Base size.\n",
    "\n",
    "- MiniLMv1-L12-H384-uncased: 12-layer, 384-hidden, 12-heads, 33M parameters, 2.7x faster than BERT-Base\n",
    "\n",
    "#### Fine-tuning on NLU tasks\n",
    "\n",
    "We present the dev results on SQuAD 2.0 and several GLUE benchmark tasks.\n",
    "\n",
    "| Model                                             | #Param | SQuAD 2.0 | MNLI-m | SST-2 | QNLI | CoLA | RTE  | MRPC | QQP  |\n",
    "|---------------------------------------------------|--------|-----------|--------|-------|------|------|------|------|------|\n",
    "| [BERT-Base](https://arxiv.org/pdf/1810.04805.pdf) | 109M   | 76.8      | 84.5   | 93.2  | 91.7 | 58.9 | 68.6 | 87.3 | 91.3 |\n",
    "| **MiniLM-L12xH384**                               | 33M    | 81.7      | 85.7   | 93.0  | 91.5 | 58.5 | 73.3 | 89.5 | 91.3 |\n",
    "\n",
    "### Citation\n",
    "\n",
    "If you find MiniLM useful in your research, please cite the following paper:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "@misc{wang2020minilm,\n",
    "title={MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},\n",
    "author={Wenhui Wang and Furu Wei and Li Dong and Hangbo Bao and Nan Yang and Ming Zhou},\n",
    "year={2020},\n",
    "eprint={2002.10957},\n",
    "archivePrefix={arXiv},\n",
    "primaryClass={cs.CL}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de58db3",
   "metadata": {},
   "source": [
    "> 此模型来源于：[https://huggingface.co/microsoft/MiniLM-L12-H384-uncased](https://huggingface.co/https://huggingface.co/microsoft/MiniLM-L12-H384-uncased)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
