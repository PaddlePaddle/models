{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e77a243",
   "metadata": {},
   "source": [
    "# COVID-Twitter-BERT (CT-BERT) v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046aa916",
   "metadata": {},
   "source": [
    ":warning: _You may want to use the [v2 model](https://huggingface.co/digitalepidemiologylab/covid-twitter-bert-v2) which was trained on more recent data and yields better performance_ :warning:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98eb397",
   "metadata": {},
   "source": [
    "BERT-large-uncased model, pretrained on a corpus of messages from Twitter about COVID-19. Find more info on our [GitHub page](https://github.com/digitalepidemiologylab/covid-twitter-bert).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd82529",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This model was trained on 160M tweets collected between January 12 and April 16, 2020 containing at least one of the keywords \"wuhan\", \"ncov\", \"coronavirus\", \"covid\", or \"sars-cov-2\". These tweets were filtered and preprocessed to reach a final sample of 22.5M tweets (containing 40.7M sentences and 633M tokens) which were used for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f7adb",
   "metadata": {},
   "source": [
    "This model was evaluated based on downstream classification tasks, but it could be used for any other NLP task which can leverage contextual embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01567e",
   "metadata": {},
   "source": [
    "In order to achieve best results, make sure to use the same text preprocessing as we did for pretraining. This involves replacing user mentions, urls and emojis. You can find a script on our projects [GitHub repo](https://github.com/digitalepidemiologylab/covid-twitter-bert).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e40714",
   "metadata": {},
   "source": [
    "## Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab19f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23469dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18670ead",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Martin Müller, Marcel Salaté, Per E Kummervold. \"COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter\" arXiv preprint arXiv:2005.07503 (2020).\n",
    "> 此模型来源于：[https://huggingface.co/digitalepidemiologylab/covid-twitter-bert](https://huggingface.co/https://huggingface.co/digitalepidemiologylab/covid-twitter-bert)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
