{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# COVID-Twitter-BERT v2\n", "\n", "## Model description\n", "\n", "BERT-large-uncased model, pretrained on a corpus of messages from Twitter about COVID-19. This model is identical to [covid-twitter-bert](https://huggingface.co/digitalepidemiologylab/covid-twitter-bert) - but trained on more data, resulting in higher downstream performance.\n", "\n", "Find more info on our [GitHub page](https://github.com/digitalepidemiologylab/covid-twitter-bert).\n", "\n", "\n", "## Intended uses & limitations\n", "\n", "The model can e.g. be used in the `fill-mask` task (see below). You can also use the model without the MLM/NSP heads and train a classifier with it.\n", "\n", "#### How to use\n"]}, {"cell_type": "code", "metadata": {}, "source": "import paddle\nfrom paddlenlp.transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert-v2\")\ninput_ids = paddle.randint(100, 200, shape=[1, 20])\nprint(model(input_ids))"}, {"cell_type": "code", "metadata": {}, "source": ["COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter.\n", "arXiv preprint arXiv:2005.07503 (2020).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> 此模型来源于：[https://huggingface.co/digitalepidemiologylab/covid-twitter-bert-v2](https://huggingface.co/https://huggingface.co/digitalepidemiologylab/covid-twitter-bert-v2)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> 此模型来源于：[digitalepidemiologylab/covid-twitter-bert-v2](https://huggingface.co/digitalepidemiologylab/covid-twitter-bert-v2)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}