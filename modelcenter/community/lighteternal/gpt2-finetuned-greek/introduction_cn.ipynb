{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763c10a7",
   "metadata": {},
   "source": [
    "# Greek (el) GPT2 model\n",
    "<img src=\"https://huggingface.co/lighteternal/gpt2-finetuned-greek-small/raw/main/GPT2el.png\" width=\"600\"/>\n",
    "\n",
    "### By the Hellenic Army Academy (SSE) and the Technical University of Crete (TUC)\n",
    "\n",
    "* language: el\n",
    "* licence: apache-2.0\n",
    "* dataset: ~23.4 GB of Greek corpora\n",
    "* model: GPT2 (12-layer, 768-hidden, 12-heads, 117M parameters. OpenAI GPT-2 English model, finetuned for the Greek language)\n",
    "* pre-processing: tokenization + BPE segmentation\n",
    "* metrics: perplexity\n",
    "\n",
    "### Model description\n",
    "\n",
    "A text generation (autoregressive) model, using Huggingface transformers and fastai based on the English GPT-2.\n",
    "Finetuned with gradual layer unfreezing. This is a more efficient and sustainable alternative compared to training from scratch, especially for low-resource languages.\n",
    "Based on the work of Thomas Dehaene (ML6) for the creation of a Dutch GPT2: https://colab.research.google.com/drive/1Y31tjMkB8TqKKFlZ5OJ9fcMp3p8suvs4?usp=sharing\n",
    "\n",
    "\n",
    "### How to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"lighteternal/gpt2-finetuned-greek\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ea29a",
   "metadata": {},
   "source": [
    "\n",
    "## Training data\n",
    "\n",
    "We used a 23.4GB sample from a consolidated Greek corpus from CC100, Wikimatrix, Tatoeba, Books, SETIMES and GlobalVoices containing long senquences.\n",
    "This is a better version of our GPT-2 small model (https://huggingface.co/lighteternal/gpt2-finetuned-greek-small)\n",
    "\n",
    "\n",
    "## Metrics\n",
    "\n",
    "| Metric      | Value |\n",
    "| ----------- | ----------- |\n",
    "| Train Loss | 3.67 |\n",
    "| Validation Loss | 3.83 |\n",
    "| Perplexity  | 39.12 |\n",
    "\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "The research work was supported by the Hellenic Foundation for Research and Innovation (HFRI) under the HFRI PhD Fellowship grant (Fellowship Number:50, 2nd call)\n",
    "\n",
    "Based on the work of Thomas Dehaene (ML6): https://blog.ml6.eu/dutch-gpt2-autoregressive-language-modelling-on-a-budget-cff3942dd020\n",
    "> 此模型来源于：[https://huggingface.co/lighteternal/gpt2-finetuned-greek](https://huggingface.co/https://huggingface.co/lighteternal/gpt2-finetuned-greek)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
