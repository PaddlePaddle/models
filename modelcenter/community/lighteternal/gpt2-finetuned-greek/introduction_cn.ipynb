{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9272c7",
   "metadata": {},
   "source": [
    "# Greek (el) GPT2 model\n",
    "<img src=\"https://huggingface.co/lighteternal/gpt2-finetuned-greek-small/raw/main/GPT2el.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195cfbc",
   "metadata": {},
   "source": [
    "### By the Hellenic Army Academy (SSE) and the Technical University of Crete (TUC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ac26f",
   "metadata": {},
   "source": [
    "* language: el\n",
    "* licence: apache-2.0\n",
    "* dataset: ~23.4 GB of Greek corpora\n",
    "* model: GPT2 (12-layer, 768-hidden, 12-heads, 117M parameters. OpenAI GPT-2 English model, finetuned for the Greek language)\n",
    "* pre-processing: tokenization + BPE segmentation\n",
    "* metrics: perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2babcf",
   "metadata": {},
   "source": [
    "### Model description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69d788",
   "metadata": {},
   "source": [
    "A text generation (autoregressive) model, using Huggingface transformers and fastai based on the English GPT-2.\n",
    "Finetuned with gradual layer unfreezing. This is a more efficient and sustainable alternative compared to training from scratch, especially for low-resource languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68be04e",
   "metadata": {},
   "source": [
    "### How to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bce304",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d92630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"lighteternal/gpt2-finetuned-greek\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8517b",
   "metadata": {},
   "source": [
    "## Training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d6f19",
   "metadata": {},
   "source": [
    "We used a 23.4GB sample from a consolidated Greek corpus from CC100, Wikimatrix, Tatoeba, Books, SETIMES and GlobalVoices containing long senquences.\n",
    "This is a better version of our GPT-2 small model (https://huggingface.co/lighteternal/gpt2-finetuned-greek-small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca6ffc",
   "metadata": {},
   "source": [
    "## Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc066a",
   "metadata": {},
   "source": [
    "| Metric      | Value |\n",
    "| ----------- | ----------- |\n",
    "| Train Loss | 3.67 |\n",
    "| Validation Loss | 3.83 |\n",
    "| Perplexity  | 39.12 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0b462",
   "metadata": {},
   "source": [
    "### Acknowledgement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792158ef",
   "metadata": {},
   "source": [
    "The research work was supported by the Hellenic Foundation for Research and Innovation (HFRI) under the HFRI PhD Fellowship grant (Fellowship Number:50, 2nd call)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e7b4c",
   "metadata": {},
   "source": [
    "Based on the work of Thomas Dehaene (ML6): https://blog.ml6.eu/dutch-gpt2-autoregressive-language-modelling-on-a-budget-cff3942dd020\n",
    "> 此模型来源于：[https://huggingface.co/lighteternal/gpt2-finetuned-greek](https://huggingface.co/https://huggingface.co/lighteternal/gpt2-finetuned-greek)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
