{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Greek (el) GPT2 model\n", "<img src=\"https://huggingface.co/lighteternal/gpt2-finetuned-greek-small/raw/main/GPT2el.png\" width=\"600\"/>\n", "\n", "### By the Hellenic Army Academy (SSE) and the Technical University of Crete (TUC)\n", "\n", "* language: el\n", "* licence: apache-2.0\n", "* dataset: ~23.4 GB of Greek corpora\n", "* model: GPT2 (12-layer, 768-hidden, 12-heads, 117M parameters. OpenAI GPT-2 English model, finetuned for the Greek language)\n", "* pre-processing: tokenization + BPE segmentation\n", "* metrics: perplexity\n", "\n", "### Model description\n", "\n", "A text generation (autoregressive) model, using Huggingface transformers and fastai based on the English GPT-2.\n", "Finetuned with gradual layer unfreezing. This is a more efficient and sustainable alternative compared to training from scratch, especially for low-resource languages.\n", "Based on the work of Thomas Dehaene (ML6) for the creation of a Dutch GPT2: https://colab.research.google.com/drive/1Y31tjMkB8TqKKFlZ5OJ9fcMp3p8suvs4?usp=sharing\n", "\n", "\n", "### How to use\n"]}, {"cell_type": "code", "metadata": {}, "source": "import paddle\nfrom paddlenlp.transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"lighteternal/gpt2-finetuned-greek\")\ninput_ids = paddle.randint(100, 200, shape=[1, 20])\nprint(model(input_ids))"}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## Training data\n", "\n", "We used a 23.4GB sample from a consolidated Greek corpus from CC100, Wikimatrix, Tatoeba, Books, SETIMES and GlobalVoices containing long senquences.\n", "This is a better version of our GPT-2 small model (https://huggingface.co/lighteternal/gpt2-finetuned-greek-small)\n", "\n", "\n", "## Metrics\n", "\n", "| Metric      | Value |\n", "| ----------- | ----------- |\n", "| Train Loss | 3.67 |\n", "| Validation Loss | 3.83 |\n", "| Perplexity  | 39.12 |\n", "\n", "\n", "### Acknowledgement\n", "\n", "The research work was supported by the Hellenic Foundation for Research and Innovation (HFRI) under the HFRI PhD Fellowship grant (Fellowship Number:50, 2nd call)\n", "\n", "Based on the work of Thomas Dehaene (ML6): https://blog.ml6.eu/dutch-gpt2-autoregressive-language-modelling-on-a-budget-cff3942dd020\n", "> 此模型来源于：[https://huggingface.co/lighteternal/gpt2-finetuned-greek](https://huggingface.co/https://huggingface.co/lighteternal/gpt2-finetuned-greek)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> 此模型来源于：[lighteternal/gpt2-finetuned-greek](https://huggingface.co/lighteternal/gpt2-finetuned-greek)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}