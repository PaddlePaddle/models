{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f1603d",
   "metadata": {},
   "source": [
    "# Greek (el) GPT2 model\n",
    "<img src=\"https://huggingface.co/lighteternal/gpt2-finetuned-greek-small/raw/main/GPT2el.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859f86e",
   "metadata": {},
   "source": [
    "### By the Hellenic Army Academy (SSE) and the Technical University of Crete (TUC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3ecd1",
   "metadata": {},
   "source": [
    "* language: el\n",
    "* licence: apache-2.0\n",
    "* dataset: ~23.4 GB of Greek corpora\n",
    "* model: GPT2 (12-layer, 768-hidden, 12-heads, 117M parameters. OpenAI GPT-2 English model, finetuned for the Greek language)\n",
    "* pre-processing: tokenization + BPE segmentation\n",
    "* metrics: perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b1434",
   "metadata": {},
   "source": [
    "### Model description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371dd1d9",
   "metadata": {},
   "source": [
    "A text generation (autoregressive) model, using Huggingface transformers and fastai based on the English GPT-2.\n",
    "Finetuned with gradual layer unfreezing. This is a more efficient and sustainable alternative compared to training from scratch, especially for low-resource languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697fe36",
   "metadata": {},
   "source": [
    "### How to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"lighteternal/gpt2-finetuned-greek\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885df404",
   "metadata": {},
   "source": [
    "## Training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a7e6b",
   "metadata": {},
   "source": [
    "We used a 23.4GB sample from a consolidated Greek corpus from CC100, Wikimatrix, Tatoeba, Books, SETIMES and GlobalVoices containing long senquences.\n",
    "This is a better version of our GPT-2 small model (https://huggingface.co/lighteternal/gpt2-finetuned-greek-small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5c88f",
   "metadata": {},
   "source": [
    "## Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde5df4",
   "metadata": {},
   "source": [
    "| Metric      | Value |\n",
    "| ----------- | ----------- |\n",
    "| Train Loss | 3.67 |\n",
    "| Validation Loss | 3.83 |\n",
    "| Perplexity  | 39.12 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38bf0f2",
   "metadata": {},
   "source": [
    "### Acknowledgement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f85a3",
   "metadata": {},
   "source": [
    "The research work was supported by the Hellenic Foundation for Research and Innovation (HFRI) under the HFRI PhD Fellowship grant (Fellowship Number:50, 2nd call)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594fe05b",
   "metadata": {},
   "source": [
    "Based on the work of Thomas Dehaene (ML6): https://blog.ml6.eu/dutch-gpt2-autoregressive-language-modelling-on-a-budget-cff3942dd020\n",
    "> 此模型来源于：[https://huggingface.co/lighteternal/gpt2-finetuned-greek](https://huggingface.co/https://huggingface.co/lighteternal/gpt2-finetuned-greek)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
