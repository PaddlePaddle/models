{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f345dee",
   "metadata": {},
   "source": [
    "# CodeParrot ğŸ¦œ (small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe3b46",
   "metadata": {},
   "source": [
    "CodeParrot ğŸ¦œ is a GPT-2 model (110M parameters) trained to generate Python code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259e928",
   "metadata": {},
   "source": [
    "## Usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515b13d",
   "metadata": {},
   "source": [
    "You can load the CodeParrot model and tokenizer directly in `transformers`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"lvwerra/codeparrot-small\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6572fc",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ed3c5",
   "metadata": {},
   "source": [
    "The model was trained on the cleaned [CodeParrot ğŸ¦œ dataset](https://huggingface.co/datasets/codeparrot/codeparrot-clean) with the following settings:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e691c",
   "metadata": {},
   "source": [
    "|Config|Value|\n",
    "|-------|-----|\n",
    "|Batch size| 192 |\n",
    "|Context size| 1024 |\n",
    "|Training steps| 150'000|\n",
    "|Gradient accumulation| 1|\n",
    "|Gradient checkpointing| False|\n",
    "|Learning rate| 5e-4 |\n",
    "|Weight decay | 0.1 |\n",
    "|Warmup steps| 2000 |\n",
    "|Schedule| Cosine |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364bfd84",
   "metadata": {},
   "source": [
    "The training was executed on 16 x A100 (40GB) GPUs. This setting amounts to roughly 29 billion tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5182f5",
   "metadata": {},
   "source": [
    "## Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc914c1",
   "metadata": {},
   "source": [
    "We evaluated the model on OpenAI's [HumanEval](https://huggingface.co/datasets/openai_humaneval) benchmark which consists of programming challenges:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4576cb",
   "metadata": {},
   "source": [
    "| Metric | Value |\n",
    "|-------|-----|\n",
    "|pass@1 | 3.80% |\n",
    "|pass@10 | 6.57%\t |\n",
    "|pass@100 | 12.78% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cbf14",
   "metadata": {},
   "source": [
    "The [pass@k metric](https://huggingface.co/metrics/code_eval) tells the probability that at least one out of k generations passes the tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3df20e",
   "metadata": {},
   "source": [
    "## Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98923f5",
   "metadata": {},
   "source": [
    "- Dataset: [full](https://huggingface.co/datasets/codeparrot/codeparrot-clean), [train](https://huggingface.co/datasets/codeparrot/codeparrot-clean-train), [valid](https://huggingface.co/datasets/codeparrot/codeparrot-clean-valid)\n",
    "- Code: [repository](https://github.com/huggingface/transformers/tree/master/examples/research_projects/codeparrot)\n",
    "- Spaces: [generation](), [highlighting]()\n",
    "> æ­¤æ¨¡å‹æ¥æºäºï¼š[https://huggingface.co/lvwerra/codeparrot-small](https://huggingface.co/https://huggingface.co/lvwerra/codeparrot-small)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
