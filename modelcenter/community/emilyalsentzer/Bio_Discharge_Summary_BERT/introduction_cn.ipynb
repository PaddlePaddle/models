{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb2da20",
   "metadata": {},
   "source": [
    "\n",
    "# ClinicalBERT - Bio + Discharge Summary BERT Model\n",
    "\n",
    "The [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) paper contains four unique clinicalBERT models: initialized with BERT-Base (`cased_L-12_H-768_A-12`) or BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes or only discharge summaries.\n",
    "\n",
    "This model card describes the Bio+Discharge Summary BERT model, which was initialized from [BioBERT](https://arxiv.org/abs/1901.08746) & trained on only discharge summaries from MIMIC.\n",
    "\n",
    "## Pretraining Data\n",
    "The `Bio_Discharge_Summary_BERT` model was trained on all discharge summaries from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. For more details on MIMIC, see [here](https://mimic.physionet.org/). All notes from the `NOTEEVENTS` table were included (~880M words).\n",
    "\n",
    "## Model Pretraining\n",
    "\n",
    "### Note Preprocessing\n",
    "Each note in MIMIC was first split into sections using a rules-based section splitter (e.g. discharge summary notes were split into \"History of Present Illness\", \"Family History\", \"Brief Hospital Course\", etc. sections). Then each section was split into sentences using SciSpacy (`en core sci md` tokenizer).\n",
    "\n",
    "### Pretraining Procedures\n",
    "The model was trained using code from [Google's BERT repository](https://github.com/google-research/bert) on a GeForce GTX TITAN X 12 GB GPU. Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`).\n",
    "\n",
    "### Pretraining Hyperparameters\n",
    "We used a batch size of 32, a maximum sequence length of 128, and a learning rate of 5 · 10−5 for pre-training our models. The models trained on all MIMIC notes  were trained for 150,000 steps. The dup factor for duplicating input data with different masks was set to 5. All other default parameters were used (specifically, masked language model probability = 0.15\n",
    "and max predictions per sequence = 20).\n",
    "\n",
    "## How to use the model\n",
    "\n",
    "Load the model via the transformers library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee908a4d",
   "metadata": {},
   "source": [
    "## More Information\n",
    "\n",
    "Refer to the original paper, [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) (NAACL Clinical NLP Workshop 2019) for additional details and performance on NLI and NER tasks.\n",
    "\n",
    "## Questions?\n",
    "\n",
    "Post a Github issue on the [clinicalBERT repo](https://github.com/EmilyAlsentzer/clinicalBERT) or email emilya@mit.edu with any questions.\n",
    "\n",
    "> 此模型来源于：[https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT](https://huggingface.co/https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
