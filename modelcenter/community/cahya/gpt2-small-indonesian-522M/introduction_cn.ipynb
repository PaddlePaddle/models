{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Indonesian GPT2 small model\n", "\n", "## Model description\n", "It is GPT2-small model pre-trained with indonesian Wikipedia using a causal language modeling (CLM) objective. This\n", "model is uncased: it does not make a difference between indonesia and Indonesia.\n", "\n", "This is one of several other language models that have been pre-trained with indonesian datasets. More detail about\n", "its usage on downstream tasks (text classification, text generation, etc) is available at [Transformer based Indonesian Language Models](https://github.com/cahya-wirawan/indonesian-language-models/tree/master/Transformers)\n", "\n", "## Intended uses & limitations\n", "\n", "### How to use\n", "You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness,\n", "we set a seed for reproducibility:\n"]}, {"cell_type": "code", "metadata": {}, "source": "import paddle\nfrom paddlenlp.transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"cahya/gpt2-small-indonesian-522M\")\ninput_ids = paddle.randint(100, 200, shape=[1, 20])\nprint(model(input_ids))"}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import GPT2Tokenizer, TFGPT2Model\n", "\n", "model_name='cahya/gpt2-small-indonesian-522M'\n", "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n", "model = TFGPT2Model.from_pretrained(model_name)\n", "text = \"Silakan diganti dengan text apa saja.\"\n", "encoded_input = tokenizer(text, return_tensors='tf')\n", "output = model(encoded_input)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training data\n", "\n", "This model was pre-trained with 522MB of indonesian Wikipedia.\n", "The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and\n", "a vocabulary size of 52,000. The inputs are sequences of 128 consecutive tokens.\n", "> 此模型来源于：[https://huggingface.co/cahya/gpt2-small-indonesian-522M](https://huggingface.co/https://huggingface.co/cahya/gpt2-small-indonesian-522M)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> 此模型来源于：[cahya/gpt2-small-indonesian-522M](https://huggingface.co/cahya/gpt2-small-indonesian-522M)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}