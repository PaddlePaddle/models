{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ec3f6a",
   "metadata": {},
   "source": [
    "# Indonesian GPT2 small model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde2bc0",
   "metadata": {},
   "source": [
    "## Model description\n",
    "It is GPT2-small model pre-trained with indonesian Wikipedia using a causal language modeling (CLM) objective. This\n",
    "model is uncased: it does not make a difference between indonesia and Indonesia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693a135",
   "metadata": {},
   "source": [
    "This is one of several other language models that have been pre-trained with indonesian datasets. More detail about\n",
    "its usage on downstream tasks (text classification, text generation, etc) is available at [Transformer based Indonesian Language Models](https://github.com/cahya-wirawan/indonesian-language-models/tree/master/Transformers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c43a2a",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"cahya/gpt2-small-indonesian-522M\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30cec3",
   "metadata": {},
   "source": [
    "## Training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ab126",
   "metadata": {},
   "source": [
    "This model was pre-trained with 522MB of indonesian Wikipedia.\n",
    "The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and\n",
    "a vocabulary size of 52,000. The inputs are sequences of 128 consecutive tokens.\n",
    "> The introduciton and weight file of this model are from [https://huggingface.co/cahya/gpt2-small-indonesian-522M](https://huggingface.co/cahya/gpt2-small-indonesian-522M), and we convert them to paddle related files.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
