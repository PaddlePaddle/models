{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f95bf0",
   "metadata": {},
   "source": [
    "# Indonesian BERT base model (uncased)\n",
    "\n",
    "## Model description\n",
    "It is BERT-base model pre-trained with indonesian Wikipedia using a masked language modeling (MLM) objective. This\n",
    "model is uncased: it does not make a difference between indonesia and Indonesia.\n",
    "\n",
    "This is one of several other language models that have been pre-trained with indonesian datasets. More detail about\n",
    "its usage on downstream tasks (text classification, text generation, etc) is available at [Transformer based Indonesian Language Models](https://github.com/cahya-wirawan/indonesian-language-models/tree/master/Transformers)\n",
    "\n",
    "## Intended uses & limitations\n",
    "\n",
    "### How to use\n",
    "You can use this model directly with a pipeline for masked language modeling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"cahya/bert-base-indonesian-522M\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "model_name='cahya/bert-base-indonesian-522M'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)\n",
    "text = \"Silakan diganti dengan text apa saja.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5966b",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "This model was pre-trained with 522MB of indonesian Wikipedia.\n",
    "The texts are lowercased and tokenized using WordPiece and a vocabulary size of 32,000. The inputs of the model are\n",
    "then of the form:\n",
    "\n",
    "```[CLS] Sentence A [SEP] Sentence B [SEP]```\n",
    "> 此模型来源于：[https://huggingface.co/cahya/bert-base-indonesian-522M](https://huggingface.co/https://huggingface.co/cahya/bert-base-indonesian-522M)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
