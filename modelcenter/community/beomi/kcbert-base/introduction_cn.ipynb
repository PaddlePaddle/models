{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf8cb9fb",
   "metadata": {},
   "source": [
    "# KcBERT: Korean comments BERT\n",
    "\n",
    "** Updates on 2021.04.07 **\n",
    "\n",
    "- KcELECTRA가 릴리즈 되었습니다!🤗\n",
    "- KcELECTRA는 보다 더 많은 데이터셋, 그리고 더 큰 General vocab을 통해 KcBERT 대비 **모든 태스크에서 더 높은 성능**을 보입니다.\n",
    "- 아래 깃헙 링크에서 직접 사용해보세요!\n",
    "- https://github.com/Beomi/KcELECTRA\n",
    "\n",
    "** Updates on 2021.03.14 **\n",
    "\n",
    "- KcBERT Paper 인용 표기를 추가하였습니다.(bibtex)\n",
    "- KcBERT-finetune Performance score를 본문에 추가하였습니다.\n",
    "\n",
    "** Updates on 2020.12.04 **\n",
    "\n",
    "Huggingface Transformers가 v4.0.0으로 업데이트됨에 따라 Tutorial의 코드가 일부 변경되었습니다.\n",
    "\n",
    "업데이트된 KcBERT-Large NSMC Finetuning Colab: <a href=\"https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "** Updates on 2020.09.11 **\n",
    "\n",
    "KcBERT를 Google Colab에서 TPU를 통해 학습할 수 있는 튜토리얼을 제공합니다! 아래 버튼을 눌러보세요.\n",
    "\n",
    "Colab에서 TPU로 KcBERT Pretrain 해보기: <a href=\"https://colab.research.google.com/drive/1lYBYtaXqt9S733OXdXvrvC09ysKFN30W\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "텍스트 분량만 전체 12G 텍스트 중 일부(144MB)로 줄여 학습을 진행합니다.\n",
    "\n",
    "한국어 데이터셋/코퍼스를 좀더 쉽게 사용할 수 있는 [Korpora](https://github.com/ko-nlp/Korpora) 패키지를 사용합니다.\n",
    "\n",
    "** Updates on 2020.09.08 **\n",
    "\n",
    "Github Release를 통해 학습 데이터를 업로드하였습니다.\n",
    "\n",
    "다만 한 파일당 2GB 이내의 제약으로 인해 분할압축되어있습니다.\n",
    "\n",
    "아래 링크를 통해 받아주세요. (가입 없이 받을 수 있어요. 분할압축)\n",
    "\n",
    "만약 한 파일로 받고싶으시거나/Kaggle에서 데이터를 살펴보고 싶으시다면 아래의 캐글 데이터셋을 이용해주세요.\n",
    "\n",
    "- Github릴리즈: https://github.com/Beomi/KcBERT/releases/tag/TrainData_v1\n",
    "\n",
    "** Updates on 2020.08.22 **\n",
    "\n",
    "Pretrain Dataset 공개\n",
    "\n",
    "- 캐글: https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments (한 파일로 받을 수 있어요. 단일파일)\n",
    "\n",
    "Kaggle에 학습을 위해 정제한(아래 `clean`처리를 거친) Dataset을 공개하였습니다!\n",
    "\n",
    "직접 다운받으셔서 다양한 Task에 학습을 진행해보세요 :)\n",
    "\n",
    "\n",
    "공개된 한국어 BERT는 대부분 한국어 위키, 뉴스 기사, 책 등 잘 정제된 데이터를 기반으로 학습한 모델입니다. 한편, 실제로 NSMC와 같은 댓글형 데이터셋은 정제되지 않았고 구어체 특징에 신조어가 많으며, 오탈자 등 공식적인 글쓰기에서 나타나지 않는 표현들이 빈번하게 등장합니다.\n",
    "\n",
    "KcBERT는 위와 같은 특성의 데이터셋에 적용하기 위해, 네이버 뉴스에서 댓글과 대댓글을 수집해, 토크나이저와 BERT모델을 처음부터 학습한 Pretrained BERT 모델입니다.\n",
    "\n",
    "KcBERT는 Huggingface의 Transformers 라이브러리를 통해 간편히 불러와 사용할 수 있습니다. (별도의 파일 다운로드가 필요하지 않습니다.)\n",
    "\n",
    "## KcBERT Performance\n",
    "\n",
    "- Finetune 코드는 https://github.com/Beomi/KcBERT-finetune 에서 찾아보실 수 있습니다.\n",
    "\n",
    "|                       | Size<br/>(용량)  | **NSMC**<br/>(acc) | **Naver NER**<br/>(F1) | **PAWS**<br/>(acc) | **KorNLI**<br/>(acc) | **KorSTS**<br/>(spearman) | **Question Pair**<br/>(acc) | **KorQuaD (Dev)**<br/>(EM/F1) |\n",
    "| :-------------------- | :---: | :----------------: | :--------------------: | :----------------: | :------------------: | :-----------------------: | :-------------------------: | :---------------------------: |\n",
    "| KcBERT-Base                | 417M  |       89.62        |         84.34          |       66.95        |        74.85         |           75.57           |            93.93            |         60.25 / 84.39         |\n",
    "| KcBERT-Large                | 1.2G  |       **90.68**        |         85.53          |       70.15        |        76.99         |           77.49           |            94.06            |         62.16 / 86.64          |\n",
    "| KoBERT                | 351M  |       89.63        |         86.11          |       80.65        |        79.00         |           79.64           |            93.93            |         52.81 / 80.27         |\n",
    "| XLM-Roberta-Base      | 1.03G |       89.49        |         86.26          |       82.95        |        79.92         |           79.09           |            93.53            |         64.70 / 88.94         |\n",
    "| HanBERT               | 614M  |       90.16        |       **87.31**        |       82.40        |      **80.89**       |           83.33           |            94.19            |         78.74 / 92.02         |\n",
    "| KoELECTRA-Base    | 423M  |     **90.21**      |         86.87          |       81.90        |        80.85         |           83.21           |            94.20            |         61.10 / 89.59         |\n",
    "| KoELECTRA-Base-v2 | 423M  |       89.70        |         87.02          |     **83.90**      |        80.61         |         **84.30**         |          **94.72**          |       **84.34 / 92.58**       |\n",
    "| DistilKoBERT           | 108M |       88.41        |         84.13          |       62.55        |        70.55         |           73.21           |            92.48            |         54.12 / 77.80         |\n",
    "\n",
    "\n",
    "\\*HanBERT의 Size는 Bert Model과 Tokenizer DB를 합친 것입니다.\n",
    "\n",
    "\\***config의 세팅을 그대로 하여 돌린 결과이며, hyperparameter tuning을 추가적으로 할 시 더 좋은 성능이 나올 수 있습니다.**\n",
    "\n",
    "## How to use\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- `pytorch <= 1.8.0`\n",
    "- `transformers ~= 3.0.1`\n",
    "- `transformers ~= 4.0.0` 도 호환됩니다.\n",
    "- `emoji ~= 0.6.0`\n",
    "- `soynlp ~= 0.0.493`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"beomi/kcbert-base\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf564ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inproceedings{lee2020kcbert,\n",
    "title={KcBERT: Korean Comments BERT},\n",
    "author={Lee, Junbum},\n",
    "booktitle={Proceedings of the 32nd Annual Conference on Human and Cognitive Language Technology},\n",
    "pages={437--440},\n",
    "year={2020}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad9a93",
   "metadata": {},
   "source": [
    "- 논문집 다운로드 링크: http://hclt.kr/dwn/?v=bG5iOmNvbmZlcmVuY2U7aWR4OjMy (*혹은 http://hclt.kr/symp/?lnb=conference )\n",
    "\n",
    "## Acknowledgement\n",
    "\n",
    "KcBERT Model을 학습하는 GCP/TPU 환경은 [TFRC](https://www.tensorflow.org/tfrc?hl=ko) 프로그램의 지원을 받았습니다.\n",
    "\n",
    "모델 학습 과정에서 많은 조언을 주신 [Monologg](https://github.com/monologg/) 님 감사합니다 :)\n",
    "\n",
    "## Reference\n",
    "\n",
    "### Github Repos\n",
    "\n",
    "- [BERT by Google](https://github.com/google-research/bert)\n",
    "- [KoBERT by SKT](https://github.com/SKTBrain/KoBERT)\n",
    "- [KoELECTRA by Monologg](https://github.com/monologg/KoELECTRA/)\n",
    "\n",
    "- [Transformers by Huggingface](https://github.com/huggingface/transformers)\n",
    "- [Tokenizers by Hugginface](https://github.com/huggingface/tokenizers)\n",
    "\n",
    "### Papers\n",
    "\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "### Blogs\n",
    "\n",
    "- [Monologg님의 KoELECTRA 학습기](https://monologg.kr/categories/NLP/ELECTRA/)\n",
    "- [Colab에서 TPU로 BERT 처음부터 학습시키기 - Tensorflow/Google ver.](https://beomi.github.io/2020/02/26/Train-BERT-from-scratch-on-colab-TPU-Tensorflow-ver/)\n",
    "\n",
    "> 此模型来源于：[https://huggingface.co/beomi/kcbert-base](https://huggingface.co/https://huggingface.co/beomi/kcbert-base)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
