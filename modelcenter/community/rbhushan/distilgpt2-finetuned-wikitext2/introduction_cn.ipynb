{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2406fda",
   "metadata": {},
   "source": [
    "<!-- This model card has been generated automatically according to the information the Trainer had access to. You\n",
    "should probably proofread and complete it, then remove this comment. -->\n",
    "\n",
    "# distilgpt2-finetuned-wikitext2\n",
    "\n",
    "This model is a fine-tuned version of [distilgpt2](https://huggingface.co/distilgpt2) on the None dataset.\n",
    "It achieves the following results on the evaluation set:\n",
    "- Loss: 5.2872\n",
    "\n",
    "## Model description\n",
    "\n",
    "More information needed\n",
    "\n",
    "## Intended uses & limitations\n",
    "\n",
    "More information needed\n",
    "\n",
    "## Training and evaluation data\n",
    "\n",
    "More information needed\n",
    "\n",
    "## Training procedure\n",
    "\n",
    "### Training hyperparameters\n",
    "\n",
    "The following hyperparameters were used during training:\n",
    "- learning_rate: 2e-05\n",
    "- train_batch_size: 8\n",
    "- eval_batch_size: 8\n",
    "- seed: 42\n",
    "- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
    "- lr_scheduler_type: linear\n",
    "- num_epochs: 3.0\n",
    "\n",
    "### Training results\n",
    "\n",
    "| Training Loss | Epoch | Step | Validation Loss |\n",
    "|:-------------:|:-----:|:----:|:---------------:|\n",
    "| No log        | 1.0   | 73   | 5.4169          |\n",
    "| No log        | 2.0   | 146  | 5.3145          |\n",
    "| No log        | 3.0   | 219  | 5.2872          |\n",
    "\n",
    "\n",
    "### Framework versions\n",
    "\n",
    "- Transformers 4.12.5\n",
    "- Pytorch 1.10.0\n",
    "- Datasets 1.16.1\n",
    "- Tokenizers 0.10.3\n",
    "> 此模型来源于：[https://huggingface.co/rbhushan/distilgpt2-finetuned-wikitext2](https://huggingface.co/https://huggingface.co/rbhushan/distilgpt2-finetuned-wikitext2)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
