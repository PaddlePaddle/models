{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e48589",
   "metadata": {},
   "source": [
    "# roberta-large-ner-english: model fine-tuned from roberta-large for NER task\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[roberta-large-ner-english] is an english NER model that was fine-tuned from roberta-large on conll2003 dataset.\n",
    "Model was validated on emails/chat data and outperformed other models on this type of data specifically.\n",
    "In particular the model seems to work better on entity that don't start with an upper case.\n",
    "\n",
    "\n",
    "## Training data\n",
    "\n",
    "Training data was classified as follow:\n",
    "\n",
    "Abbreviation|Description\n",
    "-|-\n",
    "O |Outside of a named entity\n",
    "MISC |Miscellaneous entity\n",
    "PER |Person’s name\n",
    "ORG |Organization\n",
    "LOC |Location\n",
    "\n",
    "In order to simplify, the prefix B- or I- from original conll2003 was removed.\n",
    "I used the train and test dataset from original conll2003 for training and the \"validation\" dataset for validation. This resulted in a dataset of size:\n",
    "\n",
    "Train | Validation\n",
    "-|-\n",
    "17494 | 3250\n",
    "\n",
    "## How to use roberta-large-ner-english with HuggingFace\n",
    "\n",
    "##### Load roberta-large-ner-english and its sub-word tokenizer :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f59a3",
   "metadata": {},
   "source": [
    "\n",
    "## Model performances\n",
    "\n",
    "Model performances computed on conll2003 validation dataset (computed on the tokens predictions)\n",
    "\n",
    "entity|precision|recall|f1\n",
    "-|-|-|-\n",
    "PER|0.9914|0.9927|0.9920\n",
    "ORG|0.9627|0.9661|0.9644\n",
    "LOC|0.9795|0.9862|0.9828\n",
    "MISC|0.9292|0.9262|0.9277\n",
    "Overall|0.9740|0.9766|0.9753\n",
    "\n",
    "\n",
    "On private dataset (email, chat, informal discussion), computed on word predictions:\n",
    "\n",
    "entity|precision|recall|f1\n",
    "-|-|-|-\n",
    "PER|0.8823|0.9116|0.8967\n",
    "ORG|0.7694|0.7292|0.7487\n",
    "LOC|0.8619|0.7768|0.8171\n",
    "\n",
    "By comparison on the same private dataset, Spacy (en_core_web_trf-3.2.0) was giving:\n",
    "\n",
    "entity|precision|recall|f1\n",
    "-|-|-|-\n",
    "PER|0.9146|0.8287|0.8695\n",
    "ORG|0.7655|0.6437|0.6993\n",
    "LOC|0.8727|0.6180|0.7236\n",
    "\n",
    "\n",
    "\n",
    "For those who could be interested, here is a short article on how I used the results of this model to train a LSTM model for signature detection in emails:\n",
    "https://medium.com/@jean-baptiste.polle/lstm-model-for-email-signature-detection-8e990384fefa\n",
    "> 此模型来源于：[https://huggingface.co/Jean-Baptiste/roberta-large-ner-english](https://huggingface.co/https://huggingface.co/Jean-Baptiste/roberta-large-ner-english)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
