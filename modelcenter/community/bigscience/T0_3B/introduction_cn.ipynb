{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["**How do I pronounce the name of the model?** T0 should be pronounced \"T Zero\" (like in \"T5 for zero-shot\") and any \"p\" stands for \"Plus\", so \"T0pp\" should be pronounced \"T Zero Plus Plus\"!\n", "\n", "**Official repository**: [bigscience-workshop/t-zero](https://github.com/bigscience-workshop/t-zero)\n", "\n", "# Model Description\n", "\n", "T0* shows zero-shot task generalization on English natural language prompts, outperforming GPT-3 on many tasks, while being 16x smaller. It is a series of encoder-decoder models trained on a large set of different tasks specified in natural language prompts. We convert numerous English supervised datasets into prompts, each with multiple templates using varying formulations. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. To obtain T0*, we fine-tune a pretrained language model on this multitask mixture covering many different NLP tasks.\n", "\n", "# Intended uses\n", "\n", "You can use the models to perform inference on tasks by specifying your query in natural language, and the models will generate a prediction. For instance, you can ask *\"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\"*, and the model will hopefully generate *\"Positive\"*.\n", "\n", "A few other examples that you can try:\n", "- *A is the son's of B's uncle. What is the family relationship between A and B?*\n", "- *Question A: How is air traffic controlled?<br>\n", "Question B: How do you become an air traffic controller?<br>\n", "Pick one: these questions are duplicates or not duplicates.*\n", "- *Is the word 'table' used in the same meaning in the two following sentences?<br><br>\n", "Sentence A: you can leave the books on the table over there.<br>\n", "Sentence B: the tables in this book are very hard to read.*\n", "- *Max: Know any good websites to buy clothes from?<br>\n", "Payton: Sure :) LINK 1, LINK 2, LINK 3<br>\n", "Max: That's a lot of them!<br>\n", "Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.<br>\n", "Max: I'll check them out. Thanks.<br><br>\n", "Who or what are Payton and Max referring to when they say 'them'?*\n", "- *On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.<br>\n", "The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.<br><br>\n", "Which book is the leftmost book?*\n", "- *Reorder the words in this sentence: justin and name bieber years is my am I 27 old.*\n", "\n", "# How to use\n", "\n", "We make available the models presented in our [paper](https://arxiv.org/abs/2110.08207) along with the ablation models. We recommend using the [T0pp](https://huggingface.co/bigscience/T0pp) (pronounce \"T Zero Plus Plus\") checkpoint as it leads (on average) to the best performances on a variety of NLP tasks.\n", "\n", "|Model|Number of parameters|\n", "|-|-|\n", "|[T0](https://huggingface.co/bigscience/T0)|11 billion|\n", "|[T0p](https://huggingface.co/bigscience/T0p)|11 billion|\n", "|[T0pp](https://huggingface.co/bigscience/T0pp)|11 billion|\n", "|[T0_single_prompt](https://huggingface.co/bigscience/T0_single_prompt)|11 billion|\n", "|[T0_original_task_only](https://huggingface.co/bigscience/T0_original_task_only)|11 billion|\n", "|[T0_3B](https://huggingface.co/bigscience/T0_3B)|3 billion|\n", "\n", "Here is how to use the model in PyTorch:\n"]}, {"cell_type": "code", "metadata": {}, "source": "import paddle\nfrom paddlenlp.transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bigscience/T0_3B\")\ninput_ids = paddle.randint(100, 200, shape=[1, 20])\nprint(model(input_ids))"}, {"cell_type": "code", "metadata": {}, "source": ["@misc{sanh2021multitask,\n", "title={Multitask Prompted Training Enables Zero-Shot Task Generalization},\n", "author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Stella Biderman and Leo Gao and Tali Bers and Thomas Wolf and Alexander M. Rush},\n", "year={2021},\n", "eprint={2110.08207},\n", "archivePrefix={arXiv},\n", "primaryClass={cs.LG}\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> 此模型来源于：[https://huggingface.co/bigscience/T0_3B](https://huggingface.co/https://huggingface.co/bigscience/T0_3B)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> 此模型来源于：[bigscience/T0_3B](https://huggingface.co/bigscience/T0_3B)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}