{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ade358",
   "metadata": {},
   "source": [
    "# Roberta Large Fine Tuned on RACE\n",
    "\n",
    "## Model description\n",
    "\n",
    "This model is a fine-tuned model of Roberta-large applied on RACE\n",
    "\n",
    "#### How to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"LIAMF-USP/roberta-large-finetuned-race\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd948e",
   "metadata": {},
   "source": [
    "\n",
    "## Training data\n",
    "\n",
    "The initial model was [roberta large model](https://huggingface.co/roberta-large) which was then fine-tuned on [RACE dataset](https://www.cs.cmu.edu/~glai1/data/race/)\n",
    "\n",
    "## Training procedure\n",
    "\n",
    "It was necessary to preprocess the data with a method that is exemplified for a single instance in the _How to use_ section. The used hyperparameters were the following:\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|:----:|:----:|\n",
    "| adam_beta1                  | 0.9      |\n",
    "| adam_beta2                  | 0.98     |\n",
    "| adam_epsilon                | 1.000e-8 |\n",
    "| eval_batch_size             | 32       |\n",
    "| train_batch_size            | 1        |\n",
    "| fp16                        | True     |\n",
    "| gradient_accumulation_steps | 16       |\n",
    "| learning_rate               | 0.00001  |\n",
    "| warmup_steps                | 1000     |\n",
    "| max_length                  | 512      |\n",
    "| epochs                      | 4        |\n",
    "\n",
    "## Eval results:\n",
    "| Dataset Acc | Eval | All Test |High School Test |Middle School Test |\n",
    "|:----:|:----:|:----:|:----:|:----:|\n",
    "|      | 85.2 | 84.9|83.5|88.0|\n",
    "\n",
    "**The model was trained with a Tesla V100-PCIE-16GB**\n",
    "> 此模型来源于：[https://huggingface.co/LIAMF-USP/roberta-large-finetuned-race](https://huggingface.co/https://huggingface.co/LIAMF-USP/roberta-large-finetuned-race)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
