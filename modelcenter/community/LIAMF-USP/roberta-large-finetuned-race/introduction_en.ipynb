{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c2ab41",
   "metadata": {},
   "source": [
    "# Roberta Large Fine Tuned on RACE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20498909",
   "metadata": {},
   "source": [
    "## Model description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c4194",
   "metadata": {},
   "source": [
    "This model is a fine-tuned model of Roberta-large applied on RACE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b674c93",
   "metadata": {},
   "source": [
    "#### How to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9235c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"LIAMF-USP/roberta-large-finetuned-race\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a8086",
   "metadata": {},
   "source": [
    "## Training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5be74",
   "metadata": {},
   "source": [
    "The initial model was [roberta large model](https://huggingface.co/roberta-large) which was then fine-tuned on [RACE dataset](https://www.cs.cmu.edu/~glai1/data/race/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc615b4c",
   "metadata": {},
   "source": [
    "## Training procedure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84343741",
   "metadata": {},
   "source": [
    "It was necessary to preprocess the data with a method that is exemplified for a single instance in the _How to use_ section. The used hyperparameters were the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b72b6",
   "metadata": {},
   "source": [
    "| Hyperparameter | Value |\n",
    "|:----:|:----:|\n",
    "| adam_beta1                  | 0.9      |\n",
    "| adam_beta2                  | 0.98     |\n",
    "| adam_epsilon                | 1.000e-8 |\n",
    "| eval_batch_size             | 32       |\n",
    "| train_batch_size            | 1        |\n",
    "| fp16                        | True     |\n",
    "| gradient_accumulation_steps | 16       |\n",
    "| learning_rate               | 0.00001  |\n",
    "| warmup_steps                | 1000     |\n",
    "| max_length                  | 512      |\n",
    "| epochs                      | 4        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f1342",
   "metadata": {},
   "source": [
    "## Eval results:\n",
    "| Dataset Acc | Eval | All Test |High School Test |Middle School Test |\n",
    "|:----:|:----:|:----:|:----:|:----:|\n",
    "|      | 85.2 | 84.9|83.5|88.0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd1fb1",
   "metadata": {},
   "source": [
    "**The model was trained with a Tesla V100-PCIE-16GB**\n",
    "> 此模型来源于：[https://huggingface.co/LIAMF-USP/roberta-large-finetuned-race](https://huggingface.co/https://huggingface.co/LIAMF-USP/roberta-large-finetuned-race)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
