{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# GPT-2 Medium\n", "\n", "## Model Details\n", "\n", "**Model Description:** GPT-2 Medium is the **355M parameter** version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.\n", "\n", "- **Developed by:** OpenAI, see [associated research paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [GitHub repo](https://github.com/openai/gpt-2) for model developers.\n", "- **Model Type:** Transformer-based language model\n", "- **Language(s):** English\n", "- **License:** [Modified MIT License](https://github.com/openai/gpt-2/blob/master/LICENSE)\n", "- **Related Models:** [GPT2](https://huggingface.co/gpt2), [GPT2-Large](https://huggingface.co/gpt2-large) and [GPT2-XL](https://huggingface.co/gpt2-xl)\n", "- **Resources for more information:**\n", "- [Research Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n", "- [OpenAI Blog Post](https://openai.com/blog/better-language-models/)\n", "- [GitHub Repo](https://github.com/openai/gpt-2)\n", "- [OpenAI Model Card for GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md)\n", "- Test the full generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n", "\n", "## How to Get Started with the Model\n", "\n", "Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\n", "set a seed for reproducibility:\n"]}, {"cell_type": "code", "metadata": {}, "source": "import paddle\nfrom paddlenlp.transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"gpt2-medium\")\ninput_ids = paddle.randint(100, 200, shape=[1, 20])\nprint(model(input_ids))"}, {"cell_type": "code", "metadata": {}, "source": ["@article{radford2019language,\n", "title={Language models are unsupervised multitask learners},\n", "author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},\n", "journal={OpenAI blog},\n", "volume={1},\n", "number={8},\n", "pages={9},\n", "year={2019}\n", "}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Card Authors\n", "\n", "This model card was written by the Hugging Face team.\n", "> 此模型来源于：[https://huggingface.co/gpt2-medium](https://huggingface.co/https://huggingface.co/gpt2-medium)"]}, {"cell_type": "markdown", "metadata": {}, "source": "> 此模型来源于：[gpt2-medium](https://huggingface.co/gpt2-medium)"}], "metadata": {"language_info": {"name": "python"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}