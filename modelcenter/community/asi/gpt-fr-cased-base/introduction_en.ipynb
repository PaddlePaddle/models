{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48402a3",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/AntoineSimoulin/gpt-fr/main/imgs/logo.png\" width=\"200\">\n",
    "\n",
    "## Model description\n",
    "\n",
    "**GPT-fr** üá´üá∑ is a GPT model for French developped by [Quantmetry](https://www.quantmetry.com/) and the [Laboratoire de Linguistique Formelle (LLF)](http://www.llf.cnrs.fr/en). We train the model on a very large and heterogeneous French corpus. We release the weights for the following configurations:\n",
    "\n",
    "| Model name | Number of layers | Attention Heads | Embedding Dimension | Total Parameters |\n",
    "| :------:       |   :---: | :---: | :---: | :---: |\n",
    "| `gpt-fr-cased-small` | 12    | 12    | 768   | 124 M |\n",
    "| `gpt-fr-cased-base` | 24    | 14    | 1,792   | 1,017 B |\n",
    "\n",
    "## Intended uses & limitations\n",
    "\n",
    "The model can be leveraged for language generation tasks. Besides, many tasks may be formatted such that the output is directly generated in natural language. Such configuration may be used for tasks such as automatic summary or question answering. We do hope our model might be used for both academic and industrial applications.\n",
    "\n",
    "#### How to use\n",
    "\n",
    "The model might be used through the astonishing ü§ó `Transformers` librairie. We use the work from [Shoeybi et al., (2019)](#shoeybi-2019) and calibrate our model such that during pre-training or fine-tuning, the model can fit on a single NVIDIA V100 32GB GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7967897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8c3ec",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "><div name=\"tiedemann-2012\">J√∂rg Tiedemann: Parallel Data, Tools and Interfaces in OPUS. LREC 2012: 2214-2218</div>\n",
    "\n",
    "><div name=\"li-2019\">Xian Li, Paul Michel, Antonios Anastasopoulos, Yonatan Belinkov, Nadir Durrani, Orhan Firat, Philipp Koehn, Graham Neubig, Juan Pino, Hassan Sajjad: Findings of the First Shared Task on Machine Translation Robustness. WMT (2) 2019: 91-102</div>\n",
    "\n",
    "><div name=\"shoeybi-2019\">Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. CoRR abs/1909.08053 (2019)</div>\n",
    "\n",
    "><div name=\"lacoste-2019\">Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, Thomas Dandres: Quantifying the Carbon Emissions of Machine Learning. CoRR abs/1910.09700 (2019)</div>\n",
    "\n",
    "\n",
    "> Ê≠§Ê®°ÂûãÊù•Ê∫ê‰∫éÔºö[https://huggingface.co/asi/gpt-fr-cased-base](https://huggingface.co/https://huggingface.co/asi/gpt-fr-cased-base)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
