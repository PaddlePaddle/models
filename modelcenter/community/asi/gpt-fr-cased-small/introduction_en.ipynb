{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69902eee",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/AntoineSimoulin/gpt-fr/main/imgs/logo.png\" width=\"200\">\n",
    "\n",
    "## Model description\n",
    "\n",
    "**GPT-fr** üá´üá∑ is a GPT model for French developped by [Quantmetry](https://www.quantmetry.com/) and the [Laboratoire de Linguistique Formelle (LLF)](http://www.llf.cnrs.fr/en). We train the model on a very large and heterogeneous French corpus. We release the weights for the following configurations:\n",
    "\n",
    "| Model name | Number of layers | Attention Heads | Embedding Dimension | Total Parameters |\n",
    "| :------:       |   :---: | :---: | :---: | :---: |\n",
    "| `gpt-fr-cased-small` | 12    | 12    | 768   | 124 M |\n",
    "| `gpt-fr-cased-base` | 24    | 14    | 1,792   | 1,017 B |\n",
    "\n",
    "## Intended uses & limitations\n",
    "\n",
    "The model can be leveraged for language generation tasks. Besides, many tasks may be formatted such that the output is directly generated in natural language. Such configuration may be used for tasks such as automatic summary or question answering. We do hope our model might be used for both academic and industrial applications.\n",
    "\n",
    "#### How to use\n",
    "\n",
    "The model might be used through the astonishing ü§ó `Transformers` librairie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef607aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"asi/gpt-fr-cased-small\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c929b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inproceedings{simoulin:hal-03265900,\n",
    "TITLE = {{Un mod{\\`e}le Transformer G{\\'e}n{\\'e}ratif Pr{\\'e}-entrain{\\'e} pour le \\_\\_\\_\\_\\_\\_ fran{\\c c}ais}},\n",
    "AUTHOR = {Simoulin, Antoine and Crabb{\\'e}, Benoit},\n",
    "URL = {https://hal.archives-ouvertes.fr/hal-03265900},\n",
    "BOOKTITLE = {{Traitement Automatique des Langues Naturelles}},\n",
    "ADDRESS = {Lille, France},\n",
    "EDITOR = {Denis, Pascal and Grabar, Natalia and Fraisse, Amel and Cardon, R{\\'e}mi and Jacquemin, Bernard and Kergosien, Eric and Balvet, Antonio},\n",
    "PUBLISHER = {{ATALA}},\n",
    "PAGES = {246-255},\n",
    "YEAR = {2021},\n",
    "KEYWORDS = {fran{\\c c}ais. ; GPT ; G{\\'e}n{\\'e}ratif ; Transformer ; Pr{\\'e}-entra{\\^i}n{\\'e}},\n",
    "PDF = {https://hal.archives-ouvertes.fr/hal-03265900/file/7.pdf},\n",
    "HAL_ID = {hal-03265900},\n",
    "HAL_VERSION = {v1},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573682f9",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "><div name=\"tiedemann-2012\">J√∂rg Tiedemann: Parallel Data, Tools and Interfaces in OPUS. LREC 2012: 2214-2218</div>\n",
    "> Ê≠§Ê®°ÂûãÊù•Ê∫ê‰∫éÔºö[https://huggingface.co/asi/gpt-fr-cased-small](https://huggingface.co/https://huggingface.co/asi/gpt-fr-cased-small)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
