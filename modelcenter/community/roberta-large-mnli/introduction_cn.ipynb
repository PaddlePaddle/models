{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28607d1",
   "metadata": {},
   "source": [
    "# roberta-large-mnli\n",
    "\n",
    "## Table of Contents\n",
    "- [Model Details](#model-details)\n",
    "- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n",
    "- [Uses](#uses)\n",
    "- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n",
    "- [Training](#training)\n",
    "- [Evaluation](#evaluation-results)\n",
    "- [Environmental Impact](#environmental-impact)\n",
    "- [Technical Specifications](#technical-specifications)\n",
    "- [Citation Information](#citation-information)\n",
    "- [Model Card Authors](#model-card-author)\n",
    "\n",
    "## Model Details\n",
    "\n",
    "**Model Description:** roberta-large-mnli is the [RoBERTa large model](https://huggingface.co/roberta-large) fine-tuned on the [Multi-Genre Natural Language Inference (MNLI)](https://huggingface.co/datasets/multi_nli) corpus. The model is a pretrained model on English language text using a masked language modeling (MLM) objective.\n",
    "\n",
    "- **Developed by:** See [GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/roberta) for model developers\n",
    "- **Model Type:** Transformer-based language model\n",
    "- **Language(s):** English\n",
    "- **License:** MIT\n",
    "- **Parent Model:** This model is a fine-tuned version of the RoBERTa large model. Users should see the [RoBERTa large model card](https://huggingface.co/roberta-large) for relevant information.\n",
    "- **Resources for more information:**\n",
    "- [Research Paper](https://arxiv.org/abs/1907.11692)\n",
    "- [GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/roberta)\n",
    "\n",
    "## How to Get Started with the Model\n",
    "\n",
    "Use the code below to get started with the model. The model can be loaded with the zero-shot-classification pipeline like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c195b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"roberta-large-mnli\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d60f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@article{liu2019roberta,\n",
    "title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},\n",
    "author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and\n",
    "Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and\n",
    "Luke Zettlemoyer and Veselin Stoyanov},\n",
    "journal={arXiv preprint arXiv:1907.11692},\n",
    "year = {2019},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe12ef",
   "metadata": {},
   "source": [
    "> 此模型来源于：[https://huggingface.co/roberta-large-mnli](https://huggingface.co/https://huggingface.co/roberta-large-mnli)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
