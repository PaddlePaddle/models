{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a864492",
   "metadata": {},
   "source": [
    "# AlephBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bc0f5",
   "metadata": {},
   "source": [
    "## Hebrew Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530718d3",
   "metadata": {},
   "source": [
    "State-of-the-art language model for Hebrew.\n",
    "Based on Google's BERT architecture [(Devlin et al. 2018)](https://arxiv.org/abs/1810.04805).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec877abf",
   "metadata": {},
   "source": [
    "#### How to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a562486",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"onlplab/alephbert-base\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e362c2d",
   "metadata": {},
   "source": [
    "## Training data\n",
    "1. OSCAR [(Ortiz, 2019)](https://oscar-corpus.com/) Hebrew section (10 GB text, 20 million sentences).\n",
    "2. Hebrew dump of [Wikipedia](https://dumps.wikimedia.org/hewiki/latest/) (650 MB text, 3 million sentences).\n",
    "3. Hebrew Tweets collected from the Twitter sample stream (7 GB text, 70 million sentences).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf6b25",
   "metadata": {},
   "source": [
    "## Training procedure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81eb54",
   "metadata": {},
   "source": [
    "Trained on a DGX machine (8 V100 GPUs) using the standard huggingface training procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1dffd1",
   "metadata": {},
   "source": [
    "Since the larger part of our training data is based on tweets we decided to start by optimizing using Masked Language Model loss only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b5dcf",
   "metadata": {},
   "source": [
    "To optimize training time we split the data into 4 sections based on max number of tokens:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a309c",
   "metadata": {},
   "source": [
    "1. num tokens < 32 (70M sentences)\n",
    "2. 32 <= num tokens < 64 (12M sentences)\n",
    "3. 64 <= num tokens < 128 (10M sentences)\n",
    "4. 128 <= num tokens < 512 (1.5M sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d950d",
   "metadata": {},
   "source": [
    "Each section was first trained for 5 epochs with an initial learning rate set to 1e-4. Then each section was trained for another 5 epochs with an initial learning rate set to 1e-5, for a total of 10 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b7cbce",
   "metadata": {},
   "source": [
    "Total training time was 8 days.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d96b12",
   "metadata": {},
   "source": [
    "> 此模型来源于：[https://huggingface.co/onlplab/alephbert-base](https://huggingface.co/https://huggingface.co/onlplab/alephbert-base)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
