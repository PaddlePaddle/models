{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2efe788",
   "metadata": {},
   "source": [
    "# AlephBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c2153",
   "metadata": {},
   "source": [
    "## Hebrew Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82320b7",
   "metadata": {},
   "source": [
    "State-of-the-art language model for Hebrew.\n",
    "Based on Google's BERT architecture [(Devlin et al. 2018)](https://arxiv.org/abs/1810.04805).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d5b04",
   "metadata": {},
   "source": [
    "#### How to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c574e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"onlplab/alephbert-base\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925eae35",
   "metadata": {},
   "source": [
    "## Training data\n",
    "1. OSCAR [(Ortiz, 2019)](https://oscar-corpus.com/) Hebrew section (10 GB text, 20 million sentences).\n",
    "2. Hebrew dump of [Wikipedia](https://dumps.wikimedia.org/hewiki/latest/) (650 MB text, 3 million sentences).\n",
    "3. Hebrew Tweets collected from the Twitter sample stream (7 GB text, 70 million sentences).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be6fac",
   "metadata": {},
   "source": [
    "## Training procedure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e362d",
   "metadata": {},
   "source": [
    "Trained on a DGX machine (8 V100 GPUs) using the standard huggingface training procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc4d93",
   "metadata": {},
   "source": [
    "Since the larger part of our training data is based on tweets we decided to start by optimizing using Masked Language Model loss only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4556d6",
   "metadata": {},
   "source": [
    "To optimize training time we split the data into 4 sections based on max number of tokens:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94183cd5",
   "metadata": {},
   "source": [
    "1. num tokens < 32 (70M sentences)\n",
    "2. 32 <= num tokens < 64 (12M sentences)\n",
    "3. 64 <= num tokens < 128 (10M sentences)\n",
    "4. 128 <= num tokens < 512 (1.5M sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d6ffb",
   "metadata": {},
   "source": [
    "Each section was first trained for 5 epochs with an initial learning rate set to 1e-4. Then each section was trained for another 5 epochs with an initial learning rate set to 1e-5, for a total of 10 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0535e1",
   "metadata": {},
   "source": [
    "Total training time was 8 days.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ae04d",
   "metadata": {},
   "source": [
    "> 此模型来源于：[https://huggingface.co/onlplab/alephbert-base](https://huggingface.co/https://huggingface.co/onlplab/alephbert-base)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
