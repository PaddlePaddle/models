{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72047643",
   "metadata": {},
   "source": [
    "# DistilGPT2\n",
    "\n",
    "详细内容请看[GPT2 in PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/model_zoo/gpt/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c299c9",
   "metadata": {},
   "source": [
    "DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of [GPT-2](https://huggingface.co/gpt2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624b3d1",
   "metadata": {},
   "source": [
    "## Model Details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92002396",
   "metadata": {},
   "source": [
    "- **Developed by:** Hugging Face\n",
    "- **Model type:** Transformer-based Language Model\n",
    "- **Language:** English\n",
    "- **License:** Apache 2.0\n",
    "- **Model Description:** DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters, was developed using [knowledge distillation](#knowledge-distillation) and was designed to be a faster, lighter version of GPT-2.\n",
    "- **Resources for more information:** See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including Distilled-GPT2), [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure, and this page for more about [GPT-2](https://openai.com/blog/better-language-models/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12fd82",
   "metadata": {},
   "source": [
    "## Uses, Limitations and Risks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff80410",
   "metadata": {},
   "source": [
    "#### Limitations and Risks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a246f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to expand</summary>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0445cdcc",
   "metadata": {},
   "source": [
    "**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07389413",
   "metadata": {},
   "source": [
    "As the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md), “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46e9d7",
   "metadata": {},
   "source": [
    "DistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63457c60",
   "metadata": {},
   "source": [
    "The impact of model compression techniques – such as knowledge distillation – on bias and fairness issues associated with language models is an active area of research. For example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5168f",
   "metadata": {},
   "source": [
    "- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.\n",
    "- [Xu and Hu (2022)](https://arxiv.org/pdf/2201.08542.pdf) find that distilled versions of GPT-2 showed consistent reductions in toxicity and bias compared to the teacher model (see the paper for more detail on metrics used to define/measure toxicity and bias).\n",
    "- [Gupta et al. (2022)](https://arxiv.org/pdf/2203.12574.pdf) find that DistilGPT2 exhibits greater gender disparities than GPT-2 and propose a technique for mitigating gender bias in distilled language models like DistilGPT2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a84778",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddlenlp.transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"distilgpt2\")\n",
    "input_ids = paddle.randint(100, 200, shape=[1, 20])\n",
    "print(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3d465",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "```\n",
    "@inproceedings{sanh2019distilbert,\n",
    "title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n",
    "author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n",
    "booktitle={NeurIPS EMC^2 Workshop},\n",
    "year={2019}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966636a",
   "metadata": {},
   "source": [
    "## Glossary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533038ef",
   "metadata": {},
   "source": [
    "-\t<a name=\"knowledge-distillation\">**Knowledge Distillation**</a>: As described in [Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf), “knowledge distillation is a compression technique in which a compact model – the student – is trained to reproduce the behavior of a larger model – the teacher – or an ensemble of models.” Also see [Bucila et al. (2006)](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf) and [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff7cc1",
   "metadata": {},
   "source": [
    "<a href=\"https://huggingface.co/exbert/?model=distilgpt2\">\n",
    "<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n",
    "</a>\n",
    "> 此模型来源于：[https://huggingface.co/distilgpt2](https://huggingface.co/distilgpt2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
