{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03cfffa3-2398-4d55-bf1e-fe3e01f10d68",
   "metadata": {},
   "source": [
    "## 1. SegmentAnything with PaddleSeg introduction\n",
    "\n",
    "\n",
    "We implemente the segment anything with the PaddlePaddle framework. Segment Anything Model (SAM) is a new task, model, and dataset for image segmentation. It can produce high quality object masks from different types of prompts including points, boxes, masks and text. Further, SAM can generate masks for all objects in whole image. It built a largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. SAM has impressive zero-shot performance on a variety of tasks, even often competitive with or even superior to prior fully supervised results.\n",
    "\n",
    "More details can be found in the page: [https://ai.facebook.com/research/publications/segment-anything/](https://ai.facebook.com/research/publications/segment-anything/).\n",
    "\n",
    "More about SegmentAnything，you can click [https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.8/contrib/SegmentAnything](https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.8/contrib/SegmentAnything) to learn.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cbcf510-dc56-43f9-9864-5e1de3c7b272",
   "metadata": {},
   "source": [
    "## 2. Model Effects and Application Scenarios\n",
    "The street scene effects of SegmentAnything are as follows:\n",
    "<p align=\"center\">\n",
    "<img src=\"https://user-images.githubusercontent.com/18344247/231054088-b4ebfb05-ebf6-42fd-ad84-1537101d7245.gif\" width=\"100%\" height=\"100%\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9da224-edd4-4c9e-ab2d-cba7ee5a92a4",
   "metadata": {},
   "source": [
    "## 3. How to Use the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ac57be7-c00f-441e-ad82-635f6268b6bd",
   "metadata": {},
   "source": [
    "\n",
    " * Clone PaddleSeg repository（see 3.1 for details）。\n",
    " \n",
    " More details for training please refer the document in Matting folder under PaddleSeg repository.\n",
    "https://github.com/PaddlePaddle/PaddleSeg/tree/develop/Matting 。\n",
    "### 3.1 Model Inference\n",
    "* Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8bf9a98",
   "metadata": {},
   "source": [
    "Install PaddlePaddle and relative environments based on the [installation guide](https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/install/pip/linux-pip_en.html).\n",
    "Install PaddleSeg based on the [reference](../../docs/install.md).\n",
    "Clone the PaddleSeg reporitory:\n",
    "Download the example image to ```contrib/SegmentAnything/examples```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea22cb4-5bed-4ce0-858b-3bbb342e8ccf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "!git clone https://github.com/PaddlePaddle/PaddleSeg.git\n",
    "%cd ~/PaddleSeg\n",
    "!git checkout release/2.8\n",
    "!pip install -v -e .\n",
    "%cd ~/PaddleSeg/contrib/SegmentAnything/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0dd2390-e635-432b-9e0b-d7c9323f81a9",
   "metadata": {},
   "source": [
    "* Quick experience\n",
    "\n",
    "Congratulations! Now that you've successfully installed PaddleSeg, let's get a quick feel at image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81736dd2-e90d-4dac-b3c0-58424e2e8dc4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download pretrained model\n",
    "!wget https://paddleseg.bj.bcebos.com/matting/models/ppmatting-hrnet_w18-human_512.pdparams\n",
    "# Download image\n",
    "!wget https://user-images.githubusercontent.com/30919197/200645066-6898ec5a-f1c5-4bf7-aa41-473a29977a86.jpeg\n",
    "# Predicd one image in GPU\n",
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "!python tools/predict.py \\\n",
    "    --config configs/ppmatting/ppmatting-hrnet_w18-human_512.yml \\\n",
    "    --model_path ppmatting-hrnet_w18-human_512.pdparams \\\n",
    "    --image_path 200645066-6898ec5a-f1c5-4bf7-aa41-473a29977a86.jpeg \\\n",
    "    --save_dir ./output/results \\\n",
    "    --fg_estimate True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9b05c1e-cb59-4dd7-83c0-cb30dfefba60",
   "metadata": {},
   "source": [
    "## 4. Model Principles\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://user-images.githubusercontent.com/18344247/231053823-5d291477-dfd8-4eba-a4b7-611be57e541e.png\"  width = \"80%\"  />\n",
    "* </div>\n",
    "\n",
    "* Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can\n",
    "then be efficiently queried by a variety of input prompts to produce object masks at amortized real-time speed. \n",
    "* For ambiguous prompts corresponding to more than one object, SAM can output multiple valid masks and associated confidence scores\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e238357-2f19-48c3-902e-9ba3c93f7818",
   "metadata": {},
   "source": [
    "## 5. Related papers and citations\n",
    "```\n",
    "@article{kirillov2023segment,\n",
    "  title={Segment Anything},\n",
    "  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},\n",
    "  journal={arXiv preprint arXiv:2304.02643},\n",
    "  year={2023}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
