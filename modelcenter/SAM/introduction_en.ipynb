{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03cfffa3-2398-4d55-bf1e-fe3e01f10d68",
   "metadata": {},
   "source": [
    "## 1. SegmentAnything with PaddleSeg introduction\n",
    "\n",
    "\n",
    "We implemente the segment anything with the PaddlePaddle framework. Segment Anything Model (SAM) is a new task, model, and dataset for image segmentation. It can produce high quality object masks from different types of prompts including points, boxes, masks and text. Further, SAM can generate masks for all objects in whole image. It built a largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. SAM has impressive zero-shot performance on a variety of tasks, even often competitive with or even superior to prior fully supervised results.\n",
    "\n",
    "More details can be found in the page: [https://ai.facebook.com/research/publications/segment-anything/](https://ai.facebook.com/research/publications/segment-anything/).\n",
    "\n",
    "More about SegmentAnything with PaddleSegï¼Œyou can click [https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.8/contrib/SegmentAnything](https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.8/contrib/SegmentAnything) to learn.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cbcf510-dc56-43f9-9864-5e1de3c7b272",
   "metadata": {},
   "source": [
    "## 2. Model Effects and Application Scenarios\n",
    "The street scene effects of SegmentAnything are as follows:\n",
    "<p align=\"center\">\n",
    "<img src=\"https://user-images.githubusercontent.com/18344247/231054088-b4ebfb05-ebf6-42fd-ad84-1537101d7245.gif\" width=\"100%\" height=\"100%\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9da224-edd4-4c9e-ab2d-cba7ee5a92a4",
   "metadata": {},
   "source": [
    "## 3. How to Use the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ac57be7-c00f-441e-ad82-635f6268b6bd",
   "metadata": {},
   "source": [
    "### 3.1 Model Inference\n",
    "\n",
    "* Install PaddlePaddle\n",
    "\n",
    "To install PaddlePaddle, PaddlePaddle>=2.2.0 is required. Since the image segmentation model is computationally expensive, it is recommended to use the GPU version of PaddlePaddle.\n",
    "\n",
    "In AI Studio, you can directly choose the environment where PaddlePaddle is installed. If you need to install PaddlePaddle, please refer to the PaddlePaddle official website [installation guide](https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/install/pip/linux-pip_en.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8bf9a98",
   "metadata": {},
   "source": [
    "* Download and Install PaddleSeg \n",
    "\n",
    "(When not running on Jupyter Notebook, \"!\" Or \"%\" should be removed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea22cb4-5bed-4ce0-858b-3bbb342e8ccf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "!git clone https://github.com/PaddlePaddle/PaddleSeg.git\n",
    "# Install\n",
    "%cd ~/PaddleSeg\n",
    "!git checkout release/2.8\n",
    "!pip install -v -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0dd2390-e635-432b-9e0b-d7c9323f81a9",
   "metadata": {},
   "source": [
    "* Quick experience\n",
    "\n",
    "Congratulations! Now that you've successfully installed PaddleSeg, let's get a quick feel at image segmentation based on Segment Anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81736dd2-e90d-4dac-b3c0-58424e2e8dc4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to SegmentAnything\n",
    "%cd ~/PaddleSeg/contrib/SegmentAnything/\n",
    "# Download the example image\n",
    "!wget https://paddleseg.bj.bcebos.com/dygraph/demo/cityscapes_demo.png\n",
    "\n",
    "# Predicd one image\n",
    "!python script/amg_paddle.py --input_path cityscapes_demo.png --model-type [vit_l/vit_b/vit_h] # default is vit_h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44b111b5",
   "metadata": {},
   "source": [
    "Open the webpage on your localhost: ```http://0.0.0.0:8017```\n",
    "\n",
    "Try it out by clear and upload the test image! Our example looks like:\n",
    "<div align=\"center\">\n",
    "<img src=\"https://user-images.githubusercontent.com/34859558/230873989-9597527e-bef6-47ce-988b-977198794d75.jpg\"  width = \"80%\"  />\n",
    "* </div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce014577",
   "metadata": {},
   "source": [
    "More Details please refer the document in SegmentAnything folder under PaddleSeg repository.\n",
    "https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.8/contrib/SegmentAnything"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9b05c1e-cb59-4dd7-83c0-cb30dfefba60",
   "metadata": {},
   "source": [
    "## 4. Model Principles\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://user-images.githubusercontent.com/18344247/231053823-5d291477-dfd8-4eba-a4b7-611be57e541e.png\"  width = \"80%\"  />\n",
    "* </div>\n",
    "\n",
    "* Segment Anything Model (SAM) consists of three models, including image encoder, prompt encoder and mask dercoder. \n",
    "* Image Encoder: A heavyweight image encoder outputs an image embedding.\n",
    "* Prompt Encoder: flexible encodes the sparse (points, boxes, text) and dense (masks) prompts.\n",
    "* Mask Decoder: efficiently maps the image embedding, prompt embeddings, and output tokens to masks.\n",
    "* SAM built and used a largest segmentation dataset to date (by far) for training, with over 1 billion masks on 11M licensed and privacy respecting images. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e238357-2f19-48c3-902e-9ba3c93f7818",
   "metadata": {},
   "source": [
    "## 5. Related papers and citations\n",
    "```\n",
    "@article{kirillov2023segment,\n",
    "  title={Segment Anything},\n",
    "  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},\n",
    "  journal={arXiv preprint arXiv:2304.02643},\n",
    "  year={2023}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
