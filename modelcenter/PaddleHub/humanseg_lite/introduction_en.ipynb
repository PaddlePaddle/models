{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f89270",
   "metadata": {
    "id": "humanseg_lite"
   },
   "source": [
    "# humanseg_lite\n",
    "\n",
    "|Module Name |humanseg_lite|\n",
    "| :--- | :---: |\n",
    "|Category |Image segmentation|\n",
    "|Network|shufflenet|\n",
    "|Dataset|Baidu self-built dataset|\n",
    "|Fine-tuning supported or not|No|\n",
    "|Module Size|541k|\n",
    "|Data indicators|-|\n",
    "|Latest update date|2021-02-26|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76f44e",
   "metadata": {
    "id": "i-basic-information"
   },
   "source": [
    "## I. Basic Information\n",
    "\n",
    "- ### Application Effect Display\n",
    "\n",
    "  - Sample results:\n",
    "    <p align=\"center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/35907364/130913092-312a5f37-842e-4fd0-8db4-5f853fd8419f.jpg\" width=\"337\" height=\"505\" hspace=\"10\"/> <img src=\"https://user-images.githubusercontent.com/35907364/130916087-7d537ad9-bbc8-4bce-9382-8eb132b35532.png\" width=\"337\" height=\"505\" hspace=\"10\"/>\n",
    "    </p>\n",
    "\n",
    "- ### Module Introduction\n",
    "\n",
    "    - HumanSeg_lite is based on ShuffleNetV2 network. The network size is only 541K. It is suitable for selfie portrait segmentation and can be segmented in real time on the mobile terminal.\n",
    "\n",
    "    - For more information, please refer to:[humanseg_lite](https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.2/contrib/HumanSeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3d4f7",
   "metadata": {
    "id": "ii-installation"
   },
   "source": [
    "## II. Installation\n",
    "\n",
    "- ### 1、Environmental Dependence\n",
    "\n",
    "    - paddlepaddle >= 2.0.0\n",
    "\n",
    "    - paddlehub >= 2.0.0\n",
    "\n",
    "- ### 2、Installation\n",
    "\n",
    "    - ```shell\n",
    "      $ hub install humanseg_lite\n",
    "      ```\n",
    "\n",
    "    - In case of any problems during installation, please refer to:[Windows_Quickstart](../../../../docs/docs_en/get_start/windows_quickstart.md)\n",
    "    | [Linux_Quickstart](../../../../docs/docs_en/get_start/linux_quickstart.md) | [Mac_Quickstart](../../../../docs/docs_en/get_start/mac_quickstart.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43440baf",
   "metadata": {
    "id": "iii-module-api-prediction"
   },
   "source": [
    "## III. Module API Prediction\n",
    "\n",
    "- ### 1、Command line Prediction\n",
    "\n",
    "    - ```\n",
    "      hub run humanseg_lite --input_path \"/PATH/TO/IMAGE\"\n",
    "\n",
    "      ```\n",
    "\n",
    "    - If you want to call the Hub module through the command line, please refer to: [PaddleHub Command Line Instruction](../../../../docs/docs_en/tutorial/cmd_usage.rst)\n",
    "\n",
    "- ### 2、Prediction Code Example\n",
    "    - Image segmentation and video segmentation example：\n",
    "      - ```python\n",
    "        import cv2\n",
    "        import paddlehub as hub\n",
    "\n",
    "        human_seg = hub.Module(name='humanseg_lite')\n",
    "        im = cv2.imread('/PATH/TO/IMAGE')\n",
    "        res = human_seg.segment(images=[im],visualization=True)\n",
    "        print(res[0]['data'])\n",
    "        human_seg.video_segment('/PATH/TO/VIDEO')\n",
    "        ```\n",
    "    - Video prediction example:\n",
    "\n",
    "      - ```python\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        import paddlehub as hub\n",
    "\n",
    "        human_seg = hub.Module('humanseg_lite')\n",
    "        cap_video = cv2.VideoCapture('\\PATH\\TO\\VIDEO')\n",
    "        fps = cap_video.get(cv2.CAP_PROP_FPS)\n",
    "        save_path = 'humanseg_lite_video.avi'\n",
    "        width = int(cap_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        cap_out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), fps, (width, height))\n",
    "        prev_gray = None\n",
    "        prev_cfd = None\n",
    "        while cap_video.isOpened():\n",
    "            ret, frame_org = cap_video.read()\n",
    "            if ret:\n",
    "                [img_matting, prev_gray, prev_cfd] = human_seg.video_stream_segment(frame_org=frame_org, frame_id=cap_video.get(1), prev_gray=prev_gray, prev_cfd=prev_cfd)\n",
    "                img_matting = np.repeat(img_matting[:, :, np.newaxis], 3, axis=2)\n",
    "                bg_im = np.ones_like(img_matting) * 255\n",
    "                comb = (img_matting * frame_org + (1 - img_matting) * bg_im).astype(np.uint8)\n",
    "                cap_out.write(comb)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap_video.release()\n",
    "        cap_out.release()\n",
    "\n",
    "        ```\n",
    "\n",
    "- ### 3、API\n",
    "\n",
    "    - ```python\n",
    "      def segment(images=None,\n",
    "                paths=None,\n",
    "                batch_size=1,\n",
    "                use_gpu=False,\n",
    "                visualization=False,\n",
    "                output_dir='humanseg_lite_output')\n",
    "      ```\n",
    "\n",
    "        - Prediction API, generating segmentation result.\n",
    "\n",
    "        - **Parameter**\n",
    "\n",
    "            * images (list\\[numpy.ndarray\\]): image data, ndarray.shape is in the format [H, W, C], BGR.\n",
    "            * paths (list\\[str\\]): image path.\n",
    "            * batch\\_size (int): batch size.\n",
    "            * use\\_gpu (bool): use GPU or not. **set the CUDA_VISIBLE_DEVICES environment variable first if you are using GPU**\n",
    "            * visualization (bool): Whether to save the results as picture files.\n",
    "            * output\\_dir (str): save path of images, humanseg_lite_output by default.\n",
    "\n",
    "        - **Return**\n",
    "\n",
    "            * res (list\\[dict\\]): The list of recognition results, where each element is dict and each field is:\n",
    "                * save\\_path (str, optional): Save path of the result.\n",
    "                * data (numpy.ndarray): The result of portrait segmentation.\n",
    "\n",
    "    - ```python\n",
    "      def video_stream_segment(self,\n",
    "                            frame_org,\n",
    "                            frame_id,\n",
    "                            prev_gray,\n",
    "                            prev_cfd,\n",
    "                            use_gpu=False):\n",
    "      ```\n",
    "        -  Prediction API, used to segment video portraits frame by frame.\n",
    "\n",
    "        - **Parameter**\n",
    "\n",
    "            * frame_org (numpy.ndarray): single frame for prediction，ndarray.shape is in the format [H, W, C], BGR.\n",
    "            * frame_id (int): The number of the current frame.\n",
    "            * prev_gray (numpy.ndarray): Grayscale image of the previous network input.\n",
    "            * prev_cfd (numpy.ndarray): The fusion image from optical flow and the prediction result from previous frame.\n",
    "            * use\\_gpu (bool): use GPU or not. **set the CUDA_VISIBLE_DEVICES environment variable first if you are using GPU**\n",
    "\n",
    "        - **Return**\n",
    "\n",
    "            * img_matting (numpy.ndarray): The result of portrait segmentation.\n",
    "            * cur_gray (numpy.ndarray): Grayscale image of the current network input.\n",
    "            * optflow_map (numpy.ndarray): The fusion image from optical flow and the prediction result from current frame.\n",
    "\n",
    "    - ```python\n",
    "      def video_segment(self,\n",
    "                        video_path=None,\n",
    "                        use_gpu=False,\n",
    "                        save_dir='humanseg_lite_video_result'):\n",
    "        ```\n",
    "\n",
    "        -  Prediction API to produce video segmentation result.\n",
    "\n",
    "        - **Parameter**\n",
    "\n",
    "            * video\\_path (str): Video path for segmentation。If None, the video will be obtained from the local camera, and a window will display the online segmentation result.\n",
    "            * use\\_gpu (bool): use GPU or not. **set the CUDA_VISIBLE_DEVICES environment variable first if you are using GPU**\n",
    "            * save\\_dir (str): save path of video.\n",
    "\n",
    "    -  ```python\n",
    "       def save_inference_model(dirname)\n",
    "       ```\n",
    "\n",
    "        - Save the model to the specified path.\n",
    "\n",
    "        - **Parameters**\n",
    "\n",
    "            * dirname: Model save path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b1bbf",
   "metadata": {
    "id": "iv-server-deployment"
   },
   "source": [
    "## IV. Server Deployment\n",
    "\n",
    "- PaddleHub Serving can deploy an online service of for human segmentation.\n",
    "\n",
    "- ### Step 1: Start PaddleHub Serving\n",
    "\n",
    "    - Run the startup command:\n",
    "\n",
    "        - ```shell\n",
    "          hub serving start -m humanseg_lite\n",
    "          ```\n",
    "\n",
    "    - The servitization API is now deployed and the default port number is 8866.\n",
    "\n",
    "    - **NOTE:**  If GPU is used for prediction, set CUDA_VISIBLE_DEVICES environment variable before the service, otherwise it need not be set.\n",
    "\n",
    "- ### Step 2: Send a predictive request\n",
    "\n",
    "    - With a configured server, use the following lines of code to send the prediction request and obtain the result\n",
    "\n",
    "        -   ```python\n",
    "            import requests\n",
    "            import json\n",
    "            import base64\n",
    "\n",
    "            import cv2\n",
    "            import numpy as np\n",
    "\n",
    "            def cv2_to_base64(image):\n",
    "                data = cv2.imencode('.jpg', image)[1]\n",
    "                return base64.b64encode(data.tostring()).decode('utf8')\n",
    "            def base64_to_cv2(b64str):\n",
    "                data = base64.b64decode(b64str.encode('utf8'))\n",
    "                data = np.fromstring(data, np.uint8)\n",
    "                data = cv2.imdecode(data, cv2.IMREAD_COLOR)\n",
    "                return data\n",
    "\n",
    "            # Send an HTTP request\n",
    "            org_im = cv2.imread('/PATH/TO/IMAGE')\n",
    "            data = {'images':[cv2_to_base64(org_im)]}\n",
    "            headers = {\"Content-type\": \"application/json\"}\n",
    "            url = \"http://127.0.0.1:8866/predict/humanseg_lite\"\n",
    "            r = requests.post(url=url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "            mask =cv2.cvtColor(base64_to_cv2(r.json()[\"results\"][0]['data']), cv2.COLOR_BGR2GRAY)\n",
    "            rgba = np.concatenate((org_im, np.expand_dims(mask, axis=2)), axis=2)\n",
    "            cv2.imwrite(\"segment_human_lite.png\", rgba)\n",
    "            ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9cb6a",
   "metadata": {
    "id": "v-release-note"
   },
   "source": [
    "## V. Release Note\n",
    "\n",
    "- 1.0.0\n",
    "\n",
    "    First release\n",
    "\n",
    "- 1.1.0\n",
    "\n",
    "    Added video portrait segmentation interface\n",
    "\n",
    "    Added video stream portrait segmentation interface\n",
    "\n",
    "* 1.1.1\n",
    "\n",
    "    Fix memory leakage problem of on cudnn 8.0.4\n",
    "\n",
    "* 1.2.0\n",
    "\n",
    "    Remove Fluid API\n",
    "\n",
    "    ```shell\n",
    "    $ hub install humanseg_lite == 1.2.0\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "humanseg_lite",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
