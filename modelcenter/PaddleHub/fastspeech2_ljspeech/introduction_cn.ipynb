{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419c5b55",
   "metadata": {
    "id": "fastspeech2_ljspeech"
   },
   "source": [
    "# fastspeech2_ljspeech\n",
    "\n",
    "|模型名称|fastspeech2_ljspeech|\n",
    "| :--- | :---: |\n",
    "|类别|语音-语音合成|\n",
    "|网络|FastSpeech2|\n",
    "|数据集|LJSpeech-1.1|\n",
    "|是否支持Fine-tuning|否|\n",
    "|模型大小|425MB|\n",
    "|最新更新日期|2021-10-20|\n",
    "|数据指标|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a4407",
   "metadata": {
    "id": "一-模型基本信息"
   },
   "source": [
    "## 一、模型基本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cbda8e",
   "metadata": {
    "id": "模型介绍"
   },
   "source": [
    "### 模型介绍\n",
    "\n",
    "FastSpeech2是微软亚洲研究院和微软Azure语音团队联合浙江大学于2020年提出的语音合成(Text to Speech, TTS)模型。FastSpeech2是FastSpeech的改进版，解决了FastSpeech依赖Teacher-Student的知识蒸馏框架，训练流程比较复杂和训练目标相比真实语音存在信息损失的问题。\n",
    "\n",
    "FastSpeech2的模型架构如下图所示，它沿用FastSpeech中提出的Feed-Forward Transformer(FFT)架构，但在音素编码器和梅尔频谱解码器中加入了一个可变信息适配器(Variance Adaptor)，从而支持在FastSpeech2中引入更多语音中变化的信息，例如时长、音高、音量(频谱能量)等，来解决语音合成中的一对多映射问题。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/images/fastspeech2.png\" hspace=\"10\"/> <br/>\n",
    "</p>\n",
    "\n",
    "Parallel WaveGAN是一种使用了无蒸馏的对抗生成网络，快速且占用空间小的波形生成方法。该方法通过联合优化多分辨率谱图和对抗损失函数来训练非自回归WaveNet，可以有效捕获真实语音波形的时频分布。Parallel WaveGAN的结构如下图所示：\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://paddlespeech.bj.bcebos.com/Parakeet/docs/images/pwg.png\" hspace=\"10\"/> <br/>\n",
    "</p>\n",
    "\n",
    "fastspeech2_ljspeech使用了FastSpeech2作为声学模型，使用Parallel WaveGAN作为声码器，并在[The LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/)数据集上进行了预训练，可直接用于预测合成音频。\n",
    "\n",
    "更多详情请参考:\n",
    "- [FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech](https://arxiv.org/abs/2006.04558)\n",
    "- [FastSpeech语音合成系统技术升级，微软联合浙大提出FastSpeech2](https://www.msra.cn/zh-cn/news/features/fastspeech2)\n",
    "- [Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram](https://arxiv.org/abs/1910.11480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383de02",
   "metadata": {
    "id": "二-安装"
   },
   "source": [
    "## 二、安装\n",
    "\n",
    "- ### 1、环境依赖\n",
    "\n",
    "  - paddlepaddle >= 2.1.0\n",
    "\n",
    "  - paddlehub >= 2.1.0    | [如何安装PaddleHub](../../../../docs/docs_ch/get_start/installation.rst)\n",
    "\n",
    "- ### 2、安装\n",
    "\n",
    "  - ```shell\n",
    "    $ hub install fastspeech2_ljspeech\n",
    "    ```\n",
    "  - 如您安装时遇到问题，可参考：[零基础windows安装](../../../../docs/docs_ch/get_start/windows_quickstart.md)\n",
    " | [零基础Linux安装](../../../../docs/docs_ch/get_start/linux_quickstart.md) | [零基础MacOS安装](../../../../docs/docs_ch/get_start/mac_quickstart.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fe459",
   "metadata": {
    "id": "三-模型api预测"
   },
   "source": [
    "## 三、模型API预测\n",
    "\n",
    "- ### 1、预测代码示例\n",
    "\n",
    "    ```python\n",
    "    import paddlehub as hub\n",
    "\n",
    "    # 需要合成语音的文本\n",
    "    sentences = ['The quick brown fox jumps over a lazy dog.']\n",
    "\n",
    "    model = hub.Module(\n",
    "        name='fastspeech2_ljspeech',\n",
    "        version='1.0.0')\n",
    "    wav_files =  model.generate(sentences)\n",
    "\n",
    "    # 打印合成的音频文件的路径\n",
    "    print(wav_files)\n",
    "    ```\n",
    "\n",
    "    详情可参考PaddleHub示例：\n",
    "    - [语音合成](../../../../demo/text_to_speech)\n",
    "\n",
    "- ### 2、API\n",
    "  - ```python\n",
    "    def __init__(output_dir)\n",
    "    ```\n",
    "\n",
    "    - 创建Module对象（动态图组网版本）\n",
    "\n",
    "    - **参数**\n",
    "\n",
    "      - `output_dir`： 合成音频文件的输出目录。\n",
    "\n",
    "  - ```python\n",
    "    def generate(\n",
    "        sentences,\n",
    "        device='cpu',\n",
    "    )\n",
    "    ```\n",
    "    - 将输入的文本合成为音频文件并保存到输出目录。\n",
    "\n",
    "    - **参数**\n",
    "\n",
    "      - `sentences`：合成音频的文本列表，类型为`List[str]`。\n",
    "      - `device`：预测时使用的设备，默认为`cpu`，如需使用gpu预测，请设置为`gpu`。\n",
    "\n",
    "    - **返回**\n",
    "\n",
    "      - `wav_files`：`List[str]`类型，返回合成音频的存放路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4932ea54",
   "metadata": {
    "id": "四-服务部署"
   },
   "source": [
    "## 四、服务部署\n",
    "\n",
    "- PaddleHub Serving可以部署一个在线的语音识别服务。\n",
    "\n",
    "- ### 第一步：启动PaddleHub Serving\n",
    "\n",
    "  - ```shell\n",
    "    $ hub serving start -m fastspeech2_ljspeech\n",
    "    ```\n",
    "\n",
    "  - 这样就完成了一个语音识别服务化API的部署，默认端口号为8866。\n",
    "\n",
    "  - **NOTE:** 如使用GPU预测，则需要在启动服务之前，请设置CUDA_VISIBLE_DEVICES环境变量，否则不用设置。\n",
    "\n",
    "- ### 第二步：发送预测请求\n",
    "\n",
    "  - 配置好服务端，以下数行代码即可实现发送预测请求，获取预测结果\n",
    "\n",
    "  - ```python\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    # 需要合成语音的文本\n",
    "    sentences = [\n",
    "        'The quick brown fox jumps over a lazy dog.',\n",
    "        'Today is a good day!',\n",
    "    ]\n",
    "\n",
    "    # 以key的方式指定text传入预测方法的时的参数，此例中为\"sentences\"\n",
    "    data = {\"sentences\": sentences}\n",
    "\n",
    "    # 发送post请求，content-type类型应指定json方式，url中的ip地址需改为对应机器的ip\n",
    "    url = \"http://127.0.0.1:8866/predict/fastspeech2_ljspeech\"\n",
    "\n",
    "    # 指定post请求的headers为application/json方式\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    r = requests.post(url=url, headers=headers, data=json.dumps(data))\n",
    "    print(r.json())\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e3d772",
   "metadata": {
    "id": "五-更新历史"
   },
   "source": [
    "## 五、更新历史\n",
    "\n",
    "* 1.0.0\n",
    "\n",
    "  初始发布\n",
    "\n",
    "  ```shell\n",
    "  $ hub install fastspeech2_ljspeech\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "fastspeech2_ljspeech",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
