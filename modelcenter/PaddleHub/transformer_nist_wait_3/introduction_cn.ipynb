{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a1e72f",
   "metadata": {
    "id": "transformer_nist_wait_3"
   },
   "source": [
    "# transformer_nist_wait_3\n",
    "\n",
    "|模型名称|transformer_nist_wait_3|\n",
    "| :--- | :---: |\n",
    "|类别|同声传译|\n",
    "|网络|transformer|\n",
    "|数据集|NIST 2008-中英翻译数据集|\n",
    "|是否支持Fine-tuning|否|\n",
    "|模型大小|377MB|\n",
    "|最新更新日期|2021-09-17|\n",
    "|数据指标|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d520e",
   "metadata": {
    "id": "一-模型基本信息"
   },
   "source": [
    "## 一、模型基本信息\n",
    "\n",
    "- ### 模型介绍\n",
    "\n",
    "  - 同声传译（Simultaneous Translation），即在句子完成之前进行翻译，同声传译的目标是实现同声传译的自动化，它可以与源语言同时翻译，延迟时间只有几秒钟。\n",
    "    STACL 是论文 [STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework](https://www.aclweb.org/anthology/P19-1289/) 中针对同传提出的适用于所有同传场景的翻译架构。\n",
    "    - STACL 主要具有以下优势：\n",
    "\n",
    "    - Prefix-to-Prefix架构拥有预测能力，即在未看到源词的情况下仍然可以翻译出对应的目标词，克服了SOV→SVO等词序差异\n",
    "    <p align=\"center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/40840292/133761990-13e55d0f-5c3a-476c-8865-5808d13cba97.png\"/> <br/>\n",
    "    </p>\n",
    "     和传统的机器翻译模型主要的区别在于翻译时是否需要利用全句的源句。上图中，Seq2Seq模型需要等到全句的源句（1-5）全部输入Encoder后，Decoder才开始解码进行翻译；而STACL架构采用了Wait-k（图中Wait-2）的策略，当源句只有两个词（1和2）输入到Encoder后，Decoder即可开始解码预测目标句的第一个词。\n",
    "\n",
    "    - Wait-k策略可以不需要全句的源句，直接预测目标句，可以实现任意的字级延迟，同时保持较高的翻译质量。\n",
    "    <p align=\"center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/40840292/133762098-6ea6f3ca-0d70-4a0a-981d-0fcc6f3cd96b.png\"/> <br/>\n",
    "    </p>\n",
    "     Wait-k策略首先等待源句单词，然后与源句的其余部分同时翻译，即输出总是隐藏在输入后面。这是受到同声传译人员的启发，同声传译人员通常会在几秒钟内开始翻译演讲者的演讲，在演讲者结束几秒钟后完成。例如，如果k=2，第一个目标词使用前2个源词预测，第二个目标词使用前3个源词预测，以此类推。上图中，(a)simultaneous: our wait-2 等到\"布什\"和\"总统\"输入后就开始解码预测\"pres.\"，而(b) non-simultaneous baseline 为传统的翻译模型，需要等到整句\"布什 总统 在 莫斯科 与 普京 会晤\"才开始解码预测。\n",
    "\n",
    "  - 该PaddleHub Module基于transformer网络结构，采用wait-3策略进行中文到英文的翻译。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7dcc5",
   "metadata": {
    "id": "二-安装"
   },
   "source": [
    "## 二、安装\n",
    "\n",
    "- ### 1、环境依赖\n",
    "\n",
    "  - paddlepaddle >= 2.1.0\n",
    "\n",
    "  - paddlehub >= 2.1.0    | [如何安装PaddleHub](../../../../docs/docs_ch/get_start/installation.rst)\n",
    "\n",
    "- ### 2、安装\n",
    "\n",
    "  - ```shell\n",
    "    $ hub install transformer_nist_wait_3\n",
    "    ```\n",
    "\n",
    "  - 如您安装时遇到问题，可参考：[零基础windows安装](../../../../docs/docs_ch/get_start/windows_quickstart.md)\n",
    " | [零基础Linux安装](../../../../docs/docs_ch/get_start/linux_quickstart.md) | [零基础MacOS安装](../../../../docs/docs_ch/get_start/mac_quickstart.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c7b65",
   "metadata": {
    "id": "三-模型api预测"
   },
   "source": [
    "## 三、模型API预测\n",
    "\n",
    "- ### 1、预测代码示例\n",
    "\n",
    "  - ```python\n",
    "    import paddlehub as hub\n",
    "\n",
    "    model = hub.Module(name=\"transformer_nist_wait_3\")\n",
    "\n",
    "    # 待预测数据（模拟同声传译实时输入）\n",
    "    text = [\n",
    "        \"他\",\n",
    "        \"他还\",\n",
    "        \"他还说\",\n",
    "        \"他还说现在\",\n",
    "        \"他还说现在正在\",\n",
    "        \"他还说现在正在为\",\n",
    "        \"他还说现在正在为这\",\n",
    "        \"他还说现在正在为这一\",\n",
    "        \"他还说现在正在为这一会议\",\n",
    "        \"他还说现在正在为这一会议作出\",\n",
    "        \"他还说现在正在为这一会议作出安排\",\n",
    "        \"他还说现在正在为这一会议作出安排。\",\n",
    "    ]\n",
    "\n",
    "    for t in text:\n",
    "        print(\"input: {}\".format(t))\n",
    "        result = model.translate(t)\n",
    "        print(\"model output: {}\\n\".format(result))\n",
    "\n",
    "    # input: 他\n",
    "    # model output:\n",
    "    #\n",
    "    # input: 他还\n",
    "    # model output:\n",
    "    #\n",
    "    # input: 他还说\n",
    "    # model output: he\n",
    "    #\n",
    "    # input: 他还说现在\n",
    "    # model output: he also\n",
    "    #\n",
    "    # input: 他还说现在正在\n",
    "    # model output: he also said\n",
    "    #\n",
    "    # input: 他还说现在正在为\n",
    "    # model output: he also said that\n",
    "    #\n",
    "    # input: 他还说现在正在为这\n",
    "    # model output: he also said that he\n",
    "    #\n",
    "    # input: 他还说现在正在为这一\n",
    "    # model output: he also said that he is\n",
    "    #\n",
    "    # input: 他还说现在正在为这一会议\n",
    "    # model output: he also said that he is making\n",
    "    #\n",
    "    # input: 他还说现在正在为这一会议作出\n",
    "    # model output: he also said that he is making preparations\n",
    "    #\n",
    "    # input: 他还说现在正在为这一会议作出安排\n",
    "    # model output: he also said that he is making preparations for\n",
    "    #\n",
    "    # input: 他还说现在正在为这一会议作出安排。\n",
    "    # model output: he also said that he is making preparations for this meeting .\n",
    "    ```\n",
    "\n",
    "- ### 2、 API\n",
    "\n",
    "    - ```python\n",
    "      __init__(max_length=256, max_out_len=256)\n",
    "      ```\n",
    "\n",
    "        - 初始化module， 可配置模型的输入文本的最大长度\n",
    "\n",
    "        - **参数**\n",
    "\n",
    "            - max_length(int): 输入文本的最大长度，默认值为256。\n",
    "            - max_out_len(int): 输出文本的最大解码长度，超过最大解码长度时会截断句子的后半部分，默认值为256。\n",
    "\n",
    "    - ```python\n",
    "      translate(text, use_gpu=False)\n",
    "      ```\n",
    "\n",
    "        - 预测API，输入源语言的文本（模拟同传语音输入），解码后输出翻译后的目标语言文本。\n",
    "\n",
    "        - **参数**\n",
    "\n",
    "            - text(str): 输入源语言的文本，数据类型为str\n",
    "            - use_gpu(bool): 是否使用gpu进行预测，默认为False\n",
    "\n",
    "        - **返回**\n",
    "\n",
    "            - result(str): 翻译后的目标语言文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be09fb5",
   "metadata": {
    "id": "四-服务部署"
   },
   "source": [
    "## 四、服务部署\n",
    "\n",
    "- PaddleHub Serving可以部署一个在线同声传译服务(需要用户配置一个语音转文本应用预先将语音输入转为中文文字)，可以将此接口用于在线web应用。\n",
    "\n",
    "- ### 第一步：启动PaddleHub Serving\n",
    "\n",
    "  - 运行启动命令：\n",
    "\n",
    "  - ```shell\n",
    "    $ hub serving start -m transformer_nist_wait_3\n",
    "    ```\n",
    "\n",
    "  - 启动时会显示加载模型过程，启动成功后显示\n",
    "\n",
    "  - ```shell\n",
    "    Loading transformer_nist_wait_3 successful.\n",
    "    ```\n",
    "\n",
    "  - 这样就完成了服务化API的部署，默认端口号为8866。\n",
    "\n",
    "  - **NOTE:** 如使用GPU预测，则需要在启动服务之前，请设置CUDA_VISIBLE_DEVICES环境变量，否则不用设置。\n",
    "\n",
    "- ### 第二步：发送预测请求\n",
    "\n",
    "  - 配置好服务端，以下数行代码即可实现发送预测请求，获取预测结果\n",
    "\n",
    "  - ```python\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    # 待预测数据（模拟同声传译实时输入）\n",
    "    text = [\n",
    "        \"他\",\n",
    "        \"他还\",\n",
    "        \"他还说\",\n",
    "        \"他还说现在\",\n",
    "        \"他还说现在正在\",\n",
    "        \"他还说现在正在为\",\n",
    "        \"他还说现在正在为这\",\n",
    "        \"他还说现在正在为这一\",\n",
    "        \"他还说现在正在为这一会议\",\n",
    "        \"他还说现在正在为这一会议作出\",\n",
    "        \"他还说现在正在为这一会议作出安排\",\n",
    "        \"他还说现在正在为这一会议作出安排。\",\n",
    "    ]\n",
    "\n",
    "    # 指定预测方法为transformer_nist_wait_3并发送post请求，content-type类型应指定json方式\n",
    "    # HOST_IP为服务器IP\n",
    "    url = \"http://HOST_IP:8866/predict/transformer_nist_wait_3\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for t in text:\n",
    "        print(\"input: {}\".format(t))\n",
    "        result = requests.post(url=url, headers=headers, data=json.dumps({\"text\": t}))\n",
    "        # 打印预测结果\n",
    "        print(\"model output: {}\\n\".format(result.json()['results']))\n",
    "\n",
    "  - 关于PaddleHub Serving更多信息参考：[服务部署](../../../../docs/docs_ch/tutorial/serving.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca306594",
   "metadata": {
    "id": "五-更新历史"
   },
   "source": [
    "## 五、更新历史\n",
    "\n",
    "* 1.0.0\n",
    "    初始发布\n",
    "    ```shell\n",
    "    hub install transformer_nist_wait_3==1.0.0\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "transformer_nist_wait_3",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
